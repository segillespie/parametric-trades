<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Non-Parametric Regression | Design of Experiments for Modeling and Simulation</title>
  <meta name="description" content="11 Non-Parametric Regression | Design of Experiments for Modeling and Simulation" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Non-Parametric Regression | Design of Experiments for Modeling and Simulation" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Non-Parametric Regression | Design of Experiments for Modeling and Simulation" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="advanced-experimental-designs.html"/>
<link rel="next" href="optional-advanced-doe-topics.html"/>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.17/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<script src="libs/plotly-binding-4.9.3/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://doe.dscoe.org/">DoE for M&S</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to DOE for M&amp;S</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#background."><i class="fa fa-check"></i><b>1.1</b> Background.</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#mission."><i class="fa fa-check"></i><b>1.2</b> Mission.</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#simulation-analysis."><i class="fa fa-check"></i><b>1.2.1</b> Simulation Analysis.</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#enduring-resource."><i class="fa fa-check"></i><b>1.2.2</b> Enduring Resource.</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#community."><i class="fa fa-check"></i><b>1.2.3</b> Community.</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#concept."><i class="fa fa-check"></i><b>1.3</b> Concept.</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#training-concept"><i class="fa fa-check"></i><b>1.4</b> Training Concept</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#lessons."><i class="fa fa-check"></i><b>1.4.1</b> Lessons.</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#projects."><i class="fa fa-check"></i><b>1.4.2</b> Projects.</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#assessments."><i class="fa fa-check"></i><b>1.4.3</b> Assessments.</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#curriculum."><i class="fa fa-check"></i><b>1.4.4</b> Curriculum.</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#community-concept"><i class="fa fa-check"></i><b>1.5</b> Community Concept</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#admin"><i class="fa fa-check"></i><b>1.6</b> Admin</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6.1</b> Contact</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#errors"><i class="fa fa-check"></i><b>1.6.2</b> Errors</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i><b>1.6.3</b> Resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to R</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#introduction-to-r---part-i"><i class="fa fa-check"></i><b>2.1</b> Introduction to R - Part I</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#objectives-1"><i class="fa fa-check"></i><b>2.1.1</b> Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#task-1---read-chapter-1-and-install-tidyverse"><i class="fa fa-check"></i><b>2.1.2</b> Task 1 - Read Chapter 1 and Install Tidyverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-operations"><i class="fa fa-check"></i><b>2.1.3</b> Basic Operations</a></li>
<li class="chapter" data-level="2.1.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#variable-types"><i class="fa fa-check"></i><b>2.1.4</b> Variable Types</a></li>
<li class="chapter" data-level="2.1.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-structures"><i class="fa fa-check"></i><b>2.1.5</b> Data Structures</a></li>
<li class="chapter" data-level="2.1.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#relational-and-logical-operators"><i class="fa fa-check"></i><b>2.1.6</b> Relational and Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#introduction-to-r---part-ii"><i class="fa fa-check"></i><b>2.2</b> Introduction to R - Part II</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#reading-tasks"><i class="fa fa-check"></i><b>2.2.1</b> Reading Tasks</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#problem-set"><i class="fa fa-check"></i><b>2.2.2</b> Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistics-review.html"><a href="statistics-review.html"><i class="fa fa-check"></i><b>3</b> Statistics Review</a><ul>
<li class="chapter" data-level="3.1" data-path="statistics-review.html"><a href="statistics-review.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="statistics-review.html"><a href="statistics-review.html#resources-1"><i class="fa fa-check"></i><b>3.1.1</b> Resources</a></li>
<li class="chapter" data-level="3.1.2" data-path="statistics-review.html"><a href="statistics-review.html#organization"><i class="fa fa-check"></i><b>3.1.2</b> Organization</a></li>
<li class="chapter" data-level="3.1.3" data-path="statistics-review.html"><a href="statistics-review.html#poc"><i class="fa fa-check"></i><b>3.1.3</b> POC</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics"><i class="fa fa-check"></i><b>3.2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.2.1" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics---description"><i class="fa fa-check"></i><b>3.2.1</b> Descriptive Statistics - Description</a></li>
<li class="chapter" data-level="3.2.2" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics---tutorial"><i class="fa fa-check"></i><b>3.2.2</b> Descriptive Statistics - Tutorial</a></li>
<li class="chapter" data-level="3.2.3" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics---problem-set"><i class="fa fa-check"></i><b>3.2.3</b> Descriptive Statistics - Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts"><i class="fa fa-check"></i><b>3.3</b> Statistical Concepts</a><ul>
<li class="chapter" data-level="3.3.1" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts---description"><i class="fa fa-check"></i><b>3.3.1</b> Statistical Concepts - Description</a></li>
<li class="chapter" data-level="3.3.2" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts---tutorial"><i class="fa fa-check"></i><b>3.3.2</b> Statistical Concepts - Tutorial</a></li>
<li class="chapter" data-level="3.3.3" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts---problem-set"><i class="fa fa-check"></i><b>3.3.3</b> Statistical Concepts - Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.4.1" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---description"><i class="fa fa-check"></i><b>3.4.1</b> Statistical Inference - Description</a></li>
<li class="chapter" data-level="3.4.2" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---tutorial"><i class="fa fa-check"></i><b>3.4.2</b> Statistical Inference - Tutorial</a></li>
<li class="chapter" data-level="3.4.3" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---conclusion"><i class="fa fa-check"></i><b>3.4.3</b> Statistical Inference - Conclusion</a></li>
<li class="chapter" data-level="3.4.4" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---problem-set"><i class="fa fa-check"></i><b>3.4.4</b> Statistical Inference - Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>4</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-overview"><i class="fa fa-check"></i><b>4.2</b> ANOVA Overview</a><ul>
<li class="chapter" data-level="4.2.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#motivation"><i class="fa fa-check"></i><b>4.2.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#broad-concept"><i class="fa fa-check"></i><b>4.2.2</b> Broad Concept</a></li>
<li class="chapter" data-level="4.2.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#terminology"><i class="fa fa-check"></i><b>4.2.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-fixed-effects-anova"><i class="fa fa-check"></i><b>4.3</b> Single Factor, Fixed Effects ANOVA</a><ul>
<li class="chapter" data-level="4.3.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#example"><i class="fa fa-check"></i><b>4.3.1</b> Example</a></li>
<li class="chapter" data-level="4.3.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#model"><i class="fa fa-check"></i><b>4.3.2</b> Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-anova-assumptions"><i class="fa fa-check"></i><b>4.3.3</b> Single Factor ANOVA Assumptions</a></li>
<li class="chapter" data-level="4.3.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#conducting-single-factor-fixed-effects-anova"><i class="fa fa-check"></i><b>4.3.4</b> Conducting Single Factor (Fixed Effects) ANOVA</a></li>
<li class="chapter" data-level="4.3.5" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-fixed-effects-anova-problem-set"><i class="fa fa-check"></i><b>4.3.5</b> Single Factor, Fixed Effects ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-random-effects-model"><i class="fa fa-check"></i><b>4.4</b> Single Factor, Random Effects Model</a><ul>
<li class="chapter" data-level="4.4.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-random-effects-problem-set"><i class="fa fa-check"></i><b>4.4.1</b> Single Factor, Random Effects Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#two-factor-fixed-effects-anova"><i class="fa fa-check"></i><b>4.5</b> Two Factor, Fixed Effects ANOVA</a><ul>
<li class="chapter" data-level="4.5.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#including-interaction-effects"><i class="fa fa-check"></i><b>4.5.1</b> Including Interaction Effects</a></li>
<li class="chapter" data-level="4.5.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#two-factor-anova-problem-set"><i class="fa fa-check"></i><b>4.5.2</b> Two Factor ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-for-more-than-two-factors"><i class="fa fa-check"></i><b>4.6</b> ANOVA For More Than Two Factors</a><ul>
<li class="chapter" data-level="4.6.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multi-factor-anova-problem-set"><i class="fa fa-check"></i><b>4.6.1</b> Multi-Factor ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.7</b> Multiple Comparisons</a><ul>
<li class="chapter" data-level="4.7.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#tukey-test-example"><i class="fa fa-check"></i><b>4.7.1</b> Tukey Test Example</a></li>
<li class="chapter" data-level="4.7.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#tukey-test-in-r"><i class="fa fa-check"></i><b>4.7.2</b> Tukey Test in R</a></li>
<li class="chapter" data-level="4.7.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#tukey-test-multiple-factors"><i class="fa fa-check"></i><b>4.7.3</b> Tukey Test, Multiple Factors</a></li>
<li class="chapter" data-level="4.7.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multiple-comparisons-problem-set"><i class="fa fa-check"></i><b>4.7.4</b> Multiple Comparisons Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-summary"><i class="fa fa-check"></i><b>4.8</b> ANOVA Summary</a><ul>
<li class="chapter" data-level="4.8.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#how-to-calculate-anova"><i class="fa fa-check"></i><b>4.8.1</b> How to Calculate ANOVA</a></li>
<li class="chapter" data-level="4.8.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-hypothesis-tests"><i class="fa fa-check"></i><b>4.8.2</b> ANOVA Hypothesis Test(s)</a></li>
<li class="chapter" data-level="4.8.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-assumptions"><i class="fa fa-check"></i><b>4.8.3</b> ANOVA Assumptions</a></li>
<li class="chapter" data-level="4.8.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multiple-comparisons-1"><i class="fa fa-check"></i><b>4.8.4</b> Multiple Comparisons</a></li>
<li class="chapter" data-level="4.8.5" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#further-extensions-of-anova"><i class="fa fa-check"></i><b>4.8.5</b> Further Extensions of ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html"><i class="fa fa-check"></i><b>5</b> Fundamentals of Design of Experiments</a><ul>
<li class="chapter" data-level="5.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#admin-1"><i class="fa fa-check"></i><b>5.1.1</b> Admin</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#doe-overview"><i class="fa fa-check"></i><b>5.2</b> DOE Overview</a><ul>
<li class="chapter" data-level="5.2.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#what-is-design-of-experiments"><i class="fa fa-check"></i><b>5.2.1</b> What is Design of Experiments?</a></li>
<li class="chapter" data-level="5.2.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#why-design-experiments"><i class="fa fa-check"></i><b>5.2.2</b> Why Design Experiments?</a></li>
<li class="chapter" data-level="5.2.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#strategies-for-experimental-design"><i class="fa fa-check"></i><b>5.2.3</b> Strategies for Experimental Design</a></li>
<li class="chapter" data-level="5.2.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#history-and-applications-of-experimental-design"><i class="fa fa-check"></i><b>5.2.4</b> History and Applications of Experimental Design</a></li>
<li class="chapter" data-level="5.2.5" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#principles"><i class="fa fa-check"></i><b>5.2.5</b> Principles</a></li>
<li class="chapter" data-level="5.2.6" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#guidelines"><i class="fa fa-check"></i><b>5.2.6</b> Guidelines</a></li>
<li class="chapter" data-level="5.2.7" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#problem-set-1"><i class="fa fa-check"></i><b>5.2.7</b> Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#factorial-designs"><i class="fa fa-check"></i><b>5.3</b> Factorial Designs</a><ul>
<li class="chapter" data-level="5.3.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#introduction-3"><i class="fa fa-check"></i><b>5.3.1</b> Introduction</a></li>
<li class="chapter" data-level="5.3.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#assessing-and-estimating-effects"><i class="fa fa-check"></i><b>5.3.2</b> Assessing and Estimating Effects</a></li>
<li class="chapter" data-level="5.3.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#blocking"><i class="fa fa-check"></i><b>5.3.3</b> Blocking</a></li>
<li class="chapter" data-level="5.3.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#problem-set-2"><i class="fa fa-check"></i><b>5.3.4</b> Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#k-factorial-design"><i class="fa fa-check"></i><b>5.4</b> <span class="math inline">\(2^K\)</span> Factorial Design</a><ul>
<li class="chapter" data-level="5.4.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#introduction-4"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#coding-variables-and-effects-signs"><i class="fa fa-check"></i><b>5.4.2</b> Coding Variables and Effects Signs</a></li>
<li class="chapter" data-level="5.4.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#estimating-effects-and-contrasts"><i class="fa fa-check"></i><b>5.4.3</b> Estimating Effects and Contrasts</a></li>
<li class="chapter" data-level="5.4.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#unreplicated-2k-designs"><i class="fa fa-check"></i><b>5.4.4</b> Unreplicated <span class="math inline">\(2^K\)</span> Designs</a></li>
<li class="chapter" data-level="5.4.5" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#problem-set-3"><i class="fa fa-check"></i><b>5.4.5</b> Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html"><i class="fa fa-check"></i><b>6</b> Fractional Factorial Designs</a><ul>
<li class="chapter" data-level="6.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#introduction-5"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#admin-2"><i class="fa fa-check"></i><b>6.1.1</b> Admin</a></li>
<li class="chapter" data-level="6.1.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#overview"><i class="fa fa-check"></i><b>6.1.2</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#frac12-fractional-factorial-design-2k-1"><i class="fa fa-check"></i><b>6.2</b> <span class="math inline">\(\frac{1}{2}\)</span> Fractional Factorial Design (<span class="math inline">\(2^{K-1}\)</span>)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#motivation-and-example"><i class="fa fa-check"></i><b>6.2.1</b> Motivation and Example</a></li>
<li class="chapter" data-level="6.2.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#design-generator"><i class="fa fa-check"></i><b>6.2.2</b> Design Generator</a></li>
<li class="chapter" data-level="6.2.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#estimating-effects"><i class="fa fa-check"></i><b>6.2.3</b> Estimating Effects</a></li>
<li class="chapter" data-level="6.2.4" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#geometric-view"><i class="fa fa-check"></i><b>6.2.4</b> Geometric View</a></li>
<li class="chapter" data-level="6.2.5" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#example-24-1-design"><i class="fa fa-check"></i><b>6.2.5</b> Example <span class="math inline">\(2^{4-1}\)</span> Design</a></li>
<li class="chapter" data-level="6.2.6" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#frac12-fractional-factorial-design-2k-1-problem-set"><i class="fa fa-check"></i><b>6.2.6</b> <span class="math inline">\(\frac{1}{2}\)</span> Fractional Factorial Design (<span class="math inline">\(2^{K-1}\)</span>) Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#general-2k-p-designs"><i class="fa fa-check"></i><b>6.3</b> General <span class="math inline">\(2^{K-P}\)</span> Designs</a><ul>
<li class="chapter" data-level="6.3.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#introduction-6"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#quarter-fractional-design"><i class="fa fa-check"></i><b>6.3.2</b> Quarter Fractional Design</a></li>
<li class="chapter" data-level="6.3.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#general-2k-p-design"><i class="fa fa-check"></i><b>6.3.3</b> General <span class="math inline">\(2^{K-P}\)</span> Design</a></li>
<li class="chapter" data-level="6.3.4" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#resolution"><i class="fa fa-check"></i><b>6.3.4</b> Resolution</a></li>
<li class="chapter" data-level="6.3.5" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#general-2k-p-designs-problem-set"><i class="fa fa-check"></i><b>6.3.5</b> General <span class="math inline">\(2^{K-P}\)</span> Designs Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#additional-notes-on-2k-p-designs"><i class="fa fa-check"></i><b>6.4</b> Additional Notes on <span class="math inline">\(2^{K-P}\)</span> Designs</a><ul>
<li class="chapter" data-level="6.4.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#additional-topics"><i class="fa fa-check"></i><b>6.4.1</b> Additional Topics</a></li>
<li class="chapter" data-level="6.4.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#sequencing-experiments"><i class="fa fa-check"></i><b>6.4.2</b> Sequencing Experiments</a></li>
<li class="chapter" data-level="6.4.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#fractional-factorial-design-simulation-problem-set"><i class="fa fa-check"></i><b>6.4.3</b> Fractional Factorial Design Simulation Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#screening-experiments-and-selecting-factors"><i class="fa fa-check"></i><b>6.5</b> Screening Experiments and Selecting Factors</a></li>
<li class="chapter" data-level="6.6" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#conclusion"><i class="fa fa-check"></i><b>6.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html"><i class="fa fa-check"></i><b>7</b> Fundamentals of Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#admin-3"><i class="fa fa-check"></i><b>7.1</b> Admin</a></li>
<li class="chapter" data-level="7.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#least-squares-method-manually"><i class="fa fa-check"></i><b>7.2.1</b> Least Squares Method Manually</a></li>
<li class="chapter" data-level="7.2.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>7.2.2</b> Goodness of Fit</a></li>
<li class="chapter" data-level="7.2.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#least-squares-method-in-r"><i class="fa fa-check"></i><b>7.2.3</b> Least Squares Method In <em>R</em></a></li>
<li class="chapter" data-level="7.2.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#simple-linear-regression-problem-set"><i class="fa fa-check"></i><b>7.2.4</b> Simple Linear Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#assumptions-and-diagnostics"><i class="fa fa-check"></i><b>7.3</b> Assumptions and Diagnostics</a><ul>
<li class="chapter" data-level="7.3.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#linearity"><i class="fa fa-check"></i><b>7.3.1</b> Linearity</a></li>
<li class="chapter" data-level="7.3.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#homoscedasticity"><i class="fa fa-check"></i><b>7.3.2</b> Homoscedasticity</a></li>
<li class="chapter" data-level="7.3.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#independence"><i class="fa fa-check"></i><b>7.3.3</b> Independence</a></li>
<li class="chapter" data-level="7.3.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#normality"><i class="fa fa-check"></i><b>7.3.4</b> Normality</a></li>
<li class="chapter" data-level="7.3.5" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#unusual-observations"><i class="fa fa-check"></i><b>7.3.5</b> Unusual Observations</a></li>
<li class="chapter" data-level="7.3.6" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#linear-regression-assumptions-and-diagnostics-problem-set"><i class="fa fa-check"></i><b>7.3.6</b> Linear Regression Assumptions and Diagnostics Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.4</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="7.4.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#r-example"><i class="fa fa-check"></i><b>7.4.1</b> <em>R</em> Example</a></li>
<li class="chapter" data-level="7.4.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#multiple-linear-regression-problem-set"><i class="fa fa-check"></i><b>7.4.2</b> Multiple Linear Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#categorical-variables"><i class="fa fa-check"></i><b>7.5</b> Categorical Variables</a><ul>
<li class="chapter" data-level="7.5.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#categorical-linear-regression-problem-set"><i class="fa fa-check"></i><b>7.5.1</b> Categorical Linear Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#transformation"><i class="fa fa-check"></i><b>7.6</b> Transformation</a><ul>
<li class="chapter" data-level="7.6.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#identifying-non-linear-relationships"><i class="fa fa-check"></i><b>7.6.1</b> Identifying Non-Linear Relationships</a></li>
<li class="chapter" data-level="7.6.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#checking-model-structure"><i class="fa fa-check"></i><b>7.6.2</b> Checking Model Structure</a></li>
<li class="chapter" data-level="7.6.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#how-to-transform-variables-in-r"><i class="fa fa-check"></i><b>7.6.3</b> How To Transform Variables In <em>R</em></a></li>
<li class="chapter" data-level="7.6.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#transformed-regression-problem-set"><i class="fa fa-check"></i><b>7.6.4</b> Transformed Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>7.7</b> Logistic Regression</a><ul>
<li class="chapter" data-level="7.7.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#motivating-example"><i class="fa fa-check"></i><b>7.7.1</b> Motivating Example</a></li>
<li class="chapter" data-level="7.7.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logit-function"><i class="fa fa-check"></i><b>7.7.2</b> Logit Function</a></li>
<li class="chapter" data-level="7.7.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>7.7.3</b> Logistic Regression in <em>R</em></a></li>
<li class="chapter" data-level="7.7.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression-diagnostics"><i class="fa fa-check"></i><b>7.7.4</b> Logistic Regression Diagnostics</a></li>
<li class="chapter" data-level="7.7.5" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression-problem-set"><i class="fa fa-check"></i><b>7.7.5</b> Logistic Regression Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>8</b> Model Selection</a><ul>
<li class="chapter" data-level="8.1" data-path="model-selection.html"><a href="model-selection.html#admin-4"><i class="fa fa-check"></i><b>8.1</b> Admin</a></li>
<li class="chapter" data-level="8.2" data-path="model-selection.html"><a href="model-selection.html#introduction-7"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="model-selection.html"><a href="model-selection.html#testing-based-methods"><i class="fa fa-check"></i><b>8.3</b> Testing-Based Methods</a></li>
<li class="chapter" data-level="8.4" data-path="model-selection.html"><a href="model-selection.html#criterion-based-methods"><i class="fa fa-check"></i><b>8.4</b> Criterion-Based Methods</a><ul>
<li class="chapter" data-level="8.4.1" data-path="model-selection.html"><a href="model-selection.html#criterion-problem-set"><i class="fa fa-check"></i><b>8.4.1</b> Criterion Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="model-selection.html"><a href="model-selection.html#cross-validation"><i class="fa fa-check"></i><b>8.5</b> Cross Validation</a><ul>
<li class="chapter" data-level="8.5.1" data-path="model-selection.html"><a href="model-selection.html#what-about-the-test-set"><i class="fa fa-check"></i><b>8.5.1</b> What About The Test Set?</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="model-selection.html"><a href="model-selection.html#lasso-regression"><i class="fa fa-check"></i><b>8.6</b> Lasso Regression</a><ul>
<li class="chapter" data-level="8.6.1" data-path="model-selection.html"><a href="model-selection.html#background-reading"><i class="fa fa-check"></i><b>8.6.1</b> Background Reading</a></li>
<li class="chapter" data-level="8.6.2" data-path="model-selection.html"><a href="model-selection.html#lasso-regression-in-r"><i class="fa fa-check"></i><b>8.6.2</b> Lasso Regression In R</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="model-selection.html"><a href="model-selection.html#parting-thought"><i class="fa fa-check"></i><b>8.7</b> Parting Thought</a></li>
<li class="chapter" data-level="8.8" data-path="model-selection.html"><a href="model-selection.html#lasso-regression-problem-set"><i class="fa fa-check"></i><b>8.8</b> Lasso Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="application-of-doe-to-the-advanced-warfighting-simulation.html"><a href="application-of-doe-to-the-advanced-warfighting-simulation.html"><i class="fa fa-check"></i><b>9</b> Application of DoE to the Advanced Warfighting Simulation</a></li>
<li class="chapter" data-level="10" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html"><i class="fa fa-check"></i><b>10</b> Advanced Experimental Designs</a><ul>
<li class="chapter" data-level="10.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#admin-5"><i class="fa fa-check"></i><b>10.1</b> Admin</a></li>
<li class="chapter" data-level="10.2" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#introduction-and-background"><i class="fa fa-check"></i><b>10.2</b> Introduction and Background</a></li>
<li class="chapter" data-level="10.3" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#central-composite-designs"><i class="fa fa-check"></i><b>10.3</b> Central Composite Designs</a><ul>
<li class="chapter" data-level="10.3.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#augmented-central-composite-designs"><i class="fa fa-check"></i><b>10.3.1</b> Augmented Central Composite Designs</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#response-surface-methodology"><i class="fa fa-check"></i><b>10.4</b> Response Surface Methodology</a><ul>
<li class="chapter" data-level="10.4.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#ccd-problem-set"><i class="fa fa-check"></i><b>10.4.1</b> CCD Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#nearly-orthogonal-latin-hypercube-designs"><i class="fa fa-check"></i><b>10.5</b> Nearly Orthogonal Latin Hypercube Designs</a><ul>
<li class="chapter" data-level="10.5.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#factor-codings"><i class="fa fa-check"></i><b>10.5.1</b> Factor Codings</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#orthogonality-and-variance-inflation-factors"><i class="fa fa-check"></i><b>10.6</b> Orthogonality and Variance Inflation Factors</a></li>
<li class="chapter" data-level="10.7" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#shifting-and-stacking"><i class="fa fa-check"></i><b>10.7</b> Shifting and Stacking</a><ul>
<li class="chapter" data-level="10.7.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#nolh-problem-set"><i class="fa fa-check"></i><b>10.7.1</b> NOLH Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>11</b> Non-Parametric Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#admin-6"><i class="fa fa-check"></i><b>11.1</b> Admin</a></li>
<li class="chapter" data-level="11.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#non-parametric-anova"><i class="fa fa-check"></i><b>11.2</b> Non-Parametric ANOVA</a><ul>
<li class="chapter" data-level="11.2.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#multiple-comparisons-2"><i class="fa fa-check"></i><b>11.2.1</b> Multiple Comparisons</a></li>
<li class="chapter" data-level="11.2.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#non-parametric-anova-problem-set"><i class="fa fa-check"></i><b>11.2.2</b> Non-Parametric ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#generalized-additive-models"><i class="fa fa-check"></i><b>11.3</b> Generalized Additive Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#loess"><i class="fa fa-check"></i><b>11.3.1</b> Loess</a></li>
<li class="chapter" data-level="11.3.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#splines"><i class="fa fa-check"></i><b>11.3.2</b> Splines</a></li>
<li class="chapter" data-level="11.3.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#cross-validation-1"><i class="fa fa-check"></i><b>11.3.3</b> Cross Validation</a></li>
<li class="chapter" data-level="11.3.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#gam-problem-set"><i class="fa fa-check"></i><b>11.3.4</b> GAM Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#support-vector-machines"><i class="fa fa-check"></i><b>11.4</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="11.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#support-vector-regression"><i class="fa fa-check"></i><b>11.4.1</b> Support Vector Regression</a></li>
<li class="chapter" data-level="11.4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#support-vector-classification"><i class="fa fa-check"></i><b>11.4.2</b> Support Vector Classification</a></li>
<li class="chapter" data-level="11.4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#svm-problem-set"><i class="fa fa-check"></i><b>11.4.3</b> SVM Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>11.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="11.5.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#regression-trees"><i class="fa fa-check"></i><b>11.5.1</b> Regression Trees</a></li>
<li class="chapter" data-level="11.5.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#random-forest-regression"><i class="fa fa-check"></i><b>11.5.2</b> Random Forest Regression</a></li>
<li class="chapter" data-level="11.5.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#random-forest-classification"><i class="fa fa-check"></i><b>11.5.3</b> Random Forest Classification</a></li>
<li class="chapter" data-level="11.5.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#cart-problem-set"><i class="fa fa-check"></i><b>11.5.4</b> CART Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html"><i class="fa fa-check"></i><b>12</b> Optional Advanced DOE Topics</a><ul>
<li class="chapter" data-level="12.1" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#robust-design"><i class="fa fa-check"></i><b>12.1</b> Robust Design</a></li>
<li class="chapter" data-level="12.2" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#sequential-designs"><i class="fa fa-check"></i><b>12.2</b> Sequential Designs</a></li>
<li class="chapter" data-level="12.3" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#ridge-regression"><i class="fa fa-check"></i><b>12.3</b> Ridge Regression</a></li>
<li class="chapter" data-level="12.4" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#neural-network-regression"><i class="fa fa-check"></i><b>12.4</b> Neural Network Regression</a><ul>
<li class="chapter" data-level="12.4.1" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#simple-neural-network-model"><i class="fa fa-check"></i><b>12.4.1</b> Simple Neural Network Model</a></li>
<li class="chapter" data-level="12.4.2" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#gradient-descent"><i class="fa fa-check"></i><b>12.4.2</b> Gradient Descent</a></li>
<li class="chapter" data-level="12.4.3" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>12.4.3</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="12.4.4" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#hidden-layer"><i class="fa fa-check"></i><b>12.4.4</b> Hidden Layer</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#neural-network-classification"><i class="fa fa-check"></i><b>12.5</b> Neural Network Classification</a></li>
<li class="chapter" data-level="12.6" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>12.6</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="12.7" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#non-parametric-statistics"><i class="fa fa-check"></i><b>12.7</b> Non-Parametric Statistics</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown">
Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Design of Experiments for Modeling and Simulation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="non-parametric-regression" class="section level1">
<h1><span class="header-section-number">11</span> Non-Parametric Regression</h1>
<p>As weve seen in the last few chapters, linear models can be successfully applied to many data sets. However, there may be times when even after transforming variables, your model clearly violates the assumptions of linear models. Alternatively, you may have evidence that there is a complex relationship between predictor and response that is difficult to capture through transformation. In these cases, non-parametric regression techniques offer alternative methods for modeling your data.</p>
<div id="admin-6" class="section level2">
<h2><span class="header-section-number">11.1</span> Admin</h2>
<p>For any errors associated with this section, please contact <a href="mailto:john.f.king1.mil@mail.mil">John King</a>.</p>
<p>This chapter was published using the following software:</p>
<ul>
<li>R version 3.6.0 (2019-04-26).</li>
<li>On x86_64-pc-linux-gnu (64-bit) running Ubuntu 18.04.2 LTS.</li>
<li>Packages used in this chapter are explicitly shown in the code snippets.</li>
</ul>
</div>
<div id="non-parametric-anova" class="section level2">
<h2><span class="header-section-number">11.2</span> Non-Parametric ANOVA</h2>
<p>In Chapter 4, we applied ANOVA to problems where we had a factor with three or more levels, and we wanted to test for a difference in response among the levels. One of the assumptions of parametric ANOVA is that the underlying data are normally distributed. If we have a data set that violates that assumption, as is often the case with counts (especially of relatively rare events), then well need to use a non-parametric method such as the Kruskal-Wallis (KW) test.</p>
<p>The setup for the KW test is as follows:</p>
<ul>
<li>Level 1: <span class="math inline">\(X_{11}, X_{12}, ...,X_{1J_{1}} \sim F_{1}\)</span></li>
<li>Level 2: <span class="math inline">\(X_{21}, X_{22}, ...,X_{2J_{2}} \sim F_{2}\)</span></li>
<li></li>
<li>Level I: <span class="math inline">\(X_{I1}, X_{I2}, ...,X_{IJ_{I}} \sim F_{I}\)</span></li>
</ul>
<p><strong>Null hypothesis</strong> <span class="math inline">\(H_{o}: F_{1} = F_{2} = ... = F_{I}\)</span></p>
<p><strong>Alternative hypothesis</strong> <span class="math inline">\(H_{a}:\)</span> not <span class="math inline">\(H{o}\)</span> (i.e., at least two of the distributions are different).</p>
<p>The idea behind the KW test is to sort the data and use an observations rank instead of the value itself. If we have a sample size <span class="math inline">\(N = J_{1}, J_{2}, ... J_{I}\)</span>, we first rank the observations (the lowest value gets a 1, second lowest a 2, etc.). If there are ties, then use the mid-rank (the mean of the two ranks). Then, separate the samples into their respective levels and sum the ranks for each level. For example, if we have three levels:</p>
<ul>
<li>Level 1: <span class="math inline">\(R_{11} + R_{12} + ... + R_{1J_{1}}=\)</span> sum of ranks</li>
<li>Level 2: <span class="math inline">\(R_{21} + R_{22} + ... + R_{2J_{1}}=\)</span> sum of ranks</li>
<li>Level 3: <span class="math inline">\(R_{31} + R_{32} + ... + R_{3J_{1}}=\)</span> sum of ranks</li>
</ul>
<p>Now we do the non-parametric equivalent of a sum of squares for treatment (SSTr) using these ranks. The KW test statistic takes the form:</p>
<p><span class="math display">\[K=\frac{12}{N(N+1)}\sum\limits_{i=1}^{I}{\frac{R^2_{i}}{J_{i}}-3(N+1)}\]</span></p>
<p>For hypothesis testing, we calculate a p-value where large values of K signal rejection. We do this by approximating K with a chi-square distribution having <span class="math inline">\(I-1\)</span> degrees of freedom. According to <span class="citation">Devore (<a href="#ref-devore2015">2015</a>)</span> (p.672), chi-square is a good approximation for K if:</p>
<ol style="list-style-type: decimal">
<li>If there are I = 3 treatments and each sample size is &gt;= 6, or</li>
<li>If there are I &gt; 3 treatments and each sample size is &gt;= 5</li>
</ol>
<p>Lets look at an example. Say we are a weapon manufacturer that produces rifles on three different assembly lines, and we want to know if theres a difference in the number of times a weapon jams. We select 10 random weapons from each assembly line and perform our weapon jamming test identically on all 30 weapons. Ill make up some dummy data for this situation by drawing random numbers from Poisson distributions with two different <span class="math inline">\(\lambda\)</span>s.</p>
<div class="sourceCode" id="cb790"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb790-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb790-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb790-3" data-line-number="3"></a>
<a class="sourceLine" id="cb790-4" data-line-number="4">j =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rpois</span>(<span class="dv">10</span>, <span class="dv">10</span>), <span class="kw">rpois</span>(<span class="dv">10</span>, <span class="dv">10</span>), <span class="kw">rpois</span>(<span class="dv">10</span>, <span class="dv">15</span>))</a>
<a class="sourceLine" id="cb790-5" data-line-number="5">r =<span class="st"> </span><span class="kw">rank</span>(j, <span class="dt">ties.method=</span><span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb790-6" data-line-number="6">l =<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;line&quot;</span>, <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">each =</span> <span class="dv">10</span>), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb790-7" data-line-number="7"></a>
<a class="sourceLine" id="cb790-8" data-line-number="8"><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb790-9" data-line-number="9">  <span class="dt">jams1 =</span> j[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>],</a>
<a class="sourceLine" id="cb790-10" data-line-number="10">  <span class="dt">rank1 =</span> r[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>],</a>
<a class="sourceLine" id="cb790-11" data-line-number="11">  <span class="dt">jams2 =</span> j[<span class="dv">11</span><span class="op">:</span><span class="dv">20</span>],</a>
<a class="sourceLine" id="cb790-12" data-line-number="12">  <span class="dt">rank2 =</span> r[<span class="dv">11</span><span class="op">:</span><span class="dv">20</span>],</a>
<a class="sourceLine" id="cb790-13" data-line-number="13">  <span class="dt">jams3 =</span> j[<span class="dv">21</span><span class="op">:</span><span class="dv">30</span>],</a>
<a class="sourceLine" id="cb790-14" data-line-number="14">  <span class="dt">rank3 =</span> r[<span class="dv">21</span><span class="op">:</span><span class="dv">30</span>])</a></code></pre></div>
<pre><code>## # A tibble: 10 x 6
##    jams1 rank1 jams2 rank2 jams3 rank3
##    &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;
##  1    14  19.5    14  19.5     8   3.5
##  2     8   3.5    17  27      21  30  
##  3    11  10.5     5   1      16  23.5
##  4    12  13.5    14  19.5    16  23.5
##  5    11  10.5    13  16      13  16  
##  6     9   6.5    12  13.5    18  29  
##  7    14  19.5    11  10.5    17  27  
##  8     9   6.5    13  16      11  10.5
##  9    16  23.5     7   2      16  23.5
## 10     9   6.5     9   6.5    17  27</code></pre>
<p>For this data,</p>
<ul>
<li><span class="math inline">\(I = 3\)</span></li>
<li><span class="math inline">\(J_{1} = J_{2} = J_{3} = 10\)</span></li>
<li><span class="math inline">\(N = 10 + 10 + 10 = 30\)</span></li>
<li><span class="math inline">\(R_{1} =\)</span> 120</li>
<li><span class="math inline">\(R_{2} =\)</span> 131.5</li>
<li><span class="math inline">\(R_{3} =\)</span> 213.5</li>
</ul>
<p>Which gives the K statistic:</p>
<p><span class="math display">\[K=\frac{12}{30(31)} \left[\frac{120^2}{10} + \frac{131.5^2}{10} + \frac{213.5^2}{10}\right] - 3(31)\]</span></p>
<div class="sourceCode" id="cb792"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb792-1" data-line-number="1">K =<span class="st"> </span><span class="dv">12</span><span class="op">/</span>(<span class="dv">30</span><span class="op">*</span><span class="dv">31</span>) <span class="op">*</span><span class="st"> </span>(<span class="kw">sum</span>(r[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(r[<span class="dv">11</span><span class="op">:</span><span class="dv">20</span>])<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(r[<span class="dv">21</span><span class="op">:</span><span class="dv">30</span>])<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span>) <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span><span class="dv">31</span></a>
<a class="sourceLine" id="cb792-2" data-line-number="2"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;K =&quot;</span>, K), <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## [1] K = 6.70903225806452</code></pre>
<p>Then, compute an approximate p-value using the chi-square test with two degrees of freedom.</p>
<div class="sourceCode" id="cb794"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb794-1" data-line-number="1"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pchisq</span>(K, <span class="dt">df=</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 0.03492627</code></pre>
<p>We therefore reject the null hypothesis at the <span class="math inline">\(\alpha = 0.5\)</span> test level. Of course, theres an <em>R</em> function <code>kruskall.test()</code> so we dont have to do all that by hand.</p>
<div class="sourceCode" id="cb796"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb796-1" data-line-number="1">rifles =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">jams =</span> j, <span class="dt">line =</span> l)</a>
<a class="sourceLine" id="cb796-2" data-line-number="2"></a>
<a class="sourceLine" id="cb796-3" data-line-number="3"><span class="kw">kruskal.test</span>(jams <span class="op">~</span><span class="st"> </span>line, <span class="dt">data =</span> rifles)</a></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  jams by line
## Kruskal-Wallis chi-squared = 6.7845, df = 2, p-value = 0.03363</code></pre>
<div id="multiple-comparisons-2" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Multiple Comparisons</h3>
<p>The KW test can also be used for multiple comparisons. Recall that we applied the Tukey Test to the parametric case, and it took into account that there is an increase in the probability of a Type I error when conducting multiple comparisons. The non-parametric equivalent is to combine the Bonferroni Method with the KW test. Consider the case where we conduct <span class="math inline">\(m\)</span> tests of the null hypothesis. We calculate the probability that at least one of the null hypotheses is rejected (<span class="math inline">\(P(\bar{A})\)</span>) as follows:</p>
<p><span class="math display">\[P(\bar{A}) = 1-P(A) = 1-P(A_{1} \cap A_{2} \cap...\cap A_{m})\]</span></p>
<p><span class="math display">\[=1-P(A_{1})P(A_{2})\cdot\cdot\cdot P(A_{m})\]</span></p>
<p><span class="math display">\[=1-(1-\alpha)^{m}\]</span></p>
<p>So if our individual test level target is <span class="math inline">\(\alpha=0.05\)</span> and we conduct <span class="math inline">\(m=10\)</span> tests, then <span class="math inline">\(P(\bar{A})=1-(1-0.05)^{10}=\)</span> 0.4012631. If we want to establish a <strong>family-wide Type I error rate</strong>, <span class="math inline">\(\Psi=P(\bar{A})=0.05\)</span>, then the individual test levels should be:</p>
<p><span class="math display">\[\alpha=P(\bar{A}_{i})=1-(1-\Psi)^{1/m}\]</span></p>
<p>For example, if <span class="math inline">\(\Psi=0.05\)</span> and we again conduct <span class="math inline">\(m=10\)</span> tests, then <span class="math inline">\(\alpha=1-(1-0.05)^{1/10}=\)</span> 0.0051162. The downfall of this approach is that the same data are used to test the collection of hypotheses, which violates the independence assumption. The Bonferroni Inequality saves us from this situation because it doesnt rely on the assumption of independence. Using the Bonferroni method, we simply calculate <span class="math inline">\(\alpha=\Psi/m = 0.005\)</span>. Note that this is almost identical to the result when we assumed independence. Unfortunately, the method isnt perfect. As noted in <a href="https://www.stat.berkeley.edu/~mgoldman/Section0402.pdf">Section 2 of this paper</a> the Bonferroni method tends to be overly conservative, which increases the chance of a false negative.</p>
<p>Using the <code>rifles</code> data from earlier, if we wanted to conduct all three pair-wise hypothesis tests, then <span class="math inline">\(\alpha=0.05/3=0.01667\)</span> for the individual KW tests.</p>
<p>The <code>dunn.test()</code> function from the aptly-named <code>dunn.test</code> package performs the multiple pair-wise tests and offers several methods for accounting for performing multiple tests, including Bonferroni. For example (and notice we get the KW test results also):</p>
<div class="sourceCode" id="cb798"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb798-1" data-line-number="1">dunn.test<span class="op">::</span><span class="kw">dunn.test</span>(rifles<span class="op">$</span>jams, rifles<span class="op">$</span>line, <span class="dt">method=</span><span class="st">&quot;bonferroni&quot;</span>)</a></code></pre></div>
<pre><code>##   Kruskal-Wallis rank sum test
## 
## data: x and group
## Kruskal-Wallis chi-squared = 6.7845, df = 2, p-value = 0.03
## 
## 
##                            Comparison of x by group                            
##                                  (Bonferroni)                                  
## Col Mean-|
## Row Mean |      line1      line2
## ---------+----------------------
##    line2 |  -0.293738
##          |     1.0000
##          |
##    line3 |  -2.388222  -2.094483
##          |     0.0254     0.0543
## 
## alpha = 0.05
## Reject Ho if p &lt;= alpha/2</code></pre>
<p>According to the <code>dunn.test</code> documentation, the null hypothesis for the Dunn test is:</p>
<blockquote>
<p>The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the ranksum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunns test may be understood as a test for median difference. dunn.test accounts for tied ranks.</p>
</blockquote>
</div>
<div id="non-parametric-anova-problem-set" class="section level3">
<h3><span class="header-section-number">11.2.2</span> Non-Parametric ANOVA Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter11_ProblemSets/KW_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter11_ProblemSets/kw_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter11_ProblemSets/KW_PS_Solutions.html'>here</a>.</p>
</div>
</div>
<div id="generalized-additive-models" class="section level2">
<h2><span class="header-section-number">11.3</span> Generalized Additive Models</h2>
<p>Recall that if there is a non-linear relationship between predictor and response, we can attempt to transform the predictor using a known function (log, reciprocal, polynomial, etc.) to improve the model structure and fit. What if the relationship is more complex and is not well captured with a known function? Generalized additive models may be used in these cases.</p>
<p>Recall that a linear model takes the form:</p>
<p><span class="math display">\[y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+...+\varepsilon\]</span></p>
<p>Additive models replace the linear terms (the <span class="math inline">\(\beta\)</span>s) with flexible smoothing functions and take the form:</p>
<p><span class="math display">\[y=\beta_{0}+f_{1}(x_{1})+f_{2}(x_{2})+...+\varepsilon\]</span></p>
<p>There are many techniques and options for selecting the smoothing functions, but for this tutorial, well discuss two: locally weighted error sum of squares (lowess and also commonly abbreviated as loess) and smoothing splines.</p>
<div id="loess" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Loess</h3>
<p>For the theory behind loess smoothing, please read <a href="https://www.itl.nist.gov/div898/handbook/pmd/section1/pmd144.htm">this page</a> on the NIST website. This chapter will focus on implementing loess smoothing in <em>R</em>.</p>
<p>All smoothers have a tuning parameter that controls how smooth the smoother is. The tuning parameter in loess is referred to as the span with larger values producing more smoothness.</p>
<div class="sourceCode" id="cb800"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb800-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb800-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb800-3" data-line-number="3"></a>
<a class="sourceLine" id="cb800-4" data-line-number="4">df =<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb800-5" data-line-number="5">  <span class="dt">x =</span> <span class="kw">runif</span>(<span class="dv">100</span>, <span class="fl">1.5</span>, <span class="fl">5.5</span>),</a>
<a class="sourceLine" id="cb800-6" data-line-number="6">  <span class="dt">y =</span> <span class="kw">sin</span>(x<span class="op">*</span>pi) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb800-7" data-line-number="7">)</a>
<a class="sourceLine" id="cb800-8" data-line-number="8"></a>
<a class="sourceLine" id="cb800-9" data-line-number="9">ex1.ls =<span class="st"> </span><span class="kw">loess</span>(y<span class="op">~</span>x, <span class="dt">span=</span><span class="fl">0.25</span>, <span class="dt">data=</span>df)</a>
<a class="sourceLine" id="cb800-10" data-line-number="10">ex2.ls =<span class="st"> </span><span class="kw">loess</span>(y<span class="op">~</span>x, <span class="dt">span=</span><span class="fl">0.5</span>, <span class="dt">data=</span>df)</a>
<a class="sourceLine" id="cb800-11" data-line-number="11">ex3.ls =<span class="st"> </span><span class="kw">loess</span>(y<span class="op">~</span>x, <span class="dt">span=</span><span class="fl">0.75</span>, <span class="dt">data=</span>df)</a>
<a class="sourceLine" id="cb800-12" data-line-number="12">xseq =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">1.6</span>, <span class="fl">5.4</span>,<span class="dt">length=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb800-13" data-line-number="13"></a>
<a class="sourceLine" id="cb800-14" data-line-number="14">df =<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb800-15" data-line-number="15">  <span class="dt">span25 =</span> <span class="kw">predict</span>(ex1.ls, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">x=</span>xseq)),</a>
<a class="sourceLine" id="cb800-16" data-line-number="16">  <span class="dt">span50 =</span> <span class="kw">predict</span>(ex2.ls, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">x=</span>xseq)),</a>
<a class="sourceLine" id="cb800-17" data-line-number="17">  <span class="dt">span75 =</span> <span class="kw">predict</span>(ex3.ls, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">x=</span>xseq))</a>
<a class="sourceLine" id="cb800-18" data-line-number="18">  )</a>
<a class="sourceLine" id="cb800-19" data-line-number="19"></a>
<a class="sourceLine" id="cb800-20" data-line-number="20"><span class="kw">ggplot</span>(df) <span class="op">+</span></a>
<a class="sourceLine" id="cb800-21" data-line-number="21"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(x, y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb800-22" data-line-number="22"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xseq, span25, <span class="dt">linetype=</span><span class="st">&#39;span = 0.25&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb800-23" data-line-number="23"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xseq, span50, <span class="dt">linetype=</span><span class="st">&#39;span = 0.50&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb800-24" data-line-number="24"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>xseq, span75, <span class="dt">linetype=</span><span class="st">&#39;span = 0.75&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb800-25" data-line-number="25"><span class="st">  </span><span class="kw">scale_linetype_manual</span>(<span class="dt">name=</span><span class="st">&quot;Legend&quot;</span>, <span class="dt">values=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb800-26" data-line-number="26"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Loess Smoother Example&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb800-27" data-line-number="27"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>From this plot, a span of 0.75 provided too much smoothness, whereas the lower values of span we tested appear to be a better fit. Now lets apply this to the <code>airqaulity</code> data set from the previous chapter. Initially, well just consider the response(<code>Ozone</code>) and one predictor (<code>Solar.R</code>).</p>
<div class="sourceCode" id="cb801"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb801-1" data-line-number="1">aq1.ls =<span class="st"> </span><span class="kw">loess</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">span=</span><span class="fl">0.25</span>, <span class="dt">data=</span>airquality)</a>
<a class="sourceLine" id="cb801-2" data-line-number="2">aq2.ls =<span class="st"> </span><span class="kw">loess</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">span=</span><span class="fl">0.5</span>, <span class="dt">data=</span>airquality)</a>
<a class="sourceLine" id="cb801-3" data-line-number="3">aq3.ls =<span class="st"> </span><span class="kw">loess</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">span=</span><span class="fl">0.75</span>, <span class="dt">data=</span>airquality)</a>
<a class="sourceLine" id="cb801-4" data-line-number="4">srseq =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">10</span>, <span class="dv">330</span>, <span class="dt">length=</span><span class="kw">nrow</span>(airquality))</a>
<a class="sourceLine" id="cb801-5" data-line-number="5"></a>
<a class="sourceLine" id="cb801-6" data-line-number="6">aq =<span class="st"> </span>airquality <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb801-7" data-line-number="7">  <span class="dt">span25 =</span> <span class="kw">predict</span>(aq1.ls, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">Solar.R=</span>srseq)),</a>
<a class="sourceLine" id="cb801-8" data-line-number="8">  <span class="dt">span50 =</span> <span class="kw">predict</span>(aq2.ls, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">Solar.R=</span>srseq)),</a>
<a class="sourceLine" id="cb801-9" data-line-number="9">  <span class="dt">span75 =</span> <span class="kw">predict</span>(aq3.ls, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">Solar.R=</span>srseq))</a>
<a class="sourceLine" id="cb801-10" data-line-number="10">  )</a>
<a class="sourceLine" id="cb801-11" data-line-number="11"></a>
<a class="sourceLine" id="cb801-12" data-line-number="12"><span class="kw">ggplot</span>(aq) <span class="op">+</span></a>
<a class="sourceLine" id="cb801-13" data-line-number="13"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(Solar.R, Ozone)) <span class="op">+</span></a>
<a class="sourceLine" id="cb801-14" data-line-number="14"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>srseq, span25, <span class="dt">linetype=</span><span class="st">&#39;span = 0.25&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb801-15" data-line-number="15"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>srseq, span50, <span class="dt">linetype=</span><span class="st">&#39;span = 0.50&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb801-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>srseq, span75, <span class="dt">linetype=</span><span class="st">&#39;span = 0.75&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb801-17" data-line-number="17"><span class="st">  </span><span class="kw">scale_linetype_manual</span>(<span class="dt">name=</span><span class="st">&quot;Legend&quot;</span>, <span class="dt">values=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb801-18" data-line-number="18"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Loess Smoother Example&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb801-19" data-line-number="19"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Here we can see that the higher span values appear to provide a better fit. In this case, choosing a low span value would be akin to over fitting a linear model with too high of a degree of polynomial. We can repeat this process to determine appropriate values of span for the other predictors.</p>
<p>Including loess smoothers in a GAM is as simple as including the non-linear terms within <code>lo()</code>. The <code>gam</code> package provides the needed functionality. The script below applies loess smoothers to three of the predictors and displays the model summary (note that the default value for span is 0.5).</p>
<div class="sourceCode" id="cb802"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb802-1" data-line-number="1"><span class="kw">library</span>(gam)</a>
<a class="sourceLine" id="cb802-2" data-line-number="2"></a>
<a class="sourceLine" id="cb802-3" data-line-number="3">aq.gam =<span class="st"> </span><span class="kw">gam</span>(Ozone <span class="op">~</span><span class="st"> </span><span class="kw">lo</span>(Solar.R, <span class="dt">span=</span><span class="fl">0.75</span>) <span class="op">+</span><span class="st"> </span><span class="kw">lo</span>(Wind) <span class="op">+</span><span class="st"> </span><span class="kw">lo</span>(Temp), <span class="dt">data=</span>airquality, <span class="dt">na=</span>na.gam.replace)</a>
<a class="sourceLine" id="cb802-4" data-line-number="4"><span class="kw">summary</span>(aq.gam)</a></code></pre></div>
<pre><code>## 
## Call: gam(formula = Ozone ~ lo(Solar.R, span = 0.75) + lo(Wind) + lo(Temp), 
##     data = airquality, na.action = na.gam.replace)
## Deviance Residuals:
##     Min      1Q  Median      3Q     Max 
## -47.076  -9.601  -2.721   8.977  76.583 
## 
## (Dispersion Parameter for gaussian family taken to be 319.3603)
## 
##     Null Deviance: 125143.1 on 115 degrees of freedom
## Residual Deviance: 33679.85 on 105.4604 degrees of freedom
## AIC: 1010.117 
## 
## Number of Local Scoring Iterations: NA 
## 
## Anova for Parametric Effects
##                              Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## lo(Solar.R, span = 0.75)   1.00  14248   14248  44.615 1.160e-09 ***
## lo(Wind)                   1.00  35734   35734 111.894 &lt; 2.2e-16 ***
## lo(Temp)                   1.00  15042   15042  47.099 4.794e-10 ***
## Residuals                105.46  33680     319                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Anova for Nonparametric Effects
##                          Npar Df Npar F     Pr(F)    
## (Intercept)                                          
## lo(Solar.R, span = 0.75)     1.2 2.9766 0.0804893 .  
## lo(Wind)                     2.8 9.2752 2.617e-05 ***
## lo(Temp)                     2.5 6.8089 0.0006584 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb804"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb804-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb804-2" data-line-number="2"><span class="kw">plot</span>(aq.gam, <span class="dt">se=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-10-1.png" width="960" /></p>
</div>
<div id="splines" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Splines</h3>
<p>Spline smoothing can be conceptualized by imagining that your task is to bend a strip of soft metal into a curved shape. One way to do this would be to place pegs on a board (referred to as knots in non-linear regression parlance) to control the bends, and then guide the strip of metal over and under the pegs. Mathematically, this is accomplished by combining cubic regression at each knot with calculus to smoothly join the individual bends. The tuning parameter in the <code>smooth.splines</code> function is <code>spar</code>.</p>
<div class="sourceCode" id="cb805"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb805-1" data-line-number="1">aq =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop_na</span>()</a>
<a class="sourceLine" id="cb805-2" data-line-number="2"></a>
<a class="sourceLine" id="cb805-3" data-line-number="3">ss25 =<span class="st"> </span><span class="kw">smooth.spline</span>(aq<span class="op">$</span>Solar.R,aq<span class="op">$</span>Ozone,<span class="dt">spar=</span><span class="fl">0.25</span>)</a>
<a class="sourceLine" id="cb805-4" data-line-number="4">ss50 =<span class="st"> </span><span class="kw">smooth.spline</span>(aq<span class="op">$</span>Solar.R,aq<span class="op">$</span>Ozone,<span class="dt">spar=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb805-5" data-line-number="5">ss75 =<span class="st"> </span><span class="kw">smooth.spline</span>(aq<span class="op">$</span>Solar.R,aq<span class="op">$</span>Ozone,<span class="dt">spar=</span><span class="fl">0.75</span>)</a>
<a class="sourceLine" id="cb805-6" data-line-number="6"></a>
<a class="sourceLine" id="cb805-7" data-line-number="7"><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb805-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>aq, <span class="kw">aes</span>(Solar.R, Ozone)) <span class="op">+</span></a>
<a class="sourceLine" id="cb805-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>ss25<span class="op">$</span>x, ss25<span class="op">$</span>y, <span class="dt">linetype=</span><span class="st">&#39;spar = 0.25&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb805-10" data-line-number="10"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>ss50<span class="op">$</span>x, ss50<span class="op">$</span>y, <span class="dt">linetype=</span><span class="st">&#39;spar = 0.50&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb805-11" data-line-number="11"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>ss75<span class="op">$</span>x, ss75<span class="op">$</span>y, <span class="dt">linetype=</span><span class="st">&#39;spar = 0.75&#39;</span>), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb805-12" data-line-number="12"><span class="st">  </span><span class="kw">scale_linetype_manual</span>(<span class="dt">name=</span><span class="st">&quot;Legend&quot;</span>, <span class="dt">values=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb805-13" data-line-number="13"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Spline Smoother Example&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb805-14" data-line-number="14"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="cross-validation-1" class="section level3">
<h3><span class="header-section-number">11.3.3</span> Cross Validation</h3>
<p>Comparing the spline smoother plot to the one generated with loess smoothers, we can see that the two methods essentially accomplish the same thing. Its just a matter of finding the right amount of smoothness, which can be done through cross validation. The <code>fANCOVA</code> package contains a function <code>loess.aq()</code> that includes a criterion parameter that we can set to <code>gcv</code> for generalized cross validation, which is an approximation for leave-one-out cross-validation <span class="citation">Trevor Hastie and Friedman (<a href="#ref-hastie2008">2008</a>)</span>. Applying this function to the <code>airquality</code> data with <code>Solar.R</code> as the predictor and <code>Ozone</code> as the response, we can obtain a cross validated value for span.</p>
<div class="sourceCode" id="cb806"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb806-1" data-line-number="1"><span class="kw">library</span>(fANCOVA)</a></code></pre></div>
<pre><code>## fANCOVA 0.6-1 loaded</code></pre>
<div class="sourceCode" id="cb808"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb808-1" data-line-number="1">aq.solar.cv =<span class="st"> </span><span class="kw">loess.as</span>(aq<span class="op">$</span>Solar.R, aq<span class="op">$</span>Ozone, <span class="dt">criterion=</span><span class="st">&quot;gcv&quot;</span>)</a>
<a class="sourceLine" id="cb808-2" data-line-number="2"><span class="kw">summary</span>(aq.solar.cv)</a></code></pre></div>
<pre><code>## Call:
## loess(formula = y ~ x, data = data.bind, span = span1, degree = degree, 
##     family = family)
## 
## Number of Observations: 111 
## Equivalent Number of Parameters: 3.09 
## Residual Standard Error: 29.42 
## Trace of smoother matrix: 3.56  (exact)
## 
## Control settings:
##   span     :  0.6991628 
##   degree   :  1 
##   family   :  gaussian
##   surface  :  interpolate      cell = 0.2
##   normalize:  TRUE
##  parametric:  FALSE
## drop.square:  FALSE</code></pre>
<p><code>loess.as</code> also includes a plot method so we can visualize the loess smoother.</p>
<div class="sourceCode" id="cb810"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb810-1" data-line-number="1"><span class="kw">loess.as</span>(aq<span class="op">$</span>Solar.R, aq<span class="op">$</span>Ozone, <span class="dt">criterion=</span><span class="st">&quot;gcv&quot;</span>, <span class="dt">plot=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre><code>## Call:
## loess(formula = y ~ x, data = data.bind, span = span1, degree = degree, 
##     family = family)
## 
## Number of Observations: 111 
## Equivalent Number of Parameters: 3.09 
## Residual Standard Error: 29.42</code></pre>
<p>Cross validation is also built in to <code>smooth.spline()</code> and is set to generalized cross validation by default. Instead of specifying <code>spar</code> in the call to <code>smooth.spline()</code>, we just leave it out to invoke cross validation.</p>
<div class="sourceCode" id="cb812"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb812-1" data-line-number="1">aq.spl =<span class="st"> </span><span class="kw">smooth.spline</span>(aq<span class="op">$</span>Solar.R, aq<span class="op">$</span>Ozone)</a>
<a class="sourceLine" id="cb812-2" data-line-number="2">aq.spl</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = aq$Solar.R, y = aq$Ozone)
## 
## Smoothing Parameter  spar= 0.9837718  lambda= 0.01867197 (12 iterations)
## Equivalent Degrees of Freedom (Df): 4.060081
## Penalized Criterion (RSS): 66257.74
## GCV: 892.29</code></pre>
<p>Plotting the cross validated spline smoother, we get a line that looks very similar to the lasso smoother.</p>
<div class="sourceCode" id="cb814"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb814-1" data-line-number="1"><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb814-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>aq, <span class="kw">aes</span>(Solar.R, Ozone)) <span class="op">+</span></a>
<a class="sourceLine" id="cb814-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>aq.spl<span class="op">$</span>x, aq.spl<span class="op">$</span>y), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb814-4" data-line-number="4"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;CV Spline Smoother&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb814-5" data-line-number="5"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="gam-problem-set" class="section level3">
<h3><span class="header-section-number">11.3.4</span> GAM Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter11_ProblemSets/Loess_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter11_ProblemSets/Loess_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter11_ProblemSets/Loess_PS_Solutions.html'>here</a>.</p>
</div>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">11.4</span> Support Vector Machines</h2>
<p>If you have already been introduced to support vector machines (SVM), chances are that the methodology was applied to a classification problem, which is referred to as support vector classification (SVC). Support vector regression (SVR) is closely related to SVC but is used for linear and non-linear regression problems. Well begin with regression, and then move to classification.</p>
<div id="support-vector-regression" class="section level3">
<h3><span class="header-section-number">11.4.1</span> Support Vector Regression</h3>
<p>SVR attempts to include as many data points as possible in the area between two lines. The following figure demonstrates this using dummy data with a linear relationship. The two parallel lines are the <strong>margin</strong>, and its width is a hyperparameter <span class="math inline">\(\varepsilon\)</span> that we can tune. If you draw a line through one of the points that fall outside the margin so that it is perpendicular to the margin, you have a <strong>support vector</strong>. A <strong>cost</strong> is applied to each point that falls outside the margin, and minimizing the cost determines the slope of the margin. Cost is another tunable hyperparameter, which is sometimes represented as <span class="math inline">\(1/\lambda\)</span>. Notice that unlike linear regression, if we were to add more points inside the margin, it would have no impact on the slope. SVR is also much less influence by outliers than linear regression. For the mathematical details behind SVR, refer to Section 12.3.6 in <span class="citation">Trevor Hastie and Friedman (<a href="#ref-hastie2008">2008</a>)</span>.</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Choosing values for the hyperparameters <span class="math inline">\(\varepsilon\)</span> and <span class="math inline">\(\lambda\)</span> is once again done through cross validation. To do this in <em>R</em>, well use some functions from the <code>e1071</code> package (another option is the <code>LiblineaR</code> package). Before we get to cross validation, lets just look at how to build an SVR model. The syntax is the same as for linear models, we just replace <code>lm()</code> with <code>svm()</code>. Note that the function is not <code>svr()</code> because the function can do both regression and classification. To make this more interesting, well switch back to the <code>airquality</code> data. From the model summary below, <code>SVM-type:  eps-regression</code> tells us that the function is performing regression and not classification, then we see the hyperparameter values and the number of support vectors used to fit the model.</p>
<p>For the kernel, we have four choices: linear, polynomial, radial basis, and sigmoid. Selecting a linear kernel will force a straight line fit, and the other three kernels are different methods for adding curvature to the regression line<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a>. The theory behind SVR kernels is beyond the scope of this tutorial, but if you want to dig deeper:</p>
<ul>
<li><p>Here are some slides titled <a href="http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf">SVM dual, kernels and regression</a> from The University of Oxford.</p></li>
<li><p>Heres <a href="http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf">An Idiots Guide to Support Vector Machines</a>, a catchy title from MIT.</p></li>
<li><p>Heres post titled <a href="https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d">Support Vector Machine: Kernel Trick; Mercers Theorem</a> at towardsdatascience.com.</p></li>
</ul>
<p>For our purposes, we just need to know that the three non-linear kernels have <code>gamma</code> as a hyperparameter that controls curvature.</p>
<p>To force a straight regression line, specify <code>kernel='linear'</code>. Also, the <code>svm()</code> by default scales all variables in the data set to have a mean of zero and equal variance. Scaling the variables will improve the models performance, but well turn that off in this example so we can directly compare the coefficients to those produced by <code>lm()</code>.</p>
<div class="sourceCode" id="cb815"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb815-1" data-line-number="1"><span class="kw">library</span>(e1071)</a>
<a class="sourceLine" id="cb815-2" data-line-number="2"></a>
<a class="sourceLine" id="cb815-3" data-line-number="3">aq.svm =<span class="st"> </span><span class="kw">svm</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">data=</span>aq, <span class="dt">kernel=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">scale=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb815-4" data-line-number="4"><span class="kw">summary</span>(aq.svm)</a></code></pre></div>
<pre><code>## 
## Call:
## svm(formula = Ozone ~ Solar.R, data = aq, kernel = &quot;linear&quot;, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  eps-regression 
##  SVM-Kernel:  linear 
##        cost:  1 
##       gamma:  1 
##     epsilon:  0.1 
## 
## 
## Number of Support Vectors:  110</code></pre>
<p>We can then extract the coefficients with <code>coef()</code>.</p>
<div class="sourceCode" id="cb817"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb817-1" data-line-number="1">(<span class="dt">coeffs =</span> <span class="kw">coef</span>(aq.svm))</a></code></pre></div>
<pre><code>## (Intercept)     Solar.R 
## 12.52321429  0.09107143</code></pre>
<p>Using <code>lm()</code>, we get the following coefficients.</p>
<div class="sourceCode" id="cb819"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb819-1" data-line-number="1">aq.lm =<span class="st"> </span><span class="kw">lm</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">data=</span>aq)</a>
<a class="sourceLine" id="cb819-2" data-line-number="2"><span class="kw">summary</span>(aq.lm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Solar.R, data = aq)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -48.292 -21.361  -8.864  16.373 119.136 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 18.59873    6.74790   2.756 0.006856 ** 
## Solar.R      0.12717    0.03278   3.880 0.000179 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 31.33 on 109 degrees of freedom
## Multiple R-squared:  0.1213, Adjusted R-squared:  0.1133 
## F-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793</code></pre>
<p>The coefficients produced by the two models might seem fairly different. The following plot shows the data with the two regression lines for comparison. Notice how the linear model is more influenced by the extreme high ozone values (possible outliers).</p>
<div class="sourceCode" id="cb821"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb821-1" data-line-number="1"><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb821-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> aq, <span class="kw">aes</span>(<span class="dt">x=</span>Solar.R, <span class="dt">y=</span>Ozone)) <span class="op">+</span></a>
<a class="sourceLine" id="cb821-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope=</span>coeffs[<span class="dv">2</span>], <span class="dt">intercept=</span>coeffs[<span class="dv">1</span>], <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb821-4" data-line-number="4"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="dv">315</span>, <span class="dt">y=</span><span class="dv">50</span>, <span class="dt">label=</span><span class="st">&quot;svm()&quot;</span>, <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb821-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope=</span>aq.lm<span class="op">$</span>coefficients[<span class="dv">2</span>], <span class="dt">intercept=</span>aq.lm<span class="op">$</span>coefficients[<span class="dv">1</span>], <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb821-6" data-line-number="6"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="dv">315</span>, <span class="dt">y=</span><span class="dv">70</span>, <span class="dt">label=</span><span class="st">&quot;lm()&quot;</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb821-7" data-line-number="7"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Now well re-fit the model with a non-linear regression line and invoking scaling. To extract the predicted response, we use the <code>predict()</code> function just like with linear models. Plotting the predicted response gives is the following.</p>
<div class="sourceCode" id="cb822"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb822-1" data-line-number="1">aq.svm2 =<span class="st"> </span><span class="kw">svm</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">data=</span>aq)</a>
<a class="sourceLine" id="cb822-2" data-line-number="2"></a>
<a class="sourceLine" id="cb822-3" data-line-number="3">aq =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">svrY =</span> <span class="kw">predict</span>(aq.svm2, <span class="dt">data=</span>aq))</a>
<a class="sourceLine" id="cb822-4" data-line-number="4"></a>
<a class="sourceLine" id="cb822-5" data-line-number="5"><span class="kw">ggplot</span>(aq) <span class="op">+</span></a>
<a class="sourceLine" id="cb822-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(Solar.R, Ozone), <span class="dt">color=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb822-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(Solar.R, svrY), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb822-8" data-line-number="8"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;SVR With Default Hyperparameters&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb822-9" data-line-number="9"><span class="st">  </span><span class="kw">coord_fixed</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb822-10" data-line-number="10"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>To tune the hyperparameters with cross validation, we can use the <code>tune</code> function from the <code>e1017</code> package. If we give the <code>tune</code> function a range of values for the hyperparameters, it will perform a grid search of those values. In the following example, were therefore fitting 100 different models. If we print the object returned from <code>tune</code>, we see that it performed 10-fold cross validation, the best hyperparameter values, and the mean squared error of the best performing model.</p>
<div class="sourceCode" id="cb823"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb823-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb823-2" data-line-number="2">aq.tune =<span class="st"> </span><span class="kw">tune.svm</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">data =</span> aq, <span class="dt">gamma=</span><span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="fl">0.1</span>), <span class="dt">cost =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">10</span>))</a>
<a class="sourceLine" id="cb823-3" data-line-number="3"><span class="kw">print</span>(aq.tune)</a></code></pre></div>
<pre><code>## 
## Parameter tuning of &#39;svm&#39;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  gamma cost
##    0.1   91
## 
## - best performance: 909.1502</code></pre>
<p>We can visualize the tune results as well by printing the <code>aq.tune</code> object. Here we see the range of cost and epsilon values with their associated mean squared error. The lower the error, the better, and those are indicated by the darkest blue regions.</p>
<div class="sourceCode" id="cb825"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb825-1" data-line-number="1"><span class="kw">plot</span>(aq.tune)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>I prefer to choose a wide range of tuning parameter values initially, and then do a finer search in the area with the lowest error. It looks like we need a low gamma and a high cost.</p>
<div class="sourceCode" id="cb826"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb826-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb826-2" data-line-number="2">aq.tune =<span class="st"> </span><span class="kw">tune.svm</span>(Ozone <span class="op">~</span><span class="st"> </span>Solar.R, <span class="dt">data =</span> aq, <span class="dt">gamma=</span><span class="kw">seq</span>(<span class="fl">0.02</span>, <span class="fl">0.22</span>, <span class="fl">0.05</span>), <span class="dt">cost =</span> <span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">100</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb826-3" data-line-number="3"><span class="kw">print</span>(aq.tune)</a></code></pre></div>
<pre><code>## 
## Parameter tuning of &#39;svm&#39;:
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  gamma cost
##   0.22   96
## 
## - best performance: 907.4115</code></pre>
<p>The best model from the tuning call can be obtained with <code>aq.tune$best.model</code>, and we can then apply the <code>predict</code> function to get the best fit regression.</p>
<div class="sourceCode" id="cb828"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb828-1" data-line-number="1">aq<span class="op">$</span>svrY =<span class="st"> </span><span class="kw">predict</span>(aq.tune<span class="op">$</span>best.model, <span class="dt">data=</span>aq)</a>
<a class="sourceLine" id="cb828-2" data-line-number="2"></a>
<a class="sourceLine" id="cb828-3" data-line-number="3"><span class="kw">ggplot</span>(aq) <span class="op">+</span></a>
<a class="sourceLine" id="cb828-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(Solar.R, Ozone), <span class="dt">color=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb828-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(Solar.R, svrY), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb828-6" data-line-number="6"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;SVR With Tuned Hyperparameters&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb828-7" data-line-number="7"><span class="st">  </span><span class="kw">coord_fixed</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb828-8" data-line-number="8"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="support-vector-classification" class="section level3">
<h3><span class="header-section-number">11.4.2</span> Support Vector Classification</h3>
<p>Classification problems have either a binary or categorical response variable. To demonstrate how SVC works, well start with the <code>iris</code> data set, which contains four predictors and one categorical response variable. Plotting petal length versus petal width for the setosa and versicolor species shows that the two species are <strong>linearly separable</strong>, meaning we can draw a straight line on the plot that completely separates the two species. If we want to train an SVC to make predictions on new data, the question becomes: how do we draw the line that separates the data? There are infinitely many options, three of which are shown on the plot.</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Support vector classification uses margins, but in a different way than SVR, to find a line that separates the data. If you think of the two parallel margin lines as a street, the idea is that we want to fit the widest possible street between the species because doing so results in the rest of the data points being as far off the street as possible. The two points below that fall on the margin determine the location of the support vectors.</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>What happens when two categories arent linearly separable, as is the case when we look at versicolor and virginica below?</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>We still want to draw two parallel lines through the data sets, but the only way to do it is to have some observations in the middle of the street, or even on the wrong side of the line (called <strong>margin violations</strong>). We still want to fit as wide of a street as possible through the data points, but now we must also limit the number of margin violations. As with SVR, we can assign a <strong>cost</strong> for each margin violation. Since margin violations are generally bad, we might be tempted to apply a large cost; however, we must also consider how well the model will generalize. Below are the linear boundaries for two choices of cost. Support vectors are based on the points surrounded by black.</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-29-1.png" width="768" /></p>
<p>Interestingly, the margins (and therefore the decision boundary) dont have to be straight lines. SVC also accommodates a curved boundary as in the example below. With a polynomial kernel, the curvature is controlled by the degree of the polynomial. In the plot, note that the support vectors are the <code>X</code> points.</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<div id="example-in-r" class="section level4">
<h4><span class="header-section-number">11.4.2.1</span> Example In <em>R</em></h4>
<p>In this section, well walk through an example using the full <code>iris</code> data set. First, well split the data set into a training set that includes 80% of the data, and a test set with the remaining 20% using the <code>caTools</code> package.</p>
<div class="sourceCode" id="cb829"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb829-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb829-2" data-line-number="2">train =<span class="st"> </span>caTools<span class="op">::</span><span class="kw">sample.split</span>(iris, <span class="dt">SplitRatio =</span> <span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb829-3" data-line-number="3">iris_train =<span class="st"> </span><span class="kw">subset</span>(iris, train <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb829-4" data-line-number="4">iris_test =<span class="st"> </span><span class="kw">subset</span>(iris, train <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>)</a></code></pre></div>
<p>Next, well tune two models using a linear kernel and a radial basis function (which allows for curvature). Well tune both models over a range of gamma and cost values.</p>
<div class="sourceCode" id="cb830"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb830-1" data-line-number="1">iris.lin =<span class="st"> </span><span class="kw">tune.svm</span>(Species<span class="op">~</span>., <span class="dt">data=</span>iris_train, </a>
<a class="sourceLine" id="cb830-2" data-line-number="2">                    <span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>, </a>
<a class="sourceLine" id="cb830-3" data-line-number="3">                    <span class="dt">gamma =</span> <span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="fl">0.1</span>), </a>
<a class="sourceLine" id="cb830-4" data-line-number="4">                    <span class="dt">cost =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">10</span>))</a>
<a class="sourceLine" id="cb830-5" data-line-number="5"></a>
<a class="sourceLine" id="cb830-6" data-line-number="6">iris.rbf =<span class="st"> </span><span class="kw">tune.svm</span>(Species<span class="op">~</span>., <span class="dt">data=</span>iris_train, </a>
<a class="sourceLine" id="cb830-7" data-line-number="7">                    <span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>, </a>
<a class="sourceLine" id="cb830-8" data-line-number="8">                    <span class="dt">gamma =</span> <span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="fl">0.1</span>), </a>
<a class="sourceLine" id="cb830-9" data-line-number="9">                    <span class="dt">cost =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">10</span>))</a>
<a class="sourceLine" id="cb830-10" data-line-number="10"></a>
<a class="sourceLine" id="cb830-11" data-line-number="11">iris.lin<span class="op">$</span>best.model</a></code></pre></div>
<pre><code>## 
## Call:
## best.svm(x = Species ~ ., data = iris_train, gamma = seq(0.1, 1, 
##     0.1), cost = seq(1, 100, 10), kernel = &quot;linear&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
## 
## Number of Support Vectors:  25</code></pre>
<div class="sourceCode" id="cb832"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb832-1" data-line-number="1">iris.rbf<span class="op">$</span>best.model</a></code></pre></div>
<pre><code>## 
## Call:
## best.svm(x = Species ~ ., data = iris_train, gamma = seq(0.1, 1, 
##     0.1), cost = seq(1, 100, 10), kernel = &quot;radial&quot;)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
## 
## Number of Support Vectors:  48</code></pre>
<p>Both models are using a low cost, but the radial basis function model has twice as many support vectors. To compare model performance, well make predictions using the test set and display each models <strong>confusion matrix</strong> using the <code>cvms</code> package (note: we could also create a simple confusion matrix with <code>table(iris_test[, 5], predictions)</code>).</p>
<div class="sourceCode" id="cb834"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb834-1" data-line-number="1"><span class="co"># get the confusion matrix for the linear kernel</span></a>
<a class="sourceLine" id="cb834-2" data-line-number="2">lin_conf_mat =<span class="st"> </span>cvms<span class="op">::</span><span class="kw">confusion_matrix</span>(</a>
<a class="sourceLine" id="cb834-3" data-line-number="3">  <span class="dt">targets =</span> iris_test[, <span class="dv">5</span>], </a>
<a class="sourceLine" id="cb834-4" data-line-number="4">  <span class="dt">predictions =</span> <span class="kw">predict</span>(iris.lin<span class="op">$</span>best.model, <span class="dt">type =</span> <span class="st">&#39;response&#39;</span>, <span class="dt">newdata =</span> iris_test[<span class="op">-</span><span class="dv">5</span>]))</a>
<a class="sourceLine" id="cb834-5" data-line-number="5"></a>
<a class="sourceLine" id="cb834-6" data-line-number="6"><span class="co"># get the confusion matrix for the radial kernel</span></a>
<a class="sourceLine" id="cb834-7" data-line-number="7">rbf_conf_mat =<span class="st"> </span>cvms<span class="op">::</span><span class="kw">confusion_matrix</span>(</a>
<a class="sourceLine" id="cb834-8" data-line-number="8">  <span class="dt">targets =</span> iris_test[, <span class="dv">5</span>],</a>
<a class="sourceLine" id="cb834-9" data-line-number="9">  <span class="dt">predictions =</span> <span class="kw">predict</span>(iris.rbf<span class="op">$</span>best.model, <span class="dt">type =</span> <span class="st">&#39;response&#39;</span>, <span class="dt">newdata =</span> iris_test[<span class="op">-</span><span class="dv">5</span>]))</a>
<a class="sourceLine" id="cb834-10" data-line-number="10"></a>
<a class="sourceLine" id="cb834-11" data-line-number="11"><span class="co"># plot the confusion matrix for the linear kernel (it&#39;s a ggplot2 object!)</span></a>
<a class="sourceLine" id="cb834-12" data-line-number="12">cvms<span class="op">::</span><span class="kw">plot_confusion_matrix</span>(lin_conf_mat<span class="op">$</span><span class="st">`</span><span class="dt">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Linear Kernel&quot;</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>The SVC model with the linear kernel did a great job! Of the 30 observations in the test set, only two were incorrectly classified. If this is the first time youve seen a confusion matrix, then what you see are the target (or actual) species by column and the species predictions from the SVC by row. In each cell, we see the percent and count of the total observations that fell into that cell. From this plot, we can identify true positives, false positives, etc. using the following guide.</p>
<p><span class="math inline">\(~\)</span></p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;
  padding:10px 5px;word-break:normal;}
.tg th{border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-18eh{border-color:#000000;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-wp8o{border-color:#000000;text-align:center;vertical-align:top}
.tg .tg-xs2q{border-color:#000000;font-size:medium;text-align:center;vertical-align:middle}
.tg .tg-mqa1{border-color:#000000;font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-1tol{border-color:#000000;font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 354px">
<colgroup>
<col style="width: 109px">
<col style="width: 54px">
<col style="width: 95px">
<col style="width: 95px">
</colgroup>
<tbody>
<tr>
<td class="tg-xs2q" colspan="2" rowspan="2">
<span style="font-weight:bold">Confusion</span><br><span style="font-weight:bold">Matrix</span>
</td>
<td class="tg-mqa1" colspan="2">
Target
</td>
</tr>
<tr>
<td class="tg-mqa1">
Yes
</td>
<td class="tg-mqa1">
No
</td>
</tr>
<tr>
<td class="tg-1tol" rowspan="2">
Prediction
</td>
<td class="tg-18eh">
Yes
</td>
<td class="tg-wp8o">
True<br>Positive
</td>
<td class="tg-wp8o">
False<br>Positive
</td>
</tr>
<tr>
<td class="tg-18eh">
No
</td>
<td class="tg-wp8o">
False<br>Negative
</td>
<td class="tg-wp8o">
True<br>Positive
</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(~\)</span></p>
<p>A perfect classifier will have zeros everywhere in the table except the diagonal. In our case, its close to perfect. We just have two false negatives because two flowers that were actually virginica, were predicted to be versicolor. Now lets look at the radial kernel results.</p>
<div class="sourceCode" id="cb835"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb835-1" data-line-number="1">cvms<span class="op">::</span><span class="kw">plot_confusion_matrix</span>(rbf_conf_mat<span class="op">$</span><span class="st">`</span><span class="dt">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Radial Kernel&quot;</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
</div>
</div>
<div id="svm-problem-set" class="section level3">
<h3><span class="header-section-number">11.4.3</span> SVM Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter11_ProblemSets/SVM_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter11_ProblemSets/SVM_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter11_ProblemSets/SVM_PS_Solutions.html'>here</a>.</p>
</div>
</div>
<div id="classification-and-regression-trees" class="section level2">
<h2><span class="header-section-number">11.5</span> Classification and Regression Trees</h2>
<p>As with support vector machines, and as the name implies, classification and regression trees<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> (CART) can be used for either classification or regression tasks. Again, well start with regression and then move to classification.</p>
<div id="regression-trees" class="section level3">
<h3><span class="header-section-number">11.5.1</span> Regression Trees</h3>
<p>The algorithm is best explained as we walk through an example, and well continue to use the <code>airquality</code> data set. The basic machine learning algorithm used in tree-based methods follows these steps:</p>
<ol style="list-style-type: decimal">
<li><p>Consider the entire data set including all predictors and the response. We call this the <strong>root node</strong>, and it is represented by the top center node in the figure below. The information displayed in the node includes the mean response for that node (42.1 is the mean of <code>Ozone</code> for the whole data set), the number of observations in the node (<code>n=116</code>), and the percent of the overall observations in the node.</p></li>
<li><p>Iterate through each predictor, <span class="math inline">\(k\)</span>, and split the data into two subsets (referred to as the left and right <strong>child nodes</strong>) using some threshold, <span class="math inline">\(t_k\)</span>. For example, with the <code>airquality</code> data set, the predictor and threshold could be <code>Temp &gt;= 83</code>. The choice of <span class="math inline">\(k\)</span> and <span class="math inline">\(t_k\)</span> for a given split is the pair that increases the purity of the child nodes (weighted by their size) the most. Well explicitly define purity shortly. If you equate a data split with a decision, then at this point, we have a basic decision tree.</p></li>
</ol>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<ol start="3" style="list-style-type: decimal">
<li>Each child node in turn becomes the new parent node and the process is repeated. Below is the decision tree produced by the first two splits. Notice that the first split is on the <code>Temp</code> predictor, and the second split is on the <code>Wind</code> predictor. Although we dont have coefficients for these two predictors like we would in a linear model, we can still interpret the order of the splits as the predictors relative significance. In this case, <code>Temp</code> is the most significant predictor of <code>Ozone</code> followed by <code>Wind</code>. After two splits, the decision tree has three <strong>leaf nodes</strong>, which are those in the bottom row. We can also define the <strong>depth</strong> of the tree as the number rows in the tree below the root node (in this case depth = 2). Note that the sum of the observations in the leaf nodes equals the total number of observations (69 + 10 + 37 = 116), and so the percentages shown in the leaf nodes sum to 100%.</li>
</ol>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Continuing the process once more, we see that the third split is again on <code>Temp</code> but at a different <span class="math inline">\(t_k\)</span>.</li>
</ol>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>If we continued to repeat the process until each observation was in its own node, then we would have drastically over-fit the model. To control over-fitting, we stop the splitting process when some user-defined condition (or set of conditions) is met. Example stopping conditions include a minimum number of observations in a node or a maximum depth of the tree. We can also use cross validation with a 1 standard error rule to limit the complexity of the final model.</p>
<p>Well stop at this point and visually represent this model as a scatter plot. The above leaves from left to right are labeled as Leaf 1 - 4 on the scatter plot.</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>Plotting predicted <code>Ozone</code> on the z-axis produces the following response surface, which highlights the step-like characteristic of regression tree predictions.</p>
<div id="htmlwidget-bd6d752361be19e012d0" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-bd6d752361be19e012d0">{"x":{"visdat":{"fba739ec979":["function () ","plotlyVisDat"],"fba5fe3a752":["function () ","data"]},"cur_data":"fba5fe3a752","attrs":{"fba739ec979":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],"y":[57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97],"z":{},"type":"surface","opacity":0.9,"showlegend":false,"inherit":true},"fba5fe3a752":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"color":"black","size":3},"showlegend":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"CART Response Surface","showlegend":false,"scene":{"xaxis":{"title":"Wind"},"yaxis":{"title":"Temp"},"zaxis":{"title":"z"}},"hovermode":"closest"},"source":"A","config":{"showSendToCloud":false},"data":[{"colorbar":{"title":"z<br />Ozone","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],"y":[57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97],"z":[[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[55.6,55.6,55.6,55.6,55.6,55.6,55.6,55.6,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333,22.3333333333333],[62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95],[62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95],[62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95],[62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95],[62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95,62.95],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118],[90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118,90.0588235294118]],"type":"surface","opacity":0.9,"showlegend":false,"frame":null},{"x":[7.4,8,12.6,11.5,8.6,13.8,20.1,9.7,9.2,10.9,13.2,11.5,12,18.4,11.5,9.7,9.7,16.6,9.7,12,12,14.9,5.7,7.4,9.7,13.8,11.5,8,14.9,20.7,9.2,11.5,10.3,4.1,9.2,9.2,4.6,10.9,5.1,6.3,5.7,7.4,14.3,14.9,14.3,6.9,10.3,6.3,5.1,11.5,6.9,8.6,8,8.6,12,7.4,7.4,7.4,9.2,6.9,13.8,7.4,4,10.3,8,11.5,11.5,9.7,10.3,6.3,7.4,10.9,10.3,15.5,14.3,9.7,3.4,8,9.7,2.3,6.3,6.3,6.9,5.1,2.8,4.6,7.4,15.5,10.9,10.3,10.9,9.7,14.9,15.5,6.3,10.9,11.5,6.9,13.8,10.3,10.3,8,12.6,9.2,10.3,10.3,16.6,6.9,14.3,8,11.5],"y":[67,72,74,62,65,59,61,69,66,68,58,64,66,57,68,62,59,73,61,61,67,81,79,76,82,90,87,82,77,72,65,73,76,84,85,81,83,83,88,92,92,89,73,81,80,81,82,84,87,85,74,86,85,82,86,88,86,83,81,81,81,82,89,90,90,86,82,80,77,79,76,78,78,77,72,79,81,86,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,75,76,68],"z":[41,36,12,18,23,19,8,16,11,14,18,14,34,6,30,11,1,11,4,32,23,45,115,37,29,71,39,23,21,37,20,12,13,135,49,32,64,40,77,97,97,85,10,27,7,48,35,61,79,63,16,80,108,20,52,82,50,64,59,39,9,16,122,89,110,44,28,65,22,59,23,31,44,21,9,45,168,73,76,118,84,85,96,78,73,91,47,32,20,23,21,24,44,21,28,9,13,46,18,13,24,16,13,23,36,7,14,30,14,18,20],"type":"scatter3d","mode":"markers","marker":{"color":"black","size":3,"line":{"color":"rgba(255,127,14,1)"}},"showlegend":false,"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Plotting just <code>Temp</code> versus <code>Ozone</code> in two dimensions further highlights a difference between this method and linear regression. From this plot we can infer that linear regression may outperform CART if there is a smooth trend in the relationship between the predictors and response because CART does not produce smooth estimates.</p>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div id="impurity-measure" class="section level4">
<h4><span class="header-section-number">11.5.1.1</span> Impurity Measure</h4>
<p>Previously, it was stated that the predictor-threshold pair chosen for a split was the pair that most increased the purity (or, decreased the impurity) of the child nodes. A node with all identical response values will have an impurity of 0, so that as a node becomes more impure, its impurity value increases. We will then define a nodes impurity to be proportional to the <strong>residual deviance</strong>, which for a continuous response variable like <code>Ozone</code>, is the residual sum of squares (RSS).</p>
<p><span class="math display">\[RSS = \sum\limits_{i\:in\: Node}{(y_{i} - \bar{y})^2}\]</span></p>
<p>where <span class="math inline">\(\bar{y}\)</span> is the mean of the ys in the node.</p>
<p>Well start with the first split. To determine which predictor-threshold pair decreases impurity the most, start with the first factor, send the lowest <code>Ozone</code> value to the left node and the remainder to the right node, and calculate RSS for each child node (<span class="math inline">\(RSS_{left}\)</span> and <span class="math inline">\(RSS_{right}\)</span>). The decrease in impurity for this split is <span class="math inline">\(RSS_{root} - (RSS_{left} + RSS_{right})\)</span>. Then send the lowest two <code>Ozone</code> values to the left node and the remainder to the right. Repeat this process for each predictor-threshold pair, and split the data based using the pair that decreased impurity the most. Any regression tree package will iterate through all of these combinations for you, but to demonstrate the process explicitly, Well just consider the <code>Temp</code> predictor for the first split.</p>
<div class="sourceCode" id="cb836"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb836-1" data-line-number="1"><span class="co"># we&#39;ll do a lot of filtering, so convert dataframe to tibble for convenience</span></a>
<a class="sourceLine" id="cb836-2" data-line-number="2"><span class="co"># we&#39;ll also drop the NA&#39;s for the calculations (but the regression tree</span></a>
<a class="sourceLine" id="cb836-3" data-line-number="3"><span class="co"># methodology itself doesn&#39;t care if there are NA&#39;s or not)</span></a>
<a class="sourceLine" id="cb836-4" data-line-number="4">aq  =<span class="st"> </span><span class="kw">as_tibble</span>(airquality) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop_na</span>(Ozone)</a>
<a class="sourceLine" id="cb836-5" data-line-number="5"></a>
<a class="sourceLine" id="cb836-6" data-line-number="6"><span class="co"># root node deviance</span></a>
<a class="sourceLine" id="cb836-7" data-line-number="7">root_dev =<span class="st"> </span><span class="kw">sum</span>((aq<span class="op">$</span>Ozone <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(aq<span class="op">$</span>Ozone))<span class="op">^</span><span class="dv">2</span>) </a>
<a class="sourceLine" id="cb836-8" data-line-number="8"></a>
<a class="sourceLine" id="cb836-9" data-line-number="9"><span class="co"># keep track of the highest decrease</span></a>
<a class="sourceLine" id="cb836-10" data-line-number="10">best_split =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb836-11" data-line-number="11"></a>
<a class="sourceLine" id="cb836-12" data-line-number="12"><span class="co"># iterate through all the unique Temp values</span></a>
<a class="sourceLine" id="cb836-13" data-line-number="13"><span class="cf">for</span>(s <span class="cf">in</span> <span class="kw">sort</span>(<span class="kw">unique</span>(aq<span class="op">$</span>Temp))){</a>
<a class="sourceLine" id="cb836-14" data-line-number="14">  left_node =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Temp <span class="op">&lt;=</span><span class="st"> </span>s) <span class="op">%&gt;%</span><span class="st"> </span>.<span class="op">$</span>Ozone</a>
<a class="sourceLine" id="cb836-15" data-line-number="15">  left_dev =<span class="st"> </span><span class="kw">sum</span>((left_node <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(left_node))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb836-16" data-line-number="16">  right_node =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Temp <span class="op">&gt;</span><span class="st"> </span>s) <span class="op">%&gt;%</span><span class="st"> </span>.<span class="op">$</span>Ozone</a>
<a class="sourceLine" id="cb836-17" data-line-number="17">  right_dev =<span class="st"> </span><span class="kw">sum</span>((right_node <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(right_node))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb836-18" data-line-number="18">  split_dev =<span class="st"> </span>root_dev <span class="op">-</span><span class="st"> </span>(left_dev <span class="op">+</span><span class="st"> </span>right_dev)</a>
<a class="sourceLine" id="cb836-19" data-line-number="19">  <span class="cf">if</span>(split_dev <span class="op">&gt;</span><span class="st"> </span>best_split){</a>
<a class="sourceLine" id="cb836-20" data-line-number="20">    best_split =<span class="st"> </span>split_dev</a>
<a class="sourceLine" id="cb836-21" data-line-number="21">    temp =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span><span class="dv">1</span>}  <span class="co"># + 1 because we filtered Temp &lt;= s and Temp is integer</span></a>
<a class="sourceLine" id="cb836-22" data-line-number="22">}</a>
<a class="sourceLine" id="cb836-23" data-line-number="23"></a>
<a class="sourceLine" id="cb836-24" data-line-number="24"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Best split at Temp &lt;&quot;</span>, temp), <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## [1] Best split at Temp &lt; 83</code></pre>
</div>
<div id="tree-deviance" class="section level4">
<h4><span class="header-section-number">11.5.1.2</span> Tree Deviance</h4>
<p>Armed with our impurity measure, we can also calculate the tree deviance, which well use to calculate the regression tree equivalent of <span class="math inline">\(R^2\)</span>. For the tree with the four leaf nodes, we calculate the deviance for each leaf.</p>
<div class="sourceCode" id="cb838"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb838-1" data-line-number="1"><span class="co"># leaf 1</span></a>
<a class="sourceLine" id="cb838-2" data-line-number="2">leaf_<span class="dv">1</span> =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Temp <span class="op">&lt;</span><span class="st"> </span><span class="dv">83</span> <span class="op">&amp;</span><span class="st"> </span>Wind <span class="op">&gt;=</span><span class="st"> </span><span class="fl">7.15</span>) <span class="op">%&gt;%</span><span class="st"> </span>.<span class="op">$</span>Ozone</a>
<a class="sourceLine" id="cb838-3" data-line-number="3">leaf_<span class="dv">1</span>_dev =<span class="st"> </span><span class="kw">sum</span>((leaf_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(leaf_<span class="dv">1</span>))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb838-4" data-line-number="4"><span class="co"># leaf 2</span></a>
<a class="sourceLine" id="cb838-5" data-line-number="5">leaf_<span class="dv">2</span> =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Temp <span class="op">&lt;</span><span class="st"> </span><span class="dv">83</span> <span class="op">&amp;</span><span class="st"> </span>Wind <span class="op">&lt;</span><span class="st"> </span><span class="fl">7.15</span>) <span class="op">%&gt;%</span><span class="st"> </span>.<span class="op">$</span>Ozone</a>
<a class="sourceLine" id="cb838-6" data-line-number="6">leaf_<span class="dv">2</span>_dev =<span class="st"> </span><span class="kw">sum</span>((leaf_<span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(leaf_<span class="dv">2</span>))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb838-7" data-line-number="7"><span class="co"># leaf 3</span></a>
<a class="sourceLine" id="cb838-8" data-line-number="8">leaf_<span class="dv">3</span> =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Temp <span class="op">&gt;=</span><span class="st"> </span><span class="dv">83</span> <span class="op">&amp;</span><span class="st"> </span>Temp <span class="op">&lt;</span><span class="st"> </span><span class="dv">88</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop_na</span>(Ozone) <span class="op">%&gt;%</span><span class="st"> </span>.<span class="op">$</span>Ozone</a>
<a class="sourceLine" id="cb838-9" data-line-number="9">leaf_<span class="dv">3</span>_dev =<span class="st"> </span><span class="kw">sum</span>((leaf_<span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(leaf_<span class="dv">3</span>))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb838-10" data-line-number="10"><span class="co"># leaf 4</span></a>
<a class="sourceLine" id="cb838-11" data-line-number="11">leaf_<span class="dv">4</span> =<span class="st"> </span>aq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Temp <span class="op">&gt;=</span><span class="st"> </span><span class="dv">88</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop_na</span>(Ozone) <span class="op">%&gt;%</span><span class="st"> </span>.<span class="op">$</span>Ozone</a>
<a class="sourceLine" id="cb838-12" data-line-number="12">leaf_<span class="dv">4</span>_dev =<span class="st"> </span><span class="kw">sum</span>((leaf_<span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(leaf_<span class="dv">4</span>))<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<p>The tree deviance is the sum of the leaf node deviances, which we use to determine how much the entire tree decreases the root deviance.</p>
<div class="sourceCode" id="cb839"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb839-1" data-line-number="1">tree_dev =<span class="st"> </span><span class="kw">sum</span>(leaf_<span class="dv">1</span>_dev, leaf_<span class="dv">2</span>_dev, leaf_<span class="dv">3</span>_dev, leaf_<span class="dv">4</span>_dev)</a>
<a class="sourceLine" id="cb839-2" data-line-number="2"></a>
<a class="sourceLine" id="cb839-3" data-line-number="3">(root_dev <span class="op">-</span><span class="st"> </span>tree_dev) <span class="op">/</span><span class="st"> </span>root_dev</a></code></pre></div>
<pre><code>## [1] 0.6119192</code></pre>
<p>The tree decreases the root deviance by 61.2%, which also means that 61.2% of the variability in <code>Ozone</code> is explained by the tree.</p>
</div>
<div id="prediction" class="section level4">
<h4><span class="header-section-number">11.5.1.3</span> Prediction</h4>
<p>Making a prediction with a new value is easy as following the logic of the decision tree until you end up in a leaf node. The mean of the response values for that leaf node is the prediction for the new value.</p>
</div>
<div id="pros-and-cons" class="section level4">
<h4><span class="header-section-number">11.5.1.4</span> Pros And Cons</h4>
<p>Regression trees have a lot of good things going for them:</p>
<ul>
<li>They are easy to explain combined with an intuitive graphic output</li>
<li>They can handle categorical and numeric predictor and response variables</li>
<li>They easily handle missing data</li>
<li>They are robust to outliers</li>
<li>They make no assumptions about normality</li>
<li>They can accommodate wide data (more predictors than observations)</li>
<li>They automatically include interactions</li>
</ul>
<p>Regression trees by themselves and as presented so far have two major drawbacks:</p>
<ul>
<li>They do not tend to perform as well as other methods (but theres a plan for this that makes them one of the best prediction methods around)</li>
<li>They do not capture simple additive structure (theres a plan for this, too)</li>
</ul>
</div>
<div id="regression-trees-in-r" class="section level4">
<h4><span class="header-section-number">11.5.1.5</span> Regression Trees in <em>R</em></h4>
<p>The regression trees shown above were grown using the <code>rpart</code> and <code>rpart.plot</code> packages. I didnt show the code so that we could focus on the algorithm first. Growing a regression tree is as easy as a linear model. The object created by <code>rpart()</code> contains some useful information.</p>
<div class="sourceCode" id="cb841"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb841-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb841-2" data-line-number="2"><span class="kw">library</span>(rpart.plot)</a>
<a class="sourceLine" id="cb841-3" data-line-number="3"></a>
<a class="sourceLine" id="cb841-4" data-line-number="4">aq.tree =<span class="st"> </span><span class="kw">rpart</span>(Ozone <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>airquality)</a>
<a class="sourceLine" id="cb841-5" data-line-number="5"></a>
<a class="sourceLine" id="cb841-6" data-line-number="6">aq.tree</a></code></pre></div>
<pre><code>## n=116 (37 observations deleted due to missingness)
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 116 125143.1000 42.12931  
##    2) Temp&lt; 82.5 79  42531.5900 26.54430  
##      4) Wind&gt;=7.15 69  10919.3300 22.33333  
##        8) Solar.R&lt; 79.5 18    777.1111 12.22222 *
##        9) Solar.R&gt;=79.5 51   7652.5100 25.90196  
##         18) Temp&lt; 77.5 33   2460.9090 21.18182 *
##         19) Temp&gt;=77.5 18   3108.4440 34.55556 *
##      5) Wind&lt; 7.15 10  21946.4000 55.60000 *
##    3) Temp&gt;=82.5 37  22452.9200 75.40541  
##      6) Temp&lt; 87.5 20  12046.9500 62.95000  
##       12) Wind&gt;=8.9 7    617.7143 45.57143 *
##       13) Wind&lt; 8.9 13   8176.7690 72.30769 *
##      7) Temp&gt;=87.5 17   3652.9410 90.05882 *</code></pre>
<p>First, we see that the NAs were deleted, and then we see the tree structure in a text format that includes the node number, how the node was split, the number of observations in the node, the deviance, and the mean response. To plot the tree, use <code>rpart.plot()</code> or <code>prp()</code>.</p>
<div class="sourceCode" id="cb843"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb843-1" data-line-number="1"><span class="kw">rpart.plot</span>(aq.tree)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p><code>rpart.plot()</code> provides several options for customizing the plot, among them are <code>digits</code>, <code>type</code>, and <code>extra</code>, which I invoked to produce the earlier plots. Refer to the help to see all of the options.</p>
<div class="sourceCode" id="cb844"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb844-1" data-line-number="1"><span class="kw">rpart.plot</span>(aq.tree, <span class="dt">digits =</span> <span class="dv">3</span>, <span class="dt">type=</span><span class="dv">4</span>, <span class="dt">extra=</span><span class="dv">101</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Another useful function is <code>printcp()</code>, which provides a deeper glimpse into whats going on in the algorithm. Here we see that just three predictors were used to grow the tree (<code>Solar.R</code>, <code>Temp</code>, and <code>Wind</code>). This means that the other predictors did not significantly contribute to increasing node purity, which is equivalent to a predictor in a linear model with a high p-value. We also see the root node error (weighted by the number of observations in the root node).</p>
<p>In the table, <code>printcp()</code> provides optimal tuning based on a <strong>complexity parameter</strong> (<code>CP</code>), which we can manipulate to manually prune the tree, if desired. The relative error column is the amount of reduction in root deviance for each split. For example, in our earlier example with three splits and four leaf nodes, we had a 61.2% reduction in root deviance, and below we see that at an <code>nsplit</code> of 3, we also get <span class="math inline">\(1.000 - 0.388 = 61.2\)</span>%.<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> <code>xerror</code> and <code>xstd</code> are cross-validation error and standard deviation, respectfully, so we get cross validation built-in for free!</p>
<div class="sourceCode" id="cb845"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb845-1" data-line-number="1"><span class="kw">printcp</span>(aq.tree)</a></code></pre></div>
<pre><code>## 
## Regression tree:
## rpart(formula = Ozone ~ ., data = airquality)
## 
## Variables actually used in tree construction:
## [1] Solar.R Temp    Wind   
## 
## Root node error: 125143/116 = 1078.8
## 
## n=116 (37 observations deleted due to missingness)
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.480718      0   1.00000 1.01865 0.16890
## 2 0.077238      1   0.51928 0.61672 0.19729
## 3 0.053962      2   0.44204 0.68502 0.18631
## 4 0.025990      3   0.38808 0.53568 0.15111
## 5 0.019895      4   0.36209 0.53216 0.15103
## 6 0.016646      5   0.34220 0.54833 0.16382
## 7 0.010000      6   0.32555 0.53996 0.16385</code></pre>
<p>With <code>plotcp()</code> we can see the 1 standard error rule implemented in the same manner weve seen before to identify the best fit model. At the top of the plot, the number of splits is displayed so that we can choose two splits when defining the best fit model.</p>
<div class="sourceCode" id="cb847"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb847-1" data-line-number="1"><span class="kw">plotcp</span>(aq.tree, <span class="dt">upper =</span> <span class="st">&quot;splits&quot;</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>Specify the best fit model using the <code>cp</code> parameter with a value slightly greater than shown in the table.</p>
<div class="sourceCode" id="cb848"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb848-1" data-line-number="1">best_aq.tree =<span class="st"> </span><span class="kw">rpart</span>(Ozone <span class="op">~</span><span class="st"> </span>., <span class="dt">cp=</span><span class="fl">0.055</span>, <span class="dt">data=</span>airquality)</a>
<a class="sourceLine" id="cb848-2" data-line-number="2"></a>
<a class="sourceLine" id="cb848-3" data-line-number="3"><span class="kw">rpart.plot</span>(best_aq.tree)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>As with <code>lm()</code> objects, the <code>summary()</code> function provides a wealth of information. Note the results following variable importance. Earlier we opined that the first split on <code>Temp</code> indicated that is was the most significant predictor followed by <code>Wind</code>. The <code>rpart</code> documentation provides a detailed description of variable importance:</p>
<blockquote>
<p>An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness * (adjusted agreement) for all splits in which it was a surrogate.</p>
</blockquote>
<p>Note that the results are scaled so that they sum to 100, which is useful for directly comparing each predictors relative contribution.</p>
<div class="sourceCode" id="cb849"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb849-1" data-line-number="1"><span class="kw">summary</span>(aq.tree)</a></code></pre></div>
<pre><code>## Call:
## rpart(formula = Ozone ~ ., data = airquality)
##   n=116 (37 observations deleted due to missingness)
## 
##           CP nsplit rel error    xerror      xstd
## 1 0.48071820      0 1.0000000 1.0186538 0.1689020
## 2 0.07723849      1 0.5192818 0.6167174 0.1972945
## 3 0.05396246      2 0.4420433 0.6850207 0.1863083
## 4 0.02598999      3 0.3880808 0.5356794 0.1511121
## 5 0.01989493      4 0.3620909 0.5321568 0.1510310
## 6 0.01664620      5 0.3421959 0.5483283 0.1638152
## 7 0.01000000      6 0.3255497 0.5399625 0.1638533
## 
## Variable importance
##    Temp    Wind     Day Solar.R   Month 
##      60      28       8       2       2 
## 
## Node number 1: 116 observations,    complexity param=0.4807182
##   mean=42.12931, MSE=1078.819 
##   left son=2 (79 obs) right son=3 (37 obs)
##   Primary splits:
##       Temp    &lt; 82.5  to the left,  improve=0.48071820, (0 missing)
##       Wind    &lt; 6.6   to the right, improve=0.40426690, (0 missing)
##       Solar.R &lt; 153   to the left,  improve=0.21080020, (5 missing)
##       Month   &lt; 6.5   to the left,  improve=0.11595770, (0 missing)
##       Day     &lt; 24.5  to the left,  improve=0.08216807, (0 missing)
##   Surrogate splits:
##       Wind &lt; 6.6   to the right, agree=0.776, adj=0.297, (0 split)
##       Day  &lt; 10.5  to the right, agree=0.724, adj=0.135, (0 split)
## 
## Node number 2: 79 observations,    complexity param=0.07723849
##   mean=26.5443, MSE=538.3746 
##   left son=4 (69 obs) right son=5 (10 obs)
##   Primary splits:
##       Wind    &lt; 7.15  to the right, improve=0.22726310, (0 missing)
##       Temp    &lt; 77.5  to the left,  improve=0.22489660, (0 missing)
##       Day     &lt; 24.5  to the left,  improve=0.13807170, (0 missing)
##       Solar.R &lt; 153   to the left,  improve=0.10449720, (2 missing)
##       Month   &lt; 8.5   to the right, improve=0.01924449, (0 missing)
## 
## Node number 3: 37 observations,    complexity param=0.05396246
##   mean=75.40541, MSE=606.8356 
##   left son=6 (20 obs) right son=7 (17 obs)
##   Primary splits:
##       Temp    &lt; 87.5  to the left,  improve=0.300763900, (0 missing)
##       Wind    &lt; 10.6  to the right, improve=0.273929800, (0 missing)
##       Solar.R &lt; 273.5 to the right, improve=0.114526900, (3 missing)
##       Day     &lt; 6.5   to the left,  improve=0.048950680, (0 missing)
##       Month   &lt; 7.5   to the left,  improve=0.007595265, (0 missing)
##   Surrogate splits:
##       Wind  &lt; 6.6   to the right, agree=0.676, adj=0.294, (0 split)
##       Month &lt; 7.5   to the left,  agree=0.649, adj=0.235, (0 split)
##       Day   &lt; 27.5  to the left,  agree=0.622, adj=0.176, (0 split)
## 
## Node number 4: 69 observations,    complexity param=0.01989493
##   mean=22.33333, MSE=158.2512 
##   left son=8 (18 obs) right son=9 (51 obs)
##   Primary splits:
##       Solar.R &lt; 79.5  to the left,  improve=0.22543670, (1 missing)
##       Temp    &lt; 77.5  to the left,  improve=0.21455360, (0 missing)
##       Day     &lt; 27    to the left,  improve=0.05183544, (0 missing)
##       Wind    &lt; 10.6  to the right, improve=0.04850548, (0 missing)
##       Month   &lt; 8.5   to the right, improve=0.01998100, (0 missing)
##   Surrogate splits:
##       Temp &lt; 63.5  to the left,  agree=0.794, adj=0.222, (1 split)
##       Wind &lt; 16.05 to the right, agree=0.750, adj=0.056, (0 split)
## 
## Node number 5: 10 observations
##   mean=55.6, MSE=2194.64 
## 
## Node number 6: 20 observations,    complexity param=0.02598999
##   mean=62.95, MSE=602.3475 
##   left son=12 (7 obs) right son=13 (13 obs)
##   Primary splits:
##       Wind    &lt; 8.9   to the right, improve=0.269982600, (0 missing)
##       Month   &lt; 7.5   to the right, improve=0.078628670, (0 missing)
##       Day     &lt; 18.5  to the left,  improve=0.073966850, (0 missing)
##       Solar.R &lt; 217.5 to the left,  improve=0.058145680, (3 missing)
##       Temp    &lt; 85.5  to the right, improve=0.007674142, (0 missing)
## 
## Node number 7: 17 observations
##   mean=90.05882, MSE=214.8789 
## 
## Node number 8: 18 observations
##   mean=12.22222, MSE=43.17284 
## 
## Node number 9: 51 observations,    complexity param=0.0166462
##   mean=25.90196, MSE=150.0492 
##   left son=18 (33 obs) right son=19 (18 obs)
##   Primary splits:
##       Temp    &lt; 77.5  to the left,  improve=0.27221870, (0 missing)
##       Wind    &lt; 10.6  to the right, improve=0.09788213, (0 missing)
##       Day     &lt; 22.5  to the left,  improve=0.07292523, (0 missing)
##       Month   &lt; 8.5   to the right, improve=0.04981065, (0 missing)
##       Solar.R &lt; 255   to the right, improve=0.03603008, (1 missing)
##   Surrogate splits:
##       Month &lt; 6.5   to the left,  agree=0.686, adj=0.111, (0 split)
##       Wind  &lt; 10.6  to the right, agree=0.667, adj=0.056, (0 split)
## 
## Node number 12: 7 observations
##   mean=45.57143, MSE=88.2449 
## 
## Node number 13: 13 observations
##   mean=72.30769, MSE=628.9822 
## 
## Node number 18: 33 observations
##   mean=21.18182, MSE=74.573 
## 
## Node number 19: 18 observations
##   mean=34.55556, MSE=172.6914</code></pre>
<p>The best fit model contains two predictors and explains 55.8% of the variance in <code>Ozone</code> as shown below.</p>
<div class="sourceCode" id="cb851"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb851-1" data-line-number="1"><span class="kw">printcp</span>(best_aq.tree)</a></code></pre></div>
<pre><code>## 
## Regression tree:
## rpart(formula = Ozone ~ ., data = airquality, cp = 0.055)
## 
## Variables actually used in tree construction:
## [1] Temp Wind
## 
## Root node error: 125143/116 = 1078.8
## 
## n=116 (37 observations deleted due to missingness)
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.480718      0   1.00000 1.00498 0.16717
## 2 0.077238      1   0.51928 0.56181 0.17679
## 3 0.055000      2   0.44204 0.58220 0.18019</code></pre>
<p>How does it compare to a linear model with the same two predictors? The linear model explains 56.1% of the variance in <code>Ozone</code>, which is only slightly more than the regression tree. Earlier I claimed there was a plan for improving the performance of regression trees. That plan is revealed in the next section on Random Forests.</p>
<div class="sourceCode" id="cb853"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb853-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">lm</span>(Ozone<span class="op">~</span>Wind <span class="op">+</span><span class="st"> </span>Temp, <span class="dt">data=</span>airquality))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Wind + Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.251 -13.695  -2.856  11.390 100.367 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -71.0332    23.5780  -3.013   0.0032 ** 
## Wind         -3.0555     0.6633  -4.607 1.08e-05 ***
## Temp          1.8402     0.2500   7.362 3.15e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 21.85 on 113 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.5687, Adjusted R-squared:  0.5611 
## F-statistic:  74.5 on 2 and 113 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<div id="random-forest-regression" class="section level3">
<h3><span class="header-section-number">11.5.2</span> Random Forest Regression</h3>
<p>In 1994, Leo Breiman at UC, Berkeley published <a href="https://www.stat.berkeley.edu/~breiman/bagging.pdf">this paper</a> in which he presented a method he called <strong>Bootstrap AGGregation</strong> (or BAGGing) that improves the predictive power of regression trees by growing many trees (a forest) using bootstrapping techniques (thereby making it a random forest). The details are explained in the link to the paper above, but in short, we grow many trees, each on a bootstrapped sample of the training set (i.e., sample <span class="math inline">\(n\)</span> times <em>with replacement</em> from a data set of size <span class="math inline">\(n\)</span>). Then, to make a prediction, we either let each tree vote and predict based on the most votes, or we use the average of the estimated responses. Cross-validation isnt necessary with this method because each bootstrapped tree has an internal error, referred to as the <strong>out-of-bag (OOB) error</strong>. With this method, about a third of the samples are left out of the bootstrapped sample, a prediction is made, and the OOB error calculated. The algorithm stops when the OOB error begins to increase.</p>
<p>A drawback of the method is that larger trees tend to be correlated with each other, and so <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">in a 2001 paper</a>, Breiman developed a method to lower the correlation between trees. For each bootstrapped sample, his idea was to use a random selection of predictors to split each node. The number of randomly selected predictors, <strong>mtry</strong>, is a function of the total number of predictors in the data set. For regression, the <code>randomForest()</code> function from the <code>randomForest</code> package uses <span class="math inline">\(1/k\)</span> as the default <code>mtry</code> value, but this can be manually specified. The following code chunks demonstrate the use of some of the <code>randomForest</code> functions. First, we fit a random forest model and specify that we want to assess the importance of predictors, omit <code>NA</code>s, and randomly sample two predictors at each split (<code>mtry</code>). There are a host of other parameters that can be specified, but well keep them all at their default settings for this example.</p>
<div class="sourceCode" id="cb855"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb855-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb855-2" data-line-number="2"></a>
<a class="sourceLine" id="cb855-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb855-4" data-line-number="4"></a>
<a class="sourceLine" id="cb855-5" data-line-number="5">aq.rf&lt;-<span class="st"> </span><span class="kw">randomForest</span>(Ozone<span class="op">~</span>., <span class="dt">importance=</span><span class="ot">TRUE</span>, <span class="dt">na.action=</span>na.omit, <span class="dt">mtry=</span><span class="dv">2</span>, <span class="dt">data=</span>airquality)</a>
<a class="sourceLine" id="cb855-6" data-line-number="6">aq.rf</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = Ozone ~ ., data = airquality, importance = TRUE,      mtry = 2, na.action = na.omit) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##           Mean of squared residuals: 301.7377
##                     % Var explained: 72.5</code></pre>
<p>This random forest model consists of 500 trees and explains 72.% of the variance in <code>Ozone</code>, which is a nice improvement over the 55.8% we got with the single regression tree. Plotting the <code>aq.rf</code> object shows the error as a function of the size of the forest. We want to see the error stabilize as the number of trees increases, which it does in the plot below.</p>
<div class="sourceCode" id="cb857"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb857-1" data-line-number="1"><span class="kw">plot</span>(aq.rf)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<div id="interpretation" class="section level4">
<h4><span class="header-section-number">11.5.2.1</span> Interpretation</h4>
<p>When the relationships between predictors and response are non-linear and complex, random forest models generally perform better than standard linear models. However, the increase in predictive power comes with a corresponding decrease in interpretability. For this reason, random forests and some other machine learning-based models such as neural networks are sometimes referred to as black box models. If you are applying machine learning techniques to build a model that performs optical character recognition, you might not be terribly concerned about the interpretability of your model. However, if your model will be used to inform a decision maker, interpretability is much more important - especially if you are asked to explain the model to the decision maker. In fact, some machine learning practitioners argue against using black box models for all high stakes decision making. For example, read <a href="https://arxiv.org/pdf/1811.10154.pdf">this paper</a> by Cynthia Rudin, a computer scientist at Duke University. Recently, advancements have been made in improving the interpretability of some types of machine learning models (for example, download and read <a href="https://www.h2o.ai/resources/ebook/introduction-to-machine-learning-interpretability/">this paper from h2o.ai</a> or <a href="https://christophm.github.io/interpretable-ml-book/">this e-book</a> by Christoph Molnar, a Ph.D.candidate at the University of Munich), and we will explore these techniques below.</p>
<p>Linear models have coefficients (the <span class="math inline">\(\beta\)</span>s) that explain the nature of the relationship between predictors and the response. Classification and regression trees have an analogous concept of variable importance, which can be extended to random forest models. The documentation for <code>importance()</code> from the <code>randomForest</code> package provides the following definitions of two variable importance measures:</p>
<blockquote>
<p>The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case).</p>
</blockquote>
<blockquote>
<p>The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.</p>
</blockquote>
<p>These two measures can be accessed with:</p>
<div class="sourceCode" id="cb858"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb858-1" data-line-number="1"><span class="kw">importance</span>(aq.rf)</a></code></pre></div>
<pre><code>##           %IncMSE IncNodePurity
## Solar.R 13.495267     15939.238
## Wind    19.989633     39498.922
## Temp    37.489127     48112.583
## Month    4.053344      4160.278
## Day      3.052987      9651.722</code></pre>
<p>Alternatively, we can plot variable importance with <code>varImpPlot()</code>.</p>
<div class="sourceCode" id="cb860"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb860-1" data-line-number="1"><span class="kw">varImpPlot</span>(aq.rf)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<p>Variable importance can be related to a linear model coefficient in that a large variable importance value is akin to a large coefficient value. However, it doesnt indicate whether the coefficient is positive or negative. For example, from the above plot, we see that <code>Temp</code> is an important predictor of <code>Ozone</code>, but we dont know if increasing temperatures result in increasing or decreasing ozone measurements (or if its a non-linear relationship). <strong>Partial dependence</strong> plots (PDP) were developed to solve this problem, and they can be interpreted in the same way as a loess or spline smoother.</p>
<p>For the <code>airquality</code> data, one would expect that increasing temperatures would increase ozone concentrations, and that increasing wind speed would decrease ozone concentrations. The <code>partialPlot()</code> function provided with the <code>randomForest</code> package produces PDPs, but they are basic and difficult to customize. Instead, well use the <code>pdp</code> package, which works nicely with <code>ggplot2</code> and includes a loess smoother (another option is the <code>iml</code> package - for interpretable machine learning - which well also explore).</p>
<div class="sourceCode" id="cb861"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb861-1" data-line-number="1"><span class="co">#library(pdp)</span></a>
<a class="sourceLine" id="cb861-2" data-line-number="2"></a>
<a class="sourceLine" id="cb861-3" data-line-number="3">p3 =<span class="st"> </span>aq.rf <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb861-4" data-line-number="4"><span class="st">  </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Temp&quot;</span>) <span class="op">%&gt;%</span><span class="st">                        </span><span class="co"># from the pdp package</span></a>
<a class="sourceLine" id="cb861-5" data-line-number="5"><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">smooth =</span> <span class="ot">TRUE</span>, <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">f</span>(Temp))) <span class="op">+</span></a>
<a class="sourceLine" id="cb861-6" data-line-number="6"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb861-7" data-line-number="7"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Partial Dependence of Temp&quot;</span>)</a>
<a class="sourceLine" id="cb861-8" data-line-number="8"></a>
<a class="sourceLine" id="cb861-9" data-line-number="9">p4 =<span class="st"> </span>aq.rf <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb861-10" data-line-number="10"><span class="st">  </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Wind&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb861-11" data-line-number="11"><span class="st">  </span><span class="kw">autoplot</span>(<span class="dt">smooth =</span> <span class="ot">TRUE</span>, <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">f</span>(Temp))) <span class="op">+</span></a>
<a class="sourceLine" id="cb861-12" data-line-number="12"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb861-13" data-line-number="13"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Partial Dependence of Wind&quot;</span>)</a>
<a class="sourceLine" id="cb861-14" data-line-number="14"></a>
<a class="sourceLine" id="cb861-15" data-line-number="15">gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p3, p4, <span class="dt">ncol=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<p>Earlier, we produced a response surface plot based on a regression tree. Now we can produce a response surface based on the random forest model, which looks similar but more detailed. Specifying <code>chull = TRUE</code> (chull stands for convex hull) limits the plot to the range of values in the training data set, which prevents predictions being shown for regions in which there is no data. A 2D heat map and a 3D mesh are shown below.</p>
<div class="sourceCode" id="cb862"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb862-1" data-line-number="1"><span class="co"># Compute partial dependence data for Wind and Temp</span></a>
<a class="sourceLine" id="cb862-2" data-line-number="2">pd =<span class="st"> </span>pdp<span class="op">::</span><span class="kw">partial</span>(aq.rf, <span class="dt">pred.var =</span> <span class="kw">c</span>(<span class="st">&quot;Wind&quot;</span>, <span class="st">&quot;Temp&quot;</span>), <span class="dt">chull =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb862-3" data-line-number="3"></a>
<a class="sourceLine" id="cb862-4" data-line-number="4"><span class="co"># Default PDP</span></a>
<a class="sourceLine" id="cb862-5" data-line-number="5">pdp1 =<span class="st"> </span>pdp<span class="op">::</span><span class="kw">plotPartial</span>(pd)</a>
<a class="sourceLine" id="cb862-6" data-line-number="6"></a>
<a class="sourceLine" id="cb862-7" data-line-number="7"><span class="co"># 3-D surface</span></a>
<a class="sourceLine" id="cb862-8" data-line-number="8">pdp2 =<span class="st"> </span>pdp<span class="op">::</span><span class="kw">plotPartial</span>(pd, <span class="dt">levelplot =</span> <span class="ot">FALSE</span>, <span class="dt">zlab =</span> <span class="st">&quot;Ozone&quot;</span>,</a>
<a class="sourceLine" id="cb862-9" data-line-number="9">                    <span class="dt">screen =</span> <span class="kw">list</span>(<span class="dt">z =</span> <span class="dv">-20</span>, <span class="dt">x =</span> <span class="dv">-60</span>))</a>
<a class="sourceLine" id="cb862-10" data-line-number="10"></a>
<a class="sourceLine" id="cb862-11" data-line-number="11">gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(pdp1, pdp2, <span class="dt">ncol=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>The <code>iml</code> package was developed by Christoph Molnar, the Ph.D.candidate referred to earlier, and contains a number of useful functions to aid in model interpretation. In machine learning vernacular, predictors are commonly called features, so instead of variable importance, well get feature importance. With this package, we can calculate feature importance and produce PDPs as well, and a grid of partial dependence plots are shown below. Note the addition of a rug plot at the bottom of each subplot, which helps identify regions where observations are sparse and where the model might not perform as well.</p>
<div class="sourceCode" id="cb863"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb863-1" data-line-number="1"><span class="co">#library(iml) # for interpretable machine learning</span></a>
<a class="sourceLine" id="cb863-2" data-line-number="2"><span class="co">#library(patchwork) # for arranging plots - similar to gridExtra</span></a>
<a class="sourceLine" id="cb863-3" data-line-number="3"></a>
<a class="sourceLine" id="cb863-4" data-line-number="4"><span class="co"># iml doesn&#39;t like NAs, so we&#39;ll drop them from the data and re-fit the model</span></a>
<a class="sourceLine" id="cb863-5" data-line-number="5">aq =<span class="st"> </span>airquality <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop_na</span>()</a>
<a class="sourceLine" id="cb863-6" data-line-number="6">aq.rf2 =<span class="st"> </span><span class="kw">randomForest</span>(Ozone<span class="op">~</span>., <span class="dt">importance=</span><span class="ot">TRUE</span>, <span class="dt">na.action=</span>na.omit, <span class="dt">mtry=</span><span class="dv">2</span>, <span class="dt">data=</span>aq)</a>
<a class="sourceLine" id="cb863-7" data-line-number="7"></a>
<a class="sourceLine" id="cb863-8" data-line-number="8"><span class="co"># provide the random forest model, the features, and the response</span></a>
<a class="sourceLine" id="cb863-9" data-line-number="9">predictor =<span class="st"> </span>iml<span class="op">::</span>Predictor<span class="op">$</span><span class="kw">new</span>(aq.rf2, <span class="dt">data =</span> aq[, <span class="dv">2</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">y =</span> aq<span class="op">$</span>Ozone)</a>
<a class="sourceLine" id="cb863-10" data-line-number="10"></a>
<a class="sourceLine" id="cb863-11" data-line-number="11">PDP =<span class="st"> </span>iml<span class="op">::</span>FeatureEffects<span class="op">$</span><span class="kw">new</span>(predictor, <span class="dt">method=</span><span class="st">&#39;pdp&#39;</span>)</a>
<a class="sourceLine" id="cb863-12" data-line-number="12">PDP<span class="op">$</span><span class="kw">plot</span>() <span class="op">&amp;</span><span class="st"> </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<pre><code>## Warning: UNRELIABLE VALUE: Future (&#39;future_lapply-1&#39;) unexpectedly generated
## random numbers without specifying argument &#39;future.seed&#39;. There is a risk that
## those random numbers are not statistically sound and the overall results might
## be invalid. To fix this, specify &#39;future.seed=TRUE&#39;. This ensures that proper,
## parallel-safe random numbers are produced via the L&#39;Ecuyer-CMRG method. To
## disable this check, use &#39;future.seed=NULL&#39;, or set option &#39;future.rng.onMisuse&#39;
## to &quot;ignore&quot;.</code></pre>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>PDPs show the average feature effect, but if were interested in the effect for one or more individual observations, then an Individual Conditional Expectation (ICE) plot is useful. In the following plot, each black line represents one of the 111 observations in the data set, and the global partial dependence is shown in yellow. Since the individual lines are generally parallel, we can see that each individual observation follows the same general trend: increasing temperatures have little effect on ozone until around 76 degrees, at which point all observations increase. In the mid 80s, there are a few observations that have a decreasing trend while the majority continue to increase, which indicates temperature may be interacting with one or more other features. Generally speaking, however, since the individual lines are largely parallel, we can conclude that the partial dependence measure is a good representation of the whole data set.</p>
<div class="sourceCode" id="cb865"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb865-1" data-line-number="1">ice =<span class="st"> </span>iml<span class="op">::</span>FeatureEffect<span class="op">$</span><span class="kw">new</span>(predictor, <span class="dt">feature =</span> <span class="st">&quot;Temp&quot;</span>, <span class="dt">method=</span><span class="st">&#39;pdp+ice&#39;</span>) <span class="co">#center.at = min(aq$Temp))</span></a>
<a class="sourceLine" id="cb865-2" data-line-number="2">ice<span class="op">$</span><span class="kw">plot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>One of the nice attributes of tree-based models is their ability to capture interactions. The interaction effects can be explicitly measured and plotted as shown below. The x-axis scale is the percent of variance explained by interaction for each feature, so <code>Wind</code>, <code>Temp</code>, and <code>Solar.R</code> all have more than 10% of their variance explained by an interaction.</p>
<div class="sourceCode" id="cb866"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb866-1" data-line-number="1">interact =<span class="st"> </span>iml<span class="op">::</span>Interaction<span class="op">$</span><span class="kw">new</span>(predictor)</a>
<a class="sourceLine" id="cb866-2" data-line-number="2"><span class="kw">plot</span>(interact) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>To identify what the feature is interacting with, just specify the feature name. For example, <code>Temp</code> interactions are shown below.</p>
<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb867-1" data-line-number="1">interact =<span class="st"> </span>iml<span class="op">::</span>Interaction<span class="op">$</span><span class="kw">new</span>(predictor, <span class="dt">feature=</span><span class="st">&#39;Temp&#39;</span>)</a>
<a class="sourceLine" id="cb867-2" data-line-number="2"><span class="kw">plot</span>(interact) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
</div>
<div id="predictions" class="section level4">
<h4><span class="header-section-number">11.5.2.2</span> Predictions</h4>
<p>Predictions for new data are made the usual way with <code>predict()</code>, which is demonstrated below using the first two rows of the <code>airquality</code> data set.</p>
<div class="sourceCode" id="cb868"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb868-1" data-line-number="1"><span class="kw">predict</span>(aq.rf, airquality[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">6</span>)])</a></code></pre></div>
<pre><code>##        1        2 
## 38.50243 31.39827</code></pre>
</div>
</div>
<div id="random-forest-classification" class="section level3">
<h3><span class="header-section-number">11.5.3</span> Random Forest Classification</h3>
<p>For a classification example, well skip over simple classification trees and jump straight to random forests. There is very little difference in syntax with the <code>randomForest()</code> function when performing classification instead of regression. For this demonstration, well use the <code>iris</code> data set so we can compare results with the SVC results. Well use the same training and test sets as earlier.</p>
<div class="sourceCode" id="cb870"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb870-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb870-2" data-line-number="2">iris.rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>iris_train, <span class="dt">importance=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb870-3" data-line-number="3"><span class="kw">print</span>(iris.rf)</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = Species ~ ., data = iris_train, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 4.17%
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         40          0         0       0.000
## versicolor      0         37         3       0.075
## virginica       0          2        38       0.050</code></pre>
<p>The model seems to have a little trouble distinguishing virginica from versicolor. The linear SVC misclassified two observations in the test set, and the radial SVC misclassified one. Before we see how the random forest does, lets make sure we grew enough trees. We can make a visual check by plotting the random forest object.</p>
<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb872-1" data-line-number="1"><span class="kw">plot</span>(iris.rf)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>No issue there! Looks like 500 trees was plenty. Taking a look at variable importance shows that petal width and length are far more important than sepal width and length.</p>
<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb873-1" data-line-number="1"><span class="kw">varImpPlot</span>(iris.rf)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<p>Since the response variable is categorical with three levels, a little work is required to get partial dependence plots for each predictor-response combination. Below are the partial dependence plots for <code>Petal.Width</code> for each species. The relationship between petal width and species varies significantly based on the species, which is what makes petal width have a high variable importance.</p>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb874-1" data-line-number="1"><span class="kw">as_tibble</span>(iris.rf <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-2" data-line-number="2"><span class="st">  </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Petal.Width&quot;</span>, <span class="dt">which.class=</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># which.class refers to the factor level</span></a>
<a class="sourceLine" id="cb874-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Species =</span> <span class="kw">levels</span>(iris<span class="op">$</span>Species)[<span class="dv">1</span>])) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-4" data-line-number="4"><span class="st">  </span><span class="kw">bind_rows</span>(<span class="kw">as_tibble</span>(iris.rf <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-5" data-line-number="5"><span class="st">  </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Petal.Width&quot;</span>, <span class="dt">which.class=</span><span class="dv">2</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Species =</span> <span class="kw">levels</span>(iris<span class="op">$</span>Species)[<span class="dv">2</span>]))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-7" data-line-number="7"><span class="st">  </span><span class="kw">bind_rows</span>(<span class="kw">as_tibble</span>(iris.rf <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-8" data-line-number="8"><span class="st">  </span>pdp<span class="op">::</span><span class="kw">partial</span>(<span class="dt">pred.var =</span> <span class="st">&quot;Petal.Width&quot;</span>, <span class="dt">which.class=</span><span class="dv">3</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Species =</span> <span class="kw">levels</span>(iris<span class="op">$</span>Species)[<span class="dv">3</span>]))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb874-10" data-line-number="10"><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb874-11" data-line-number="11"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Petal.Width, <span class="dt">y=</span>yhat, <span class="dt">col=</span>Species), <span class="dt">size=</span><span class="fl">1.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb874-12" data-line-number="12"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Partial Dependence of Petal.Width&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb874-13" data-line-number="13"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>Enough visualizing. Time to get the confusion matrix for the random forest model using the test set.</p>
<div class="sourceCode" id="cb875"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb875-1" data-line-number="1"><span class="co"># get the confusion matrix</span></a>
<a class="sourceLine" id="cb875-2" data-line-number="2">rf_conf_mat =<span class="st"> </span>cvms<span class="op">::</span><span class="kw">confusion_matrix</span>(</a>
<a class="sourceLine" id="cb875-3" data-line-number="3">  <span class="dt">targets =</span> iris_test[, <span class="dv">5</span>],</a>
<a class="sourceLine" id="cb875-4" data-line-number="4">  <span class="dt">predictions =</span> <span class="kw">predict</span>(iris.rf, <span class="dt">newdata =</span> iris_test[<span class="op">-</span><span class="dv">5</span>]))</a>
<a class="sourceLine" id="cb875-5" data-line-number="5"></a>
<a class="sourceLine" id="cb875-6" data-line-number="6"><span class="co"># plot the confusion matrix</span></a>
<a class="sourceLine" id="cb875-7" data-line-number="7">cvms<span class="op">::</span><span class="kw">plot_confusion_matrix</span>(rf_conf_mat<span class="op">$</span><span class="st">`</span><span class="dt">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Random Forest&quot;</span>)</a></code></pre></div>
<p><img src="09-Non_Parametric_Regression_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>Two observations were misclassified just like with the linear SVC. Lets see if theyre the same two observations.</p>
<div class="sourceCode" id="cb876"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb876-1" data-line-number="1"><span class="co"># the indices of the misclassified flowers from SVC</span></a>
<a class="sourceLine" id="cb876-2" data-line-number="2"><span class="kw">which</span>(iris_test[, <span class="dv">5</span>] <span class="op">!=</span><span class="st"> </span><span class="kw">predict</span>(iris.lin<span class="op">$</span>best.model, <span class="dt">newdata =</span> iris_test[<span class="op">-</span><span class="dv">5</span>]))</a></code></pre></div>
<pre><code>## [1] 24 27</code></pre>
<div class="sourceCode" id="cb878"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb878-1" data-line-number="1"><span class="co"># the indices of the misclassified flowers from random forest</span></a>
<a class="sourceLine" id="cb878-2" data-line-number="2"><span class="kw">which</span>(iris_test[, <span class="dv">5</span>] <span class="op">!=</span><span class="st"> </span><span class="kw">predict</span>(iris.rf, <span class="dt">newdata =</span> iris_test[<span class="op">-</span><span class="dv">5</span>]))</a></code></pre></div>
<pre><code>## [1] 24 27</code></pre>
</div>
<div id="cart-problem-set" class="section level3">
<h3><span class="header-section-number">11.5.4</span> CART Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter11_ProblemSets/RF_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter11_ProblemSets/RF_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter11_ProblemSets/RF_PS_Solutions.html'>here</a>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-devore2015">
<p>Devore, Jay. 2015. <em>Probability and Statisticsfor Engineering and the Sciences</em>. 9th ed. Cengage Learning.</p>
</div>
<div id="ref-hastie2008">
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2008. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="36">
<li id="fn36"><p>Changing the kernel to specify the type of fit is known as the kernel trick.<a href="non-parametric-regression.html#fnref36" class="footnote-back"></a></p></li>
<li id="fn37"><p>Sometimes referred to as partition trees.<a href="non-parametric-regression.html#fnref37" class="footnote-back"></a></p></li>
<li id="fn38"><p>Its always nice to see that I didnt mess up the manual calculations.<a href="non-parametric-regression.html#fnref38" class="footnote-back"></a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="advanced-experimental-designs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optional-advanced-doe-topics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "chapter"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
