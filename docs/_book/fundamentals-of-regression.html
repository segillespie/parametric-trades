<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Fundamentals of Regression | Design of Experiments for Modeling and Simulation</title>
  <meta name="description" content="7 Fundamentals of Regression | Design of Experiments for Modeling and Simulation" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Fundamentals of Regression | Design of Experiments for Modeling and Simulation" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Fundamentals of Regression | Design of Experiments for Modeling and Simulation" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fractional-factorial-designs.html"/>
<link rel="next" href="model-selection.html"/>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.17/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<script src="libs/plotly-binding-4.9.3/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://doe.dscoe.org/">DoE for M&S</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to DOE for M&amp;S</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#background."><i class="fa fa-check"></i><b>1.1</b> Background.</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#mission."><i class="fa fa-check"></i><b>1.2</b> Mission.</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#simulation-analysis."><i class="fa fa-check"></i><b>1.2.1</b> Simulation Analysis.</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#enduring-resource."><i class="fa fa-check"></i><b>1.2.2</b> Enduring Resource.</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#community."><i class="fa fa-check"></i><b>1.2.3</b> Community.</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#concept."><i class="fa fa-check"></i><b>1.3</b> Concept.</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#training-concept"><i class="fa fa-check"></i><b>1.4</b> Training Concept</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#lessons."><i class="fa fa-check"></i><b>1.4.1</b> Lessons.</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#projects."><i class="fa fa-check"></i><b>1.4.2</b> Projects.</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#assessments."><i class="fa fa-check"></i><b>1.4.3</b> Assessments.</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#curriculum."><i class="fa fa-check"></i><b>1.4.4</b> Curriculum.</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#community-concept"><i class="fa fa-check"></i><b>1.5</b> Community Concept</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#admin"><i class="fa fa-check"></i><b>1.6</b> Admin</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6.1</b> Contact</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#errors"><i class="fa fa-check"></i><b>1.6.2</b> Errors</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i><b>1.6.3</b> Resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to R</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#introduction-to-r---part-i"><i class="fa fa-check"></i><b>2.1</b> Introduction to R - Part I</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#objectives-1"><i class="fa fa-check"></i><b>2.1.1</b> Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#task-1---read-chapter-1-and-install-tidyverse"><i class="fa fa-check"></i><b>2.1.2</b> Task 1 - Read Chapter 1 and Install Tidyverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-operations"><i class="fa fa-check"></i><b>2.1.3</b> Basic Operations</a></li>
<li class="chapter" data-level="2.1.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#variable-types"><i class="fa fa-check"></i><b>2.1.4</b> Variable Types</a></li>
<li class="chapter" data-level="2.1.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-structures"><i class="fa fa-check"></i><b>2.1.5</b> Data Structures</a></li>
<li class="chapter" data-level="2.1.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#relational-and-logical-operators"><i class="fa fa-check"></i><b>2.1.6</b> Relational and Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#introduction-to-r---part-ii"><i class="fa fa-check"></i><b>2.2</b> Introduction to R - Part II</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#reading-tasks"><i class="fa fa-check"></i><b>2.2.1</b> Reading Tasks</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#problem-set"><i class="fa fa-check"></i><b>2.2.2</b> Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistics-review.html"><a href="statistics-review.html"><i class="fa fa-check"></i><b>3</b> Statistics Review</a><ul>
<li class="chapter" data-level="3.1" data-path="statistics-review.html"><a href="statistics-review.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="statistics-review.html"><a href="statistics-review.html#resources-1"><i class="fa fa-check"></i><b>3.1.1</b> Resources</a></li>
<li class="chapter" data-level="3.1.2" data-path="statistics-review.html"><a href="statistics-review.html#organization"><i class="fa fa-check"></i><b>3.1.2</b> Organization</a></li>
<li class="chapter" data-level="3.1.3" data-path="statistics-review.html"><a href="statistics-review.html#poc"><i class="fa fa-check"></i><b>3.1.3</b> POC</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics"><i class="fa fa-check"></i><b>3.2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.2.1" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics---description"><i class="fa fa-check"></i><b>3.2.1</b> Descriptive Statistics - Description</a></li>
<li class="chapter" data-level="3.2.2" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics---tutorial"><i class="fa fa-check"></i><b>3.2.2</b> Descriptive Statistics - Tutorial</a></li>
<li class="chapter" data-level="3.2.3" data-path="statistics-review.html"><a href="statistics-review.html#descriptive-statistics---problem-set"><i class="fa fa-check"></i><b>3.2.3</b> Descriptive Statistics - Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts"><i class="fa fa-check"></i><b>3.3</b> Statistical Concepts</a><ul>
<li class="chapter" data-level="3.3.1" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts---description"><i class="fa fa-check"></i><b>3.3.1</b> Statistical Concepts - Description</a></li>
<li class="chapter" data-level="3.3.2" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts---tutorial"><i class="fa fa-check"></i><b>3.3.2</b> Statistical Concepts - Tutorial</a></li>
<li class="chapter" data-level="3.3.3" data-path="statistics-review.html"><a href="statistics-review.html#statistical-concepts---problem-set"><i class="fa fa-check"></i><b>3.3.3</b> Statistical Concepts - Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference"><i class="fa fa-check"></i><b>3.4</b> Statistical Inference</a><ul>
<li class="chapter" data-level="3.4.1" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---description"><i class="fa fa-check"></i><b>3.4.1</b> Statistical Inference - Description</a></li>
<li class="chapter" data-level="3.4.2" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---tutorial"><i class="fa fa-check"></i><b>3.4.2</b> Statistical Inference - Tutorial</a></li>
<li class="chapter" data-level="3.4.3" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---conclusion"><i class="fa fa-check"></i><b>3.4.3</b> Statistical Inference - Conclusion</a></li>
<li class="chapter" data-level="3.4.4" data-path="statistics-review.html"><a href="statistics-review.html#statistical-inference---problem-set"><i class="fa fa-check"></i><b>3.4.4</b> Statistical Inference - Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>4</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-overview"><i class="fa fa-check"></i><b>4.2</b> ANOVA Overview</a><ul>
<li class="chapter" data-level="4.2.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#motivation"><i class="fa fa-check"></i><b>4.2.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#broad-concept"><i class="fa fa-check"></i><b>4.2.2</b> Broad Concept</a></li>
<li class="chapter" data-level="4.2.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#terminology"><i class="fa fa-check"></i><b>4.2.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-fixed-effects-anova"><i class="fa fa-check"></i><b>4.3</b> Single Factor, Fixed Effects ANOVA</a><ul>
<li class="chapter" data-level="4.3.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#example"><i class="fa fa-check"></i><b>4.3.1</b> Example</a></li>
<li class="chapter" data-level="4.3.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#model"><i class="fa fa-check"></i><b>4.3.2</b> Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-anova-assumptions"><i class="fa fa-check"></i><b>4.3.3</b> Single Factor ANOVA Assumptions</a></li>
<li class="chapter" data-level="4.3.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#conducting-single-factor-fixed-effects-anova"><i class="fa fa-check"></i><b>4.3.4</b> Conducting Single Factor (Fixed Effects) ANOVA</a></li>
<li class="chapter" data-level="4.3.5" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-fixed-effects-anova-problem-set"><i class="fa fa-check"></i><b>4.3.5</b> Single Factor, Fixed Effects ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-random-effects-model"><i class="fa fa-check"></i><b>4.4</b> Single Factor, Random Effects Model</a><ul>
<li class="chapter" data-level="4.4.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#single-factor-random-effects-problem-set"><i class="fa fa-check"></i><b>4.4.1</b> Single Factor, Random Effects Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#two-factor-fixed-effects-anova"><i class="fa fa-check"></i><b>4.5</b> Two Factor, Fixed Effects ANOVA</a><ul>
<li class="chapter" data-level="4.5.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#including-interaction-effects"><i class="fa fa-check"></i><b>4.5.1</b> Including Interaction Effects</a></li>
<li class="chapter" data-level="4.5.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#two-factor-anova-problem-set"><i class="fa fa-check"></i><b>4.5.2</b> Two Factor ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-for-more-than-two-factors"><i class="fa fa-check"></i><b>4.6</b> ANOVA For More Than Two Factors</a><ul>
<li class="chapter" data-level="4.6.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multi-factor-anova-problem-set"><i class="fa fa-check"></i><b>4.6.1</b> Multi-Factor ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.7</b> Multiple Comparisons</a><ul>
<li class="chapter" data-level="4.7.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#tukey-test-example"><i class="fa fa-check"></i><b>4.7.1</b> Tukey Test Example</a></li>
<li class="chapter" data-level="4.7.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#tukey-test-in-r"><i class="fa fa-check"></i><b>4.7.2</b> Tukey Test in R</a></li>
<li class="chapter" data-level="4.7.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#tukey-test-multiple-factors"><i class="fa fa-check"></i><b>4.7.3</b> Tukey Test, Multiple Factors</a></li>
<li class="chapter" data-level="4.7.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multiple-comparisons-problem-set"><i class="fa fa-check"></i><b>4.7.4</b> Multiple Comparisons Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-summary"><i class="fa fa-check"></i><b>4.8</b> ANOVA Summary</a><ul>
<li class="chapter" data-level="4.8.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#how-to-calculate-anova"><i class="fa fa-check"></i><b>4.8.1</b> How to Calculate ANOVA</a></li>
<li class="chapter" data-level="4.8.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-hypothesis-tests"><i class="fa fa-check"></i><b>4.8.2</b> ANOVA Hypothesis Test(s)</a></li>
<li class="chapter" data-level="4.8.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#anova-assumptions"><i class="fa fa-check"></i><b>4.8.3</b> ANOVA Assumptions</a></li>
<li class="chapter" data-level="4.8.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#multiple-comparisons-1"><i class="fa fa-check"></i><b>4.8.4</b> Multiple Comparisons</a></li>
<li class="chapter" data-level="4.8.5" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#further-extensions-of-anova"><i class="fa fa-check"></i><b>4.8.5</b> Further Extensions of ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html"><i class="fa fa-check"></i><b>5</b> Fundamentals of Design of Experiments</a><ul>
<li class="chapter" data-level="5.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#admin-1"><i class="fa fa-check"></i><b>5.1.1</b> Admin</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#doe-overview"><i class="fa fa-check"></i><b>5.2</b> DOE Overview</a><ul>
<li class="chapter" data-level="5.2.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#what-is-design-of-experiments"><i class="fa fa-check"></i><b>5.2.1</b> What is Design of Experiments?</a></li>
<li class="chapter" data-level="5.2.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#why-design-experiments"><i class="fa fa-check"></i><b>5.2.2</b> Why Design Experiments?</a></li>
<li class="chapter" data-level="5.2.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#strategies-for-experimental-design"><i class="fa fa-check"></i><b>5.2.3</b> Strategies for Experimental Design</a></li>
<li class="chapter" data-level="5.2.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#history-and-applications-of-experimental-design"><i class="fa fa-check"></i><b>5.2.4</b> History and Applications of Experimental Design</a></li>
<li class="chapter" data-level="5.2.5" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#principles"><i class="fa fa-check"></i><b>5.2.5</b> Principles</a></li>
<li class="chapter" data-level="5.2.6" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#guidelines"><i class="fa fa-check"></i><b>5.2.6</b> Guidelines</a></li>
<li class="chapter" data-level="5.2.7" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#problem-set-1"><i class="fa fa-check"></i><b>5.2.7</b> Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#factorial-designs"><i class="fa fa-check"></i><b>5.3</b> Factorial Designs</a><ul>
<li class="chapter" data-level="5.3.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#introduction-3"><i class="fa fa-check"></i><b>5.3.1</b> Introduction</a></li>
<li class="chapter" data-level="5.3.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#assessing-and-estimating-effects"><i class="fa fa-check"></i><b>5.3.2</b> Assessing and Estimating Effects</a></li>
<li class="chapter" data-level="5.3.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#blocking"><i class="fa fa-check"></i><b>5.3.3</b> Blocking</a></li>
<li class="chapter" data-level="5.3.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#problem-set-2"><i class="fa fa-check"></i><b>5.3.4</b> Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#k-factorial-design"><i class="fa fa-check"></i><b>5.4</b> <span class="math inline">\(2^K\)</span> Factorial Design</a><ul>
<li class="chapter" data-level="5.4.1" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#introduction-4"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#coding-variables-and-effects-signs"><i class="fa fa-check"></i><b>5.4.2</b> Coding Variables and Effects Signs</a></li>
<li class="chapter" data-level="5.4.3" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#estimating-effects-and-contrasts"><i class="fa fa-check"></i><b>5.4.3</b> Estimating Effects and Contrasts</a></li>
<li class="chapter" data-level="5.4.4" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#unreplicated-2k-designs"><i class="fa fa-check"></i><b>5.4.4</b> Unreplicated <span class="math inline">\(2^K\)</span> Designs</a></li>
<li class="chapter" data-level="5.4.5" data-path="fundamentals-of-design-of-experiments.html"><a href="fundamentals-of-design-of-experiments.html#problem-set-3"><i class="fa fa-check"></i><b>5.4.5</b> Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html"><i class="fa fa-check"></i><b>6</b> Fractional Factorial Designs</a><ul>
<li class="chapter" data-level="6.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#introduction-5"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#admin-2"><i class="fa fa-check"></i><b>6.1.1</b> Admin</a></li>
<li class="chapter" data-level="6.1.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#overview"><i class="fa fa-check"></i><b>6.1.2</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#frac12-fractional-factorial-design-2k-1"><i class="fa fa-check"></i><b>6.2</b> <span class="math inline">\(\frac{1}{2}\)</span> Fractional Factorial Design (<span class="math inline">\(2^{K-1}\)</span>)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#motivation-and-example"><i class="fa fa-check"></i><b>6.2.1</b> Motivation and Example</a></li>
<li class="chapter" data-level="6.2.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#design-generator"><i class="fa fa-check"></i><b>6.2.2</b> Design Generator</a></li>
<li class="chapter" data-level="6.2.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#estimating-effects"><i class="fa fa-check"></i><b>6.2.3</b> Estimating Effects</a></li>
<li class="chapter" data-level="6.2.4" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#geometric-view"><i class="fa fa-check"></i><b>6.2.4</b> Geometric View</a></li>
<li class="chapter" data-level="6.2.5" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#example-24-1-design"><i class="fa fa-check"></i><b>6.2.5</b> Example <span class="math inline">\(2^{4-1}\)</span> Design</a></li>
<li class="chapter" data-level="6.2.6" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#frac12-fractional-factorial-design-2k-1-problem-set"><i class="fa fa-check"></i><b>6.2.6</b> <span class="math inline">\(\frac{1}{2}\)</span> Fractional Factorial Design (<span class="math inline">\(2^{K-1}\)</span>) Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#general-2k-p-designs"><i class="fa fa-check"></i><b>6.3</b> General <span class="math inline">\(2^{K-P}\)</span> Designs</a><ul>
<li class="chapter" data-level="6.3.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#introduction-6"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#quarter-fractional-design"><i class="fa fa-check"></i><b>6.3.2</b> Quarter Fractional Design</a></li>
<li class="chapter" data-level="6.3.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#general-2k-p-design"><i class="fa fa-check"></i><b>6.3.3</b> General <span class="math inline">\(2^{K-P}\)</span> Design</a></li>
<li class="chapter" data-level="6.3.4" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#resolution"><i class="fa fa-check"></i><b>6.3.4</b> Resolution</a></li>
<li class="chapter" data-level="6.3.5" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#general-2k-p-designs-problem-set"><i class="fa fa-check"></i><b>6.3.5</b> General <span class="math inline">\(2^{K-P}\)</span> Designs Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#additional-notes-on-2k-p-designs"><i class="fa fa-check"></i><b>6.4</b> Additional Notes on <span class="math inline">\(2^{K-P}\)</span> Designs</a><ul>
<li class="chapter" data-level="6.4.1" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#additional-topics"><i class="fa fa-check"></i><b>6.4.1</b> Additional Topics</a></li>
<li class="chapter" data-level="6.4.2" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#sequencing-experiments"><i class="fa fa-check"></i><b>6.4.2</b> Sequencing Experiments</a></li>
<li class="chapter" data-level="6.4.3" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#fractional-factorial-design-simulation-problem-set"><i class="fa fa-check"></i><b>6.4.3</b> Fractional Factorial Design Simulation Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#screening-experiments-and-selecting-factors"><i class="fa fa-check"></i><b>6.5</b> Screening Experiments and Selecting Factors</a></li>
<li class="chapter" data-level="6.6" data-path="fractional-factorial-designs.html"><a href="fractional-factorial-designs.html#conclusion"><i class="fa fa-check"></i><b>6.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html"><i class="fa fa-check"></i><b>7</b> Fundamentals of Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#admin-3"><i class="fa fa-check"></i><b>7.1</b> Admin</a></li>
<li class="chapter" data-level="7.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#least-squares-method-manually"><i class="fa fa-check"></i><b>7.2.1</b> Least Squares Method Manually</a></li>
<li class="chapter" data-level="7.2.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>7.2.2</b> Goodness of Fit</a></li>
<li class="chapter" data-level="7.2.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#least-squares-method-in-r"><i class="fa fa-check"></i><b>7.2.3</b> Least Squares Method In <em>R</em></a></li>
<li class="chapter" data-level="7.2.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#simple-linear-regression-problem-set"><i class="fa fa-check"></i><b>7.2.4</b> Simple Linear Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#assumptions-and-diagnostics"><i class="fa fa-check"></i><b>7.3</b> Assumptions and Diagnostics</a><ul>
<li class="chapter" data-level="7.3.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#linearity"><i class="fa fa-check"></i><b>7.3.1</b> Linearity</a></li>
<li class="chapter" data-level="7.3.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#homoscedasticity"><i class="fa fa-check"></i><b>7.3.2</b> Homoscedasticity</a></li>
<li class="chapter" data-level="7.3.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#independence"><i class="fa fa-check"></i><b>7.3.3</b> Independence</a></li>
<li class="chapter" data-level="7.3.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#normality"><i class="fa fa-check"></i><b>7.3.4</b> Normality</a></li>
<li class="chapter" data-level="7.3.5" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#unusual-observations"><i class="fa fa-check"></i><b>7.3.5</b> Unusual Observations</a></li>
<li class="chapter" data-level="7.3.6" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#linear-regression-assumptions-and-diagnostics-problem-set"><i class="fa fa-check"></i><b>7.3.6</b> Linear Regression Assumptions and Diagnostics Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.4</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="7.4.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#r-example"><i class="fa fa-check"></i><b>7.4.1</b> <em>R</em> Example</a></li>
<li class="chapter" data-level="7.4.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#multiple-linear-regression-problem-set"><i class="fa fa-check"></i><b>7.4.2</b> Multiple Linear Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#categorical-variables"><i class="fa fa-check"></i><b>7.5</b> Categorical Variables</a><ul>
<li class="chapter" data-level="7.5.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#categorical-linear-regression-problem-set"><i class="fa fa-check"></i><b>7.5.1</b> Categorical Linear Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#transformation"><i class="fa fa-check"></i><b>7.6</b> Transformation</a><ul>
<li class="chapter" data-level="7.6.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#identifying-non-linear-relationships"><i class="fa fa-check"></i><b>7.6.1</b> Identifying Non-Linear Relationships</a></li>
<li class="chapter" data-level="7.6.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#checking-model-structure"><i class="fa fa-check"></i><b>7.6.2</b> Checking Model Structure</a></li>
<li class="chapter" data-level="7.6.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#how-to-transform-variables-in-r"><i class="fa fa-check"></i><b>7.6.3</b> How To Transform Variables In <em>R</em></a></li>
<li class="chapter" data-level="7.6.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#transformed-regression-problem-set"><i class="fa fa-check"></i><b>7.6.4</b> Transformed Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>7.7</b> Logistic Regression</a><ul>
<li class="chapter" data-level="7.7.1" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#motivating-example"><i class="fa fa-check"></i><b>7.7.1</b> Motivating Example</a></li>
<li class="chapter" data-level="7.7.2" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logit-function"><i class="fa fa-check"></i><b>7.7.2</b> Logit Function</a></li>
<li class="chapter" data-level="7.7.3" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>7.7.3</b> Logistic Regression in <em>R</em></a></li>
<li class="chapter" data-level="7.7.4" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression-diagnostics"><i class="fa fa-check"></i><b>7.7.4</b> Logistic Regression Diagnostics</a></li>
<li class="chapter" data-level="7.7.5" data-path="fundamentals-of-regression.html"><a href="fundamentals-of-regression.html#logistic-regression-problem-set"><i class="fa fa-check"></i><b>7.7.5</b> Logistic Regression Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>8</b> Model Selection</a><ul>
<li class="chapter" data-level="8.1" data-path="model-selection.html"><a href="model-selection.html#admin-4"><i class="fa fa-check"></i><b>8.1</b> Admin</a></li>
<li class="chapter" data-level="8.2" data-path="model-selection.html"><a href="model-selection.html#introduction-7"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="model-selection.html"><a href="model-selection.html#testing-based-methods"><i class="fa fa-check"></i><b>8.3</b> Testing-Based Methods</a></li>
<li class="chapter" data-level="8.4" data-path="model-selection.html"><a href="model-selection.html#criterion-based-methods"><i class="fa fa-check"></i><b>8.4</b> Criterion-Based Methods</a><ul>
<li class="chapter" data-level="8.4.1" data-path="model-selection.html"><a href="model-selection.html#criterion-problem-set"><i class="fa fa-check"></i><b>8.4.1</b> Criterion Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="model-selection.html"><a href="model-selection.html#cross-validation"><i class="fa fa-check"></i><b>8.5</b> Cross Validation</a><ul>
<li class="chapter" data-level="8.5.1" data-path="model-selection.html"><a href="model-selection.html#what-about-the-test-set"><i class="fa fa-check"></i><b>8.5.1</b> What About The Test Set?</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="model-selection.html"><a href="model-selection.html#lasso-regression"><i class="fa fa-check"></i><b>8.6</b> Lasso Regression</a><ul>
<li class="chapter" data-level="8.6.1" data-path="model-selection.html"><a href="model-selection.html#background-reading"><i class="fa fa-check"></i><b>8.6.1</b> Background Reading</a></li>
<li class="chapter" data-level="8.6.2" data-path="model-selection.html"><a href="model-selection.html#lasso-regression-in-r"><i class="fa fa-check"></i><b>8.6.2</b> Lasso Regression In R</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="model-selection.html"><a href="model-selection.html#parting-thought"><i class="fa fa-check"></i><b>8.7</b> Parting Thought</a></li>
<li class="chapter" data-level="8.8" data-path="model-selection.html"><a href="model-selection.html#lasso-regression-problem-set"><i class="fa fa-check"></i><b>8.8</b> Lasso Regression Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="application-of-doe-to-the-advanced-warfighting-simulation.html"><a href="application-of-doe-to-the-advanced-warfighting-simulation.html"><i class="fa fa-check"></i><b>9</b> Application of DoE to the Advanced Warfighting Simulation</a></li>
<li class="chapter" data-level="10" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html"><i class="fa fa-check"></i><b>10</b> Advanced Experimental Designs</a><ul>
<li class="chapter" data-level="10.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#admin-5"><i class="fa fa-check"></i><b>10.1</b> Admin</a></li>
<li class="chapter" data-level="10.2" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#introduction-and-background"><i class="fa fa-check"></i><b>10.2</b> Introduction and Background</a></li>
<li class="chapter" data-level="10.3" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#central-composite-designs"><i class="fa fa-check"></i><b>10.3</b> Central Composite Designs</a><ul>
<li class="chapter" data-level="10.3.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#augmented-central-composite-designs"><i class="fa fa-check"></i><b>10.3.1</b> Augmented Central Composite Designs</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#response-surface-methodology"><i class="fa fa-check"></i><b>10.4</b> Response Surface Methodology</a><ul>
<li class="chapter" data-level="10.4.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#ccd-problem-set"><i class="fa fa-check"></i><b>10.4.1</b> CCD Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#nearly-orthogonal-latin-hypercube-designs"><i class="fa fa-check"></i><b>10.5</b> Nearly Orthogonal Latin Hypercube Designs</a><ul>
<li class="chapter" data-level="10.5.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#factor-codings"><i class="fa fa-check"></i><b>10.5.1</b> Factor Codings</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#orthogonality-and-variance-inflation-factors"><i class="fa fa-check"></i><b>10.6</b> Orthogonality and Variance Inflation Factors</a></li>
<li class="chapter" data-level="10.7" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#shifting-and-stacking"><i class="fa fa-check"></i><b>10.7</b> Shifting and Stacking</a><ul>
<li class="chapter" data-level="10.7.1" data-path="advanced-experimental-designs.html"><a href="advanced-experimental-designs.html#nolh-problem-set"><i class="fa fa-check"></i><b>10.7.1</b> NOLH Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>11</b> Non-Parametric Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#admin-6"><i class="fa fa-check"></i><b>11.1</b> Admin</a></li>
<li class="chapter" data-level="11.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#non-parametric-anova"><i class="fa fa-check"></i><b>11.2</b> Non-Parametric ANOVA</a><ul>
<li class="chapter" data-level="11.2.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#multiple-comparisons-2"><i class="fa fa-check"></i><b>11.2.1</b> Multiple Comparisons</a></li>
<li class="chapter" data-level="11.2.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#non-parametric-anova-problem-set"><i class="fa fa-check"></i><b>11.2.2</b> Non-Parametric ANOVA Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#generalized-additive-models"><i class="fa fa-check"></i><b>11.3</b> Generalized Additive Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#loess"><i class="fa fa-check"></i><b>11.3.1</b> Loess</a></li>
<li class="chapter" data-level="11.3.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#splines"><i class="fa fa-check"></i><b>11.3.2</b> Splines</a></li>
<li class="chapter" data-level="11.3.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#cross-validation-1"><i class="fa fa-check"></i><b>11.3.3</b> Cross Validation</a></li>
<li class="chapter" data-level="11.3.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#gam-problem-set"><i class="fa fa-check"></i><b>11.3.4</b> GAM Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#support-vector-machines"><i class="fa fa-check"></i><b>11.4</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="11.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#support-vector-regression"><i class="fa fa-check"></i><b>11.4.1</b> Support Vector Regression</a></li>
<li class="chapter" data-level="11.4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#support-vector-classification"><i class="fa fa-check"></i><b>11.4.2</b> Support Vector Classification</a></li>
<li class="chapter" data-level="11.4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#svm-problem-set"><i class="fa fa-check"></i><b>11.4.3</b> SVM Problem Set</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>11.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="11.5.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#regression-trees"><i class="fa fa-check"></i><b>11.5.1</b> Regression Trees</a></li>
<li class="chapter" data-level="11.5.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#random-forest-regression"><i class="fa fa-check"></i><b>11.5.2</b> Random Forest Regression</a></li>
<li class="chapter" data-level="11.5.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#random-forest-classification"><i class="fa fa-check"></i><b>11.5.3</b> Random Forest Classification</a></li>
<li class="chapter" data-level="11.5.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#cart-problem-set"><i class="fa fa-check"></i><b>11.5.4</b> CART Problem Set</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html"><i class="fa fa-check"></i><b>12</b> Optional Advanced DOE Topics</a><ul>
<li class="chapter" data-level="12.1" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#robust-design"><i class="fa fa-check"></i><b>12.1</b> Robust Design</a></li>
<li class="chapter" data-level="12.2" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#sequential-designs"><i class="fa fa-check"></i><b>12.2</b> Sequential Designs</a></li>
<li class="chapter" data-level="12.3" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#ridge-regression"><i class="fa fa-check"></i><b>12.3</b> Ridge Regression</a></li>
<li class="chapter" data-level="12.4" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#neural-network-regression"><i class="fa fa-check"></i><b>12.4</b> Neural Network Regression</a><ul>
<li class="chapter" data-level="12.4.1" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#simple-neural-network-model"><i class="fa fa-check"></i><b>12.4.1</b> Simple Neural Network Model</a></li>
<li class="chapter" data-level="12.4.2" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#gradient-descent"><i class="fa fa-check"></i><b>12.4.2</b> Gradient Descent</a></li>
<li class="chapter" data-level="12.4.3" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>12.4.3</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="12.4.4" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#hidden-layer"><i class="fa fa-check"></i><b>12.4.4</b> Hidden Layer</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#neural-network-classification"><i class="fa fa-check"></i><b>12.5</b> Neural Network Classification</a></li>
<li class="chapter" data-level="12.6" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>12.6</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="12.7" data-path="optional-advanced-doe-topics.html"><a href="optional-advanced-doe-topics.html#non-parametric-statistics"><i class="fa fa-check"></i><b>12.7</b> Non-Parametric Statistics</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown">
Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Design of Experiments for Modeling and Simulation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fundamentals-of-regression" class="section level1">
<h1><span class="header-section-number">7</span> Fundamentals of Regression</h1>
<div id="admin-3" class="section level2">
<h2><span class="header-section-number">7.1</span> Admin</h2>
<p>For any errors associated with this section, please contact <a href="mailto:john.f.king1.mil@mail.mil">John King</a> or <a href="mailto:stephen.e.gillespie.mil@mail.mil">Steve Gillespie</a>.</p>
<p>This chapter was published using the following software:</p>
<ul>
<li>R version 3.6.0 (2019-04-26).</li>
<li>On x86_64-pc-linux-gnu (64-bit) running Ubuntu 18.04.2 LTS.</li>
<li>Packages used in this chapter are explicitly shown in the code snippets.</li>
</ul>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">7.2</span> Simple Linear Regression</h2>
<p>The purpose of regression is to describe a relationship that explains one variable (the <em>response</em>, or the y variable) based on one or more other variables (the <em>predictors</em>, or the x variables). The simplest deterministic mathematical relationship between two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is a linear relationship, which we define as:</p>
<p><span class="math display">\[y = \beta_{0} + \beta_{1}x + \varepsilon\]</span></p>
<p>where,</p>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span> is the y-intercept</li>
<li><span class="math inline">\(\beta_{1}\)</span> is the slope</li>
<li><span class="math inline">\(\varepsilon\)</span> is the error in <span class="math inline">\(y\)</span> not explained by <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>.</li>
</ul>
<p>If there is no error in the model and a linear relationship exists, we could predict the true value of y given any value of x. With error, however, we can only estimate y, which we annotate by <span class="math inline">\(\hat{y}\)</span>. The regression line itself is determined using the <em>least squares</em> method, which involves drawing a line through the centroid of the data, and adjusting the slope until the squared distance between the straight line and each observed value (the <em>residual</em>) is minimized. For example, assume we have the following observations of height in inches (predictor) and weight in pounds (response).</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb534-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb534-2" data-line-number="2"><span class="kw">library</span>(GGally)</a>
<a class="sourceLine" id="cb534-3" data-line-number="3"></a>
<a class="sourceLine" id="cb534-4" data-line-number="4">df =<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb534-5" data-line-number="5">  <span class="dt">height =</span> <span class="kw">c</span>(<span class="dv">66</span>, <span class="dv">54</span>, <span class="dv">50</span>, <span class="dv">74</span>, <span class="dv">59</span>, <span class="dv">53</span>),</a>
<a class="sourceLine" id="cb534-6" data-line-number="6">  <span class="dt">weight =</span> <span class="kw">c</span>(<span class="dv">141</span>, <span class="dv">128</span>, <span class="dv">123</span>, <span class="dv">160</span>, <span class="dv">136</span>, <span class="dv">139</span>)</a>
<a class="sourceLine" id="cb534-7" data-line-number="7">)</a></code></pre></div>
<div id="least-squares-method-manually" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Least Squares Method Manually</h3>
<p>The centroid coordinates <span class="math inline">\((\bar{x},\bar{y})\)</span> are calculated simply by <span class="math inline">\(\bar{x}\)</span> = <code>mean(df$height)</code> = 59.33, and <span class="math inline">\(\bar{y}\)</span> = <code>mean(df$weight)</code> = 137.83. Plotting the data with the centroid, we get:</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>To find the slope, <span class="math inline">\(\beta_{1}\)</span>, we calculate how much each height and weight observation deviate from the centroid, multiply those paired deviations, sum them, and divide that by the sums of the squared height deviations. With the height and weight data, we find:</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb535-1" data-line-number="1">df =<span class="st"> </span>df <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb535-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb535-3" data-line-number="3">    <span class="dt">h_dev =</span> height <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(height),    <span class="co"># height deviation from centroid</span></a>
<a class="sourceLine" id="cb535-4" data-line-number="4">    <span class="dt">w_dev =</span> weight <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(weight),    <span class="co"># weight deviation from centroid</span></a>
<a class="sourceLine" id="cb535-5" data-line-number="5">    <span class="dt">dev_prod =</span> h_dev <span class="op">*</span><span class="st"> </span>w_dev,         <span class="co"># the product of the deviations</span></a>
<a class="sourceLine" id="cb535-6" data-line-number="6">    <span class="dt">h_dev_squared =</span> h_dev<span class="op">^</span><span class="dv">2</span>           <span class="co"># the squared products</span></a>
<a class="sourceLine" id="cb535-7" data-line-number="7">  )</a></code></pre></div>
<table>
<colgroup>
<col width="8%" />
<col width="8%" />
<col width="17%" />
<col width="17%" />
<col width="21%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th>height</th>
<th>weight</th>
<th>height deviance</th>
<th>weight deviance</th>
<th>deviation products</th>
<th>height deviance squared</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>x</td>
<td>y</td>
<td><span class="math inline">\(x_{i}-\bar{x}\)</span></td>
<td><span class="math inline">\(y_{i}-\bar{y}\)</span></td>
<td><span class="math inline">\((x_{i}-\bar{x})(y_{i}-\bar{y})\)</span></td>
<td><span class="math inline">\((x_{i}-\bar{x})^{2}\)</span></td>
</tr>
<tr class="even">
<td>66</td>
<td>141</td>
<td>6.67</td>
<td>3.17</td>
<td>21.14</td>
<td>44.49</td>
</tr>
<tr class="odd">
<td>54</td>
<td>128</td>
<td>-5.33</td>
<td>-9.83</td>
<td>52.39</td>
<td>28.41</td>
</tr>
<tr class="even">
<td>50</td>
<td>123</td>
<td>-9.33</td>
<td>-14.83</td>
<td>138.36</td>
<td>87.05</td>
</tr>
<tr class="odd">
<td>74</td>
<td>160</td>
<td>14.67</td>
<td>22.17</td>
<td>325.23</td>
<td>215.21</td>
</tr>
<tr class="even">
<td>59</td>
<td>136</td>
<td>-0.33</td>
<td>-1.83</td>
<td>0.60</td>
<td>0.11</td>
</tr>
<tr class="odd">
<td>53</td>
<td>139</td>
<td>-6.33</td>
<td>1.17</td>
<td>-7.41</td>
<td>40.07</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\bar{x} = 59.33\)</span></td>
<td><span class="math inline">\(\bar{y} = 137.83\)</span></td>
<td>-</td>
<td>-</td>
<td><span class="math inline">\(\Sigma = 530.33\)</span></td>
<td><span class="math inline">\(\Sigma = 415.33\)</span></td>
</tr>
</tbody>
</table>
<p>The slope is found by <span class="math inline">\(\beta_{1} = \frac{\Sigma (x_{i}-\bar{x})(y_{i}-\bar{y})}{\Sigma (x_{i}-\bar{x})^{2}} = \frac{530.33}{415.33} = 1.28\)</span>. For our dataset in <em>R</em>, that translates to:</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb536-1" data-line-number="1">beta1 =<span class="st"> </span><span class="kw">sum</span>(df<span class="op">$</span>dev_prod) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(df<span class="op">$</span>h_dev_squared)</a>
<a class="sourceLine" id="cb536-2" data-line-number="2">beta1</a></code></pre></div>
<pre><code>## [1] 1.276886</code></pre>
<p>We have now have what we need to calculate the y-intercept, <span class="math inline">\(\beta_{0} =\bar{y}-\beta{_1}\bar{x}\)</span>. Equivalently in <em>R</em>:</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb538-1" data-line-number="1">beta0 =<span class="st"> </span><span class="kw">mean</span>(df<span class="op">$</span>weight) <span class="op">-</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(df<span class="op">$</span>height)</a>
<a class="sourceLine" id="cb538-2" data-line-number="2">beta0</a></code></pre></div>
<pre><code>## [1] 62.07143</code></pre>
<p>When we plot the line defined by our beta values, we find that it does, in fact, pass through the centroid and visually appears to fit the data.</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb540-1" data-line-number="1"><span class="kw">ggplot</span>(<span class="dt">data=</span>df, <span class="kw">aes</span>(<span class="dt">x=</span>height, <span class="dt">y=</span>weight)) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb540-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>beta0, <span class="dt">slope=</span>beta1, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="co"># note the explicit use of our betas</span></a>
<a class="sourceLine" id="cb540-4" data-line-number="4"><span class="st">  </span><span class="co">#geom_smooth(formula=y~x, method=&quot;lm&quot;, se=FALSE) + # how it&#39;s normally done in practice</span></a>
<a class="sourceLine" id="cb540-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> centroid, <span class="kw">aes</span>(<span class="dt">x=</span>height, <span class="dt">y=</span>weight), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>, <span class="dt">size=</span><span class="dv">5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-6" data-line-number="6"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="dv">61</span>, <span class="dt">y=</span><span class="dv">138</span>, <span class="dt">label=</span><span class="st">&quot;centroid&quot;</span>, <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-7" data-line-number="7"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="fl">67.5</span>, <span class="dt">y=</span><span class="fl">143.5</span>, <span class="dt">label=</span><span class="st">&quot;residual&quot;</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-8" data-line-number="8"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;segment&quot;</span>, <span class="dt">x=</span><span class="dv">50</span>, <span class="dt">xend=</span><span class="dv">50</span>, <span class="dt">y=</span><span class="dv">123</span>, <span class="dt">yend=</span><span class="fl">125.5</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-9" data-line-number="9"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;segment&quot;</span>, <span class="dt">x=</span><span class="dv">53</span>, <span class="dt">xend=</span><span class="dv">53</span>, <span class="dt">y=</span><span class="dv">139</span>, <span class="dt">yend=</span><span class="fl">129.5</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-10" data-line-number="10"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;segment&quot;</span>, <span class="dt">x=</span><span class="dv">54</span>, <span class="dt">xend=</span><span class="dv">54</span>, <span class="dt">y=</span><span class="dv">128</span>, <span class="dt">yend=</span><span class="fl">131.5</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-11" data-line-number="11"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;segment&quot;</span>, <span class="dt">x=</span><span class="dv">59</span>, <span class="dt">xend=</span><span class="dv">59</span>, <span class="dt">y=</span><span class="dv">136</span>, <span class="dt">yend=</span><span class="fl">137.5</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-12" data-line-number="12"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;segment&quot;</span>, <span class="dt">x=</span><span class="dv">66</span>, <span class="dt">xend=</span><span class="dv">66</span>, <span class="dt">y=</span><span class="dv">141</span>, <span class="dt">yend=</span><span class="fl">146.5</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-13" data-line-number="13"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;segment&quot;</span>, <span class="dt">x=</span><span class="dv">74</span>, <span class="dt">xend=</span><span class="dv">74</span>, <span class="dt">y=</span><span class="dv">160</span>, <span class="dt">yend=</span><span class="fl">156.5</span>, <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb540-14" data-line-number="14"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>This method minimizes the residual sum of squares (RSS), which is represented mathematically by:</p>
<p><span class="math display">\[RSS = \sum\limits_{i=1}^{n}{(y_{i} - \hat{y}_{i})^2} = \sum\limits_{i=1}^{n}{\hat\varepsilon_{i}^{2}}\]</span></p>
<p>and is calculated as follows.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb541-1" data-line-number="1">df =<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb541-2" data-line-number="2">  <span class="dt">y_hat =</span> beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>height,</a>
<a class="sourceLine" id="cb541-3" data-line-number="3">  <span class="dt">error =</span> weight <span class="op">-</span><span class="st"> </span>y_hat,</a>
<a class="sourceLine" id="cb541-4" data-line-number="4">  <span class="dt">error_squared =</span> error<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb541-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb541-6" data-line-number="6"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;RSS =&quot;</span>, <span class="kw">round</span>(<span class="kw">sum</span>(df<span class="op">$</span>error_squared), <span class="dv">2</span>)))</a></code></pre></div>
<pre><code>## [1] &quot;RSS = 145.66&quot;</code></pre>
<table>
<thead>
<tr class="header">
<th>height</th>
<th>weight</th>
<th>predicted weight</th>
<th>error</th>
<th>squared error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>x</td>
<td>y</td>
<td><span class="math inline">\(\hat{y}_{i}\)</span></td>
<td><span class="math inline">\(y-\hat{y}_{i}\)</span></td>
<td><span class="math inline">\((y-\hat{y}_{i})^2\)</span></td>
</tr>
<tr class="even">
<td>66</td>
<td>141</td>
<td>146.35</td>
<td>-5.35</td>
<td>28.58</td>
</tr>
<tr class="odd">
<td>54</td>
<td>128</td>
<td>131.02</td>
<td>-3.02</td>
<td>9.14</td>
</tr>
<tr class="even">
<td>50</td>
<td>123</td>
<td>125.92</td>
<td>-2.92</td>
<td>8.50</td>
</tr>
<tr class="odd">
<td>74</td>
<td>160</td>
<td>156.56</td>
<td>3.44</td>
<td>11.83</td>
</tr>
<tr class="even">
<td>59</td>
<td>136</td>
<td>137.41</td>
<td>-1.41</td>
<td>1.98</td>
</tr>
<tr class="odd">
<td>53</td>
<td>139</td>
<td>129.75</td>
<td>9.25</td>
<td>85.63</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\bar{x} = 59.33\)</span></td>
<td><span class="math inline">\(\bar{y} = 137.83\)</span></td>
<td>-</td>
<td>-</td>
<td><span class="math inline">\(RSS: \Sigma = 145.66\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="goodness-of-fit" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Goodness of Fit</h3>
<p>While RSS gives us an idea of how well the regression prediction (<span class="math inline">\(\hat{y}\)</span>) can approximate the response (<span class="math inline">\(y\)</span>), it does not tell us how well the model fits the data because it has the same units as <span class="math inline">\(y\)</span>. To obtain a unitless measure of fit, <span class="math inline">\(R^2\)</span> (also called the coefficient of determination), RSS is divided by the total sum of squares (TSS), and that ratio is substracted from 1.</p>
<p><span class="math display">\[R^2 = 1- \frac{RSS}{TSS} = 1 - \frac{\Sigma(y_{i} - \hat{y}_{i})^2}{\Sigma(y_{i} - \bar{y}_{i})^2}\]</span></p>
<p>We calculate <span class="math inline">\(R^2\)</span> for the height/weight data as follows:</p>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb543-1" data-line-number="1"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(df<span class="op">$</span>error_squared) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(df<span class="op">$</span>w_dev<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 0.8229798</code></pre>
<p>We interpret <span class="math inline">\(R^2\)</span> as the proportion of weight variation explained by the linear model. As a proportion, <span class="math inline">\(R^2\)</span> varies from 0 to 1, and ideally we seek models with a high <span class="math inline">\(R^2\)</span>. A graphical depiction of RSS and TSS for one of the residuals illustrates their relationship.</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="least-squares-method-in-r" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Least Squares Method In <em>R</em></h3>
<p>That was a fair amount of work, and of course <em>R</em> simplifies the process. Fortunately, the syntax for creating a linear model is very similar to ANOVA.</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb545-1" data-line-number="1">df.lm =<span class="st"> </span><span class="kw">lm</span>(weight <span class="op">~</span><span class="st"> </span>height, <span class="dt">data=</span>df)</a>
<a class="sourceLine" id="cb545-2" data-line-number="2"><span class="kw">summary</span>(df.lm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = weight ~ height, data = df)
## 
## Residuals:
##      1      2      3      4      5      6 
## -5.346 -3.023 -2.916  3.439 -1.408  9.254 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)  62.0714    17.7405   3.499   0.0249 *
## height        1.2769     0.2961   4.312   0.0125 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.034 on 4 degrees of freedom
## Multiple R-squared:  0.823,  Adjusted R-squared:  0.7787 
## F-statistic:  18.6 on 1 and 4 DF,  p-value: 0.01252</code></pre>
<p>Going through each section of the output from top to bottom:</p>
<p><strong>Call:</strong> This is simply the formula we gave the <code>lm()</code> function.</p>
<p><strong>Residuals:</strong> The residuals in order of observation.</p>
<p><strong>Coefficients:</strong></p>
<ul>
<li><code>Estimate</code> These are the <span class="math inline">\(\beta\)</span>s, and we see that they match our values calculated above.</li>
<li><code>Std. Error</code> is the standard deviation of each <code>Estimate</code> (or <span class="math inline">\(\beta\)</span>) and can be used to determine the 95% confidence interval (CI). For example, the 95% CI for <code>height</code> is <span class="math inline">\(1.28 \pm 1.96(0.296)\)</span>.</li>
<li><code>t value</code> is <code>Estimate</code> / <code>Std. Error</code>.</li>
<li><code>Pr(&gt;|t|)</code> is the probability of observing a value at least as extreme as <span class="math inline">\(\beta\)</span> using a t-distribution and <span class="math inline">\((n-p-1)\)</span> degrees of freedom. The null hypothesis is <span class="math inline">\(H_{o}: \beta_{i}=0\)</span>. Notice that this is a test of each individual predictor.</li>
</ul>
<!-- Steve comment:  If I'm not mistaken, R doesn't use a z test if you calculate CIs on your estimates.  I think it uses a t test (which, with enough observations is about the same).  It might be worth noting that this CI estimate is similar to what we did in chapter 2.  see confint section below-->
<p><strong>Residual standard error:</strong> The square root of RSS divided by the difference of number of observations and the number of predictors. Or, <span class="math inline">\(RSE = \sqrt{\frac{RSS}{n-p}}\)</span>. Degrees of freedom is <span class="math inline">\((n-p-1)\)</span>.</p>
<p><strong>Multiple R-squared:</strong> The <span class="math inline">\(R^2\)</span> we calculated above.</p>
<p><strong>Adjusted R-squared:</strong>Normalizes <span class="math inline">\(R^2\)</span> by accounting for the number of observations and predictors in the model. When conducting multiple linear regression, this is the appropriate method of measuring goodness of fit. Adjusted r-squared is calculated by: <span class="math inline">\(\bar{R}^{2} = 1 - (1 - R^{2}) \frac{n-1}{n-p-1}\)</span>.</p>
<p><strong>F-statistic:</strong> The global test of significance where we wish to determine if <em>at least one</em> predictor is sginificant. The null hypothesis is <span class="math inline">\(H_{o}: \beta_{1}=...=\beta_{p-1}=0\)</span> under the F-distribution with <span class="math inline">\(p-1\)</span> and <span class="math inline">\(n-p-1\)</span> degrees of freedom.</p>
<p>We interpret the linear model in the following manner: <strong>for every inch increase in height, we predict a persons weight increases by 1.28 pounds</strong>.</p>
<p><em>R</em> allows us to do several things with a model. We can use <em>R</em> to give us a confidence interval on our coefficients using <code>confint</code>:</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb547-1" data-line-number="1"><span class="co"># Note confint has several options</span></a>
<a class="sourceLine" id="cb547-2" data-line-number="2"><span class="kw">confint</span>(df.lm)</a></code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) 12.8158877 111.326969
## height       0.4547796   2.098992</code></pre>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb549-1" data-line-number="1"><span class="co"># Note this is more precise than the Z estimation shown above as it accounts for our sample size and uses a t test</span></a></code></pre></div>
<p>We can also predict results using <code>predict</code> Predict can either give you a point estimate, or an interval based on either the mean predictions (using <code>'confidence'</code>) or a single point (using <code>'prediction'</code>). You can read more about these options <a href = 'http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/'>here</a>.</p>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb550-1" data-line-number="1"><span class="kw">predict</span>(df.lm, <span class="kw">list</span>(<span class="dt">height =</span> <span class="dv">66</span>))</a></code></pre></div>
<pre><code>##        1 
## 146.3459</code></pre>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb552-1" data-line-number="1"><span class="kw">predict</span>(df.lm, <span class="kw">list</span>(<span class="dt">height =</span> <span class="dv">66</span>), <span class="dt">interval =</span> <span class="st">&#39;confidence&#39;</span>)</a></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 146.3459 137.5811 155.1108</code></pre>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb554-1" data-line-number="1"><span class="kw">predict</span>(df.lm, <span class="kw">list</span>(<span class="dt">height =</span> <span class="dv">66</span>), <span class="dt">interval =</span> <span class="st">&#39;prediction&#39;</span>)</a></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 146.3459 127.4375 165.2544</code></pre>
<p>Note, <em>R</em> will not prevent one from extrapolating beyond the data. Predicting a result on values outside the observed data is bad practice and should generally be avoided.</p>
<p>Finally, one can plot the results and a regression quite simply with <code>ggplot</code> using <code>stat_smooth</code>:</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb556-1" data-line-number="1"><span class="kw">ggplot</span>(<span class="dt">data =</span> df, <span class="kw">aes</span>(<span class="dt">x =</span> height, <span class="dt">y =</span> weight)) <span class="op">+</span><span class="st"> </span><span class="co"># Provide your data and aesthetics as usual</span></a>
<a class="sourceLine" id="cb556-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st">  </span><span class="co"># plot the observations</span></a>
<a class="sourceLine" id="cb556-3" data-line-number="3"><span class="st">  </span><span class="co"># stat_smooth creates a linear regression based on your given x and y values (i.e. lm(y~x))</span></a>
<a class="sourceLine" id="cb556-4" data-line-number="4"><span class="st">  </span><span class="co"># you can also plot the standard error</span></a>
<a class="sourceLine" id="cb556-5" data-line-number="5"><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method =</span> <span class="st">&#39;lm&#39;</span>, <span class="dt">se =</span> T) </a></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="simple-linear-regression-problem-set" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Simple Linear Regression Problem Set</h3>
<p><strong>For this problem set, you will have to read the next section on assumptions and diagnostics if you are not familiar with the assumptions necessary for simple linear regression.</strong></p>
<p>The problem set for this section is located <a href = '/_Chapter6_ProblemSets/Simple_Linear_Regression_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter6_ProblemSets/Simple_Linear_Regression_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter6_ProblemSets/Simple_Linear_Regression_PS_Answers.html'>here</a>.</p>
</div>
</div>
<div id="assumptions-and-diagnostics" class="section level2">
<h2><span class="header-section-number">7.3</span> Assumptions and Diagnostics</h2>
<p>There are four assumptions fundamental to linear regression:</p>
<ol style="list-style-type: decimal">
<li><strong>Linearity:</strong> The relationship between x and the mean of y is linear.</li>
<li><strong>Homoscedasticity:</strong> The variance of residual is the same for any value of x (i.e, constant variance).</li>
<li><strong>Independence:</strong> Independence of the prediction error from every one of the predictor variables.</li>
<li><strong>Normality:</strong> The prediction error is normally distributed.</li>
</ol>
<p>When conducting linear regression, we need to always perform diagnostic check to ensure we are not violating any of the inherent assumptions.</p>
<div id="linearity" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Linearity</h3>
<p>The assumption is that the relationship between x and the mean of y is linear, but what does that mean exactly? A regression model is linear if <span class="math inline">\(E[Y|X =x]\)</span> is a linear function <strong>of the <span class="math inline">\(\beta\)</span> parameters</strong>, not of <span class="math inline">\(x\)</span>. That means each of the following is a linear model:</p>
<ul>
<li><span class="math inline">\(\beta_{0} + \beta_{1}x\)</span> (note<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a>)</li>
<li><span class="math inline">\(\beta_{0} + \beta_{1}x + \beta_{2}x^{2} + \beta_{3}x^{3}\)</span></li>
<li><span class="math inline">\(\beta_{0} + \beta_{1}log(x) + \beta_{2}sin(x)\)</span></li>
</ul>
<p>These are <em>not</em> linear models:</p>
<ul>
<li><span class="math inline">\(\beta_{0} + x^{\beta_{1}}\)</span>
<p>

</p></li>
<li><span class="math inline">\(\frac{1}{\beta_{0} + \beta_{1}x}\)</span>
<p>

</p></li>
<li><span class="math inline">\(\frac{e^{\beta_{0}+\beta_{1}x_{1}}}{1+e^{\beta_{0}+\beta_{1}x_{1}}}\)</span> (note<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a>)</li>
</ul>
<p>As with ANOVA, <em>R</em> produces diagnostic plots for objects created by the <code>lm()</code> function. The first plot may be used to evaluate both linearity and homoscedasticity. A linear relationship will be indicated by a (relatively) horizontal red line on the plot. Since our height-weight data is so simple, well switch to the <code>teengamb</code> dataset from the <code>faraway</code> package. This dataset consists of four predictor variables and one response (<code>gamble</code>). Read the help for <code>teengamb</code> to familiarize youself with the data. Since one of the predictors is binary (<code>sex</code>), well exclude it for this example.<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a> A summary of the resulting linear model is as follows.</p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb558-1" data-line-number="1"><span class="kw">library</span>(faraway)</a></code></pre></div>
<pre><code>## 
## Attaching package: &#39;faraway&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:GGally&#39;:
## 
##     happy</code></pre>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb561-1" data-line-number="1">tg.lm =<span class="st"> </span><span class="kw">lm</span>(gamble <span class="op">~</span><span class="st"> </span>. <span class="op">-</span>sex, <span class="dt">data=</span>teengamb)</a>
<a class="sourceLine" id="cb561-2" data-line-number="2"><span class="kw">summary</span>(tg.lm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gamble ~ . - sex, data = teengamb)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -49.649 -12.008  -1.242   8.239 103.390 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -1.3044    15.7760  -0.083   0.9345    
## status        0.4701     0.2509   1.873   0.0678 .  
## income        5.7707     1.0494   5.499 1.95e-06 ***
## verbal       -4.1211     2.2785  -1.809   0.0775 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 24.28 on 43 degrees of freedom
## Multiple R-squared:  0.445,  Adjusted R-squared:  0.4062 
## F-statistic: 11.49 on 3 and 43 DF,  p-value: 1.161e-05</code></pre>
<p>The diagnostic plot to check the linearity assumption is the first plot returned, and we see a slight U shape to the red line. Notice that there are only three observations on the far right which appear to be heavily influencing the results. The conical spread of the data also strongly suggests heteroscedasticity might be an issue.</p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb563-1" data-line-number="1"><span class="kw">plot</span>(tg.lm, <span class="dt">which =</span> <span class="dv">1</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Another screening method is with a paris plot, which we can quickly produce in base <em>R</em> with <code>pairs()</code>. This is a great way to do a quick check potential nonlinear relationships between pairs of variables. This is a screening method only, however, because were projecting onto two dimensions, so we may be missing things lurking in higher dimensions.</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb564-1" data-line-number="1"><span class="kw">pairs</span>(teengamb[, <span class="dv">2</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">upper.panel=</span><span class="ot">NULL</span>, <span class="dt">lower.panel=</span>panel.smooth)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>If evidence of a nonlinear relationship exists, a linear model can still be used; however, either the response variable or one or more of the predictors must be transformed. This topic is covered in detail in the Advanced Designs chapter.</p>
</div>
<div id="homoscedasticity" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Homoscedasticity</h3>
<p>The procedure for testing constant variance in residuals in a linear model is similar to ANOVA. A plot of residuals versus fitted values is shown two plots ago, and we can look at the square root of standardized residuals versus fitted values. Both plots show strong evidence of heteroscedasticity.</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb565-1" data-line-number="1"><span class="kw">plot</span>(tg.lm, <span class="dt">which =</span> <span class="dv">3</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>There is no doubt some subjectivity to visual inspections. As a guide, consider the next three sets of plots that show constance variance, mild heteroscedasticity, and strong heteroscedasticity.</p>
<p>Constant variance:</p>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb566-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb566-2" data-line-number="2"></a>
<a class="sourceLine" id="cb566-3" data-line-number="3">n &lt;-<span class="st"> </span><span class="dv">50</span> </a>
<a class="sourceLine" id="cb566-4" data-line-number="4"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) {x &lt;-<span class="st"> </span><span class="kw">runif</span>(n); <span class="kw">plot</span>(x,<span class="kw">rnorm</span>(n))} </a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Mild heteroscedasticity:</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb567-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb567-2" data-line-number="2"></a>
<a class="sourceLine" id="cb567-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) {x &lt;-<span class="st"> </span><span class="kw">runif</span>(n); <span class="kw">plot</span>(x,<span class="kw">sqrt</span>((x))<span class="op">*</span><span class="kw">rnorm</span>(n))} </a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Strong heteroscedasticity:</p>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb568-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>, <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb568-2" data-line-number="2"></a>
<a class="sourceLine" id="cb568-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) {x &lt;-<span class="st"> </span><span class="kw">runif</span>(n); <span class="kw">plot</span>(x,x<span class="op">*</span><span class="kw">rnorm</span>(n))}</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>The linear model analog to the Levene test is the Breusch-Pagan test. The null hypothesis is that the residuals have constant variance, and the alternative is that the error variance changes with the level of the response or with a linear combination of predictors. The <code>ncvTest()</code> from the <code>car</code> (companion to applied regression) package performs the test, and when applied to the <code>tg.lm</code> object confirms our suspicion of non-constant variance based on our visual inspection.</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb569-1" data-line-number="1">car<span class="op">::</span><span class="kw">ncvTest</span>(tg.lm)</a></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 26.18623, Df = 1, p = 3.1003e-07</code></pre>
</div>
<div id="independence" class="section level3">
<h3><span class="header-section-number">7.3.3</span> Independence</h3>
<p>The concept of indepedent (and identically distributed) data was covered in the statistics review and ANOVA chapters. It is no different when conducting linear regression and so will not be repeated here.</p>
</div>
<div id="normality" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Normality</h3>
<p>Again, checking whether the residuals are normally distributed is the same for linear regression as for ANOVA. Create a Q-Q plot and apply the Shapiro-Wilk test as shown below.</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb571-1" data-line-number="1"><span class="kw">plot</span>(tg.lm, <span class="dt">which=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb572-1" data-line-number="1"><span class="kw">shapiro.test</span>(<span class="kw">residuals</span>(tg.lm))</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(tg.lm)
## W = 0.8651, p-value = 6.604e-05</code></pre>
</div>
<div id="unusual-observations" class="section level3">
<h3><span class="header-section-number">7.3.5</span> Unusual Observations</h3>
<p>Although not an assumption inherent to a linear model, its good practice to also check for unusual observations when performing diagnostic checks. There are two types of unusual observations: outliers and influential. An <em>outlier</em> is an observation with a large residual - it plots substantially above or below the regression line. An <em>influential observation</em> is one that substantially changes the model fit. Keep in mind that it is possible for an observation to have both characteristics. Examples Both types of observations are shown on the following plot (note that I rigged observations 11 and 12 to be unsual observations).</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Its not necessarily bad to have unusual observations, but its good practice to check for them, and, if found, decide what to do about them. A point with high <em>leverage</em> falls within the predictor space but is significantly separated from the other points. It has the potential to influence the fit but may not actually do so.</p>
<div id="leverage-points" class="section level4">
<h4><span class="header-section-number">7.3.5.1</span> Leverage Points</h4>
<p>The amount of leverage associated with each observation is called the <em>hat value</em> and are the diagonal elements of the <em>hat matrix</em>, which you can read more about <a href="https://www.sciencedirect.com/topics/mathematics/hat-matrix">here</a>, if youre interested (or just really like linear algebra). The gist of it is that the sum of the hat values equals the number of observations. If every observation has exactly the same leverage, then the hat values will all equal <span class="math inline">\(p/n\)</span>, where p is the number of parameters and n is the number of observations (in our example we just have two parameters, so its <span class="math inline">\(2/n\)</span>). Increasing the hat value of one observation necessitates decreasing the hat values of the others, so were essentially looking for hat values significantly greater than this theoretical average. The generally accepted rule of thumb is that hat values greater than ~ <span class="math inline">\(2p/n\)</span> times the averages should be looked at more carefully. Extracting hat values from a linear model in <em>R</em> is done using the <code>hatvalues()</code> or <code>influence()</code> functions.</p>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb574-1" data-line-number="1">df.lm =<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x, <span class="dt">data=</span>df)</a>
<a class="sourceLine" id="cb574-2" data-line-number="2">hatv =<span class="st"> </span><span class="kw">hatvalues</span>(df.lm)</a>
<a class="sourceLine" id="cb574-3" data-line-number="3"><span class="kw">print</span>(hatv)</a></code></pre></div>
<pre><code>##          1          2          3          4          5          6          7 
## 0.14483261 0.12736536 0.11280932 0.10116448 0.09243086 0.08660844 0.08369723 
##          8          9         10         11         12 
## 0.08369723 0.08660844 0.09243086 0.10116448 0.88719068</code></pre>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb576-1" data-line-number="1"><span class="kw">influence</span>(df.lm)<span class="op">$</span>hat</a></code></pre></div>
<pre><code>##          1          2          3          4          5          6          7 
## 0.14483261 0.12736536 0.11280932 0.10116448 0.09243086 0.08660844 0.08369723 
##          8          9         10         11         12 
## 0.08369723 0.08660844 0.09243086 0.10116448 0.88719068</code></pre>
<p>Verify that the sum of the hat values equals the number of parameters (2):</p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb578-1" data-line-number="1"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Sum of hat values:&quot;</span>, <span class="kw">sum</span>(hatv)))</a></code></pre></div>
<pre><code>## [1] &quot;Sum of hat values: 2&quot;</code></pre>
<p>Are any hat values &gt; <span class="math inline">\(2p/n\)</span> (recall I rigged observation 12)?</p>
<div class="sourceCode" id="cb580"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb580-1" data-line-number="1">hatv <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span><span class="op">/</span><span class="kw">length</span>(df<span class="op">$</span>x)</a></code></pre></div>
<pre><code>##     1     2     3     4     5     6     7     8     9    10    11    12 
## FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE</code></pre>
<p>A graphical way of looking at leverage is with the <code>halfnorm()</code> function in the <code>faraway</code> package, which plots leverage against the positive normal quantiles. I added a red line to indicate the rule of thumb threshold.</p>
<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb582-1" data-line-number="1">faraway<span class="op">::</span><span class="kw">halfnorm</span>(hatv,<span class="dt">ylab=</span><span class="st">&quot;Leverages&quot;</span>)</a>
<a class="sourceLine" id="cb582-2" data-line-number="2"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">2</span><span class="op">*</span><span class="kw">mean</span>(hatv), <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Another measure of leaverage is <em>Cooks Distance</em>, defined as:</p>
<p><span class="math display">\[D_{i}=\frac{r^{2}_{i}}{p}\left(\frac{h_{i}}{1-h_{i}}\right)\]</span></p>
<p>The rule of thumb for Cooks Distance is an observation with <span class="math inline">\(D&gt;1\)</span>, and we can get these values in <em>R</em> with <code>cooks.distance()</code>.</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb583-1" data-line-number="1"><span class="kw">cooks.distance</span>(df.lm)</a></code></pre></div>
<pre><code>##            1            2            3            4            5            6 
## 6.193970e-02 4.398193e-05 6.553697e-04 4.789149e-10 3.736810e-03 1.917568e-02 
##            7            8            9           10           11           12 
## 3.490432e-03 4.701066e-02 1.130421e-02 9.893220e-02 3.409595e-01 1.703556e+01</code></pre>
<p>The fourth linear model plot also contains Cooks Distance.</p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb585-1" data-line-number="1"><span class="kw">plot</span>(df.lm, <span class="dt">which=</span><span class="dv">4</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</div>
<div id="outliers" class="section level4">
<h4><span class="header-section-number">7.3.5.2</span> Outliers</h4>
<p>The hat values from the previous section are also used to calculate <em>standardized residuals</em>, <span class="math inline">\(r_{i}\)</span>.</p>
<p><span class="math display">\[r_{i}=\frac{\hat{\varepsilon}_{i} }{\hat{\sigma}\sqrt{1-h_{i}}}, i=1,...,n \]</span></p>
<p>where <span class="math inline">\(\hat{\varepsilon}\)</span> are the residuals, <span class="math inline">\(\hat{\sigma}\)</span> is the estimated residual standard error, and <span class="math inline">\(h\)</span> is the leverage. The rule of thumb for identifying unusually large standardised residuals is if <span class="math inline">\(|r_{i}| &gt; 2\)</span>. We can get standardized residuals in <em>R</em> with <code>rstandard()</code>.</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb586-1" data-line-number="1"><span class="kw">rstandard</span>(df.lm)</a></code></pre></div>
<pre><code>##             1             2             3             4             5 
##  8.552478e-01 -2.454950e-02  1.015300e-01 -9.225082e-05 -2.708924e-01 
##             6             7             8             9            10 
## -6.359731e-01 -2.764512e-01 -1.014559e+00 -4.882963e-01 -1.393847e+00 
##            11            12 
##  2.461458e+00  2.081408e+00</code></pre>
<p>Here we see that observation 11 is a potential outlier, and the observation 12 is both a high leverage point and a potential outlier.</p>
<p>We can also look at <em>studentized residuals</em>, which are defined as:</p>
<p><span class="math display">\[t_{i} = r_{i}\sqrt{\frac{n-p-1}{n-p-r^{2}_{i}}}\]</span></p>
<p>In <em>R</em>, we can use <code>rstudent()</code>:</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb588-1" data-line-number="1"><span class="kw">rstudent</span>(df.lm)</a></code></pre></div>
<pre><code>##             1             2             3             4             5 
##  8.427665e-01 -2.329041e-02  9.636946e-02 -8.751682e-05 -2.579393e-01 
##             6             7             8             9            10 
## -6.159215e-01 -2.632726e-01 -1.016216e+00 -4.688619e-01 -1.473142e+00 
##            11            12 
##  3.719616e+00  2.622850e+00</code></pre>
<p>It may be useful to view all of these measures together and apply some conditional formatting.</p>
<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb590-1" data-line-number="1"><span class="kw">library</span>(kableExtra)</a>
<a class="sourceLine" id="cb590-2" data-line-number="2"></a>
<a class="sourceLine" id="cb590-3" data-line-number="3">df <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb590-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb590-5" data-line-number="5">    <span class="dt">obs =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(df),</a>
<a class="sourceLine" id="cb590-6" data-line-number="6">    <span class="dt">r.standard =</span> <span class="kw">round</span>(<span class="kw">rstandard</span>(df.lm), <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb590-7" data-line-number="7">    <span class="dt">r.student =</span> <span class="kw">round</span>(<span class="kw">rstudent</span>(df.lm), <span class="dv">3</span>), </a>
<a class="sourceLine" id="cb590-8" data-line-number="8">    <span class="dt">i.hatv =</span> <span class="kw">round</span>(<span class="kw">hatvalues</span>(df.lm), <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb590-9" data-line-number="9">    <span class="dt">i.cook =</span> <span class="kw">round</span>(<span class="kw">cooks.distance</span>(df.lm), <span class="dv">3</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb590-10" data-line-number="10"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb590-11" data-line-number="11">    <span class="dt">r.standard =</span> <span class="kw">cell_spec</span>(r.standard, <span class="st">&quot;html&quot;</span>, <span class="dt">color=</span><span class="kw">ifelse</span>(<span class="kw">abs</span>(r.standard)<span class="op">&gt;</span><span class="dv">2</span>,<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>)),</a>
<a class="sourceLine" id="cb590-12" data-line-number="12">    <span class="dt">r.student =</span> <span class="kw">cell_spec</span>(r.student, <span class="st">&quot;html&quot;</span>, <span class="dt">color=</span><span class="kw">ifelse</span>(<span class="kw">abs</span>(r.student)<span class="op">&gt;</span><span class="dv">2</span>,<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>)),</a>
<a class="sourceLine" id="cb590-13" data-line-number="13">    <span class="dt">i.hatv =</span> <span class="kw">cell_spec</span>(i.hatv, <span class="st">&quot;html&quot;</span>, <span class="dt">color=</span><span class="kw">ifelse</span>(i.hatv<span class="op">&gt;</span><span class="dv">4</span><span class="op">/</span><span class="kw">nrow</span>(df),<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>)),</a>
<a class="sourceLine" id="cb590-14" data-line-number="14">    <span class="dt">i.cook =</span> <span class="kw">cell_spec</span>(i.cook, <span class="st">&quot;html&quot;</span>, <span class="dt">color=</span><span class="kw">ifelse</span>(i.cook<span class="op">&gt;</span><span class="dv">1</span>,<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb590-15" data-line-number="15"><span class="st">  </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;html&quot;</span>, <span class="dt">escape =</span> F) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb590-16" data-line-number="16"><span class="st">  </span><span class="kw">kable_styling</span>(<span class="st">&quot;striped&quot;</span>, <span class="dt">full_width =</span> F)</a></code></pre></div>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:right;">
obs
</th>
<th style="text-align:left;">
r.standard
</th>
<th style="text-align:left;">
r.student
</th>
<th style="text-align:left;">
i.hatv
</th>
<th style="text-align:left;">
i.cook
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
1.6854792
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.855</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.843</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.145</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.062</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
0.3333333
</td>
<td style="text-align:right;">
0.7509842
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.025</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.023</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.127</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
0.6666667
</td>
<td style="text-align:right;">
1.2482309
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.102</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.096</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.113</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.001</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
1.4164313
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.101</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
1.3333333
</td>
<td style="text-align:right;">
1.3354675
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.271</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.258</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.092</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.004</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
1.6666667
</td>
<td style="text-align:right;">
1.1136044
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.636</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.616</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.087</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.019</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
2.0000000
</td>
<td style="text-align:right;">
1.9557610
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.276</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.263</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.084</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.003</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
2.3333333
</td>
<td style="text-align:right;">
1.1860038
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-1.015</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-1.016</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.084</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.047</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
2.6666667
</td>
<td style="text-align:right;">
2.2758785
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.488</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-0.469</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.087</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.011</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
3.0000000
</td>
<td style="text-align:right;">
1.2686430
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-1.394</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">-1.473</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.092</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.099</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
1.0000000
</td>
<td style="text-align:right;">
5.0000000
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:left;">
<span style="     color: red !important;">2.461</span>
</td>
<td style="text-align:left;">
<span style="     color: red !important;">3.72</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.101</span>
</td>
<td style="text-align:left;">
<span style="     color: black !important;">0.341</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
10.0000000
</td>
<td style="text-align:right;">
11.0000000
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:left;">
<span style="     color: red !important;">2.081</span>
</td>
<td style="text-align:left;">
<span style="     color: red !important;">2.623</span>
</td>
<td style="text-align:left;">
<span style="     color: red !important;">0.887</span>
</td>
<td style="text-align:left;">
<span style="     color: red !important;">17.036</span>
</td>
</tr>
</tbody>
</table>
</div>
<div id="what-to-do-about-unusual-observations" class="section level4">
<h4><span class="header-section-number">7.3.5.3</span> What To Do About Unusual Observations</h4>
<p>In the book, Linear Models With R, <span class="citation">Faraway (<a href="#ref-faraway2014">2014</a>)</span> gives advice on this topic that Ill paraphrase.</p>
<ol style="list-style-type: decimal">
<li>Check for data entry errors and correct any that are found.</li>
<li>Consider the context. An unusual observation may be the single most important observation in the study.</li>
<li>Exclude the observation from the dataset and refit a model. If it makes little to no difference in your analysis, then its usually best to leave it in.</li>
<li>Do not automate the process of excluding outliers (see #2 above).</li>
<li>If you exclude an observation, document it in your report and explain your rationale so that your analytic integrety is not questioned.</li>
</ol>
</div>
</div>
<div id="linear-regression-assumptions-and-diagnostics-problem-set" class="section level3">
<h3><span class="header-section-number">7.3.6</span> Linear Regression Assumptions and Diagnostics Problem Set</h3>
<p>There is no problem set associated with this section. We check our assumptions and do diagnostic checks in all our analyses.</p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">7.4</span> Multiple Linear Regression</h2>
<p>In the previous section we considered just one predictor and one response. The linear model can be expanded to include multiple predictors by simply adding terms to the equation:</p>
<p><span class="math display">\[y = \beta_{0} + \beta_{1}x_{1}+ \beta_{2}x_{2} + ... + \beta_{(p-1)}x_{(p-1)} + \varepsilon\]</span></p>
<p>With one predictor, least squares regression produces a regression line. With two predictors, we get a regression plane (shown below), and so on up to a <span class="math inline">\((p-1)\)</span> dimensional hyperplane.</p>
<div id="htmlwidget-2a8e9539872343d5916c" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2a8e9539872343d5916c">{"x":{"visdat":{"e9c77bd48e2":["function () ","plotlyVisDat"],"e9c251029ac":["function () ","data"],"e9c6d33b6b0":["function () ","data"]},"cur_data":"e9c6d33b6b0","attrs":{"e9c251029ac":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"color":"black","size":7},"showlegend":false,"inherit":true},"e9c6d33b6b0":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"mesh3d","facecolor":["blue","blue"],"opacity":0.75,"showlegend":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"Best Fit Plane","showlegend":false,"scene":{"xaxis":{"range":[0,10],"title":"x1"},"yaxis":{"range":[0,10],"title":"x2"},"camera":{"eye":{"x":0,"y":-2,"z":0.3}},"zaxis":{"title":"y"}},"hovermode":"closest"},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[9.14806043496355,9.37075413297862,2.86139534786344,8.30447626067325,6.41745518893003,5.19095949130133,7.36588314641267,1.3466659723781,6.56992290401831,7.05064784036949],"y":[4.5774177624844,7.19112251652405,9.34672247152776,2.55428824340925,4.62292822543532,9.40014522755519,9.78226428385824,1.17487361654639,4.74997081561014,5.60332746244967],"z":[5.99114356409588,7.11473922517624,3.22154518985722,4.26414945838675,4.41869783432461,5.3655056340129,4.57315445009747,1.46962155854145,2.1255277574059,5.95281211087115],"type":"scatter3d","mode":"markers","marker":{"color":"black","size":7,"line":{"color":"rgba(31,119,180,1)"}},"showlegend":false,"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"y<br />y","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[0,10,0,10],"y":[0,0,10,10],"z":[0.167243763471062,5.07605548829849,2.13181054993435,7.04062227476178],"type":"mesh3d","facecolor":["blue","blue"],"opacity":0.75,"showlegend":false,"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>With two or more predictors, we cant solve for the linear model coefficients the same way we did for the one predictor case. Solving for the coefficients (the <span class="math inline">\(\beta\)</span>s) requires some linear algebra. Given a data set with <span class="math inline">\(n\)</span> observations and one predictor, the <span class="math inline">\(y\)</span> (response), <span class="math inline">\(X\)</span> (predictor), and <span class="math inline">\(\beta\)</span> (coefficient) matrices are written as:</p>
<p><span class="math display">\[y= \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} , X= \begin{pmatrix} 1 &amp; x_1 \\ \vdots &amp; \vdots \\ 1 &amp; x_n \end{pmatrix}, \beta= \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}\]</span></p>
<p>Incorporating the error term, we have:</p>
<p><span class="math display">\[\varepsilon= \begin{pmatrix} \varepsilon_1 \\ \vdots \\ \varepsilon_n \end{pmatrix} = \begin{pmatrix} y_1-\beta_0-\beta_1x_1 \\ \vdots \\ y_n-\beta_0-\beta_1x_n \end{pmatrix} = y-X\beta\]</span></p>
<p>Once we solve for the coefficients, multiplying them by the predictors gives the estimated response, <span class="math inline">\(\hat{y}\)</span>.</p>
<p><span class="math display">\[X\beta \equiv \hat{y}\]</span></p>
<p>With multiple linear regression, we expand the <span class="math inline">\(X\)</span> and <span class="math inline">\(\beta\)</span> matrices accordingly.</p>
<p><span class="math display">\[y= \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} , X= \begin{pmatrix} 1 &amp; x_{11} &amp; \ldots &amp; x_{1p-1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \ldots &amp; x_{np-1} \end{pmatrix}, \beta= \begin{pmatrix} \beta_0 \\ \vdots \\ \beta_{p-1} \end{pmatrix}\]</span></p>
<p>Incorporating error:</p>
<p><span class="math display">\[\varepsilon= \begin{pmatrix} \varepsilon_1 \\ \vdots \\ \varepsilon_n \end{pmatrix} = \begin{pmatrix} y_1-(\beta_0-\beta_1x_{11} + \ldots + \beta_{p-1}x_{1p-1}) \\ \vdots \\ y_n-(\beta_0-\beta_1x_{n1} + \ldots + \beta_{p-1}x_{np-1}) \end{pmatrix} = y-X\beta\]</span></p>
<p>However, notice that the final equation remains unchanged.</p>
<p><span class="math display">\[X\beta \equiv \hat{y}\]</span></p>
<p>The residual sum of squares (RSS) also remains unchanged, and so do the other equations that have RSS as a term, such as residual standard error and <span class="math inline">\(R^2\)</span>. The following is an example of solving the system of equations for a case with two predictors and no error. Given <span class="math inline">\(n=4\)</span> observations, we have the following system of equations:</p>
<p><span class="math display">\[x_0 = 10\]</span>
<span class="math display">\[x_0 + x_2 = 17\]</span>
<span class="math display">\[x_0 + x_1 = 15\]</span>
<span class="math display">\[x_0 + x_1 + x_2 = 22\]</span></p>
<p>In this example, we technically have all of the information we need to solve this system of equations without linear algebra, but well apply it anyway to demonstrate the method. Rewriting the above system of equations into matrix form gives:</p>
<p><span class="math display">\[X= \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{pmatrix}, y= \begin{pmatrix} 10 \\ 17 \\ 15 \\ 22 \end{pmatrix}\]</span></p>
<p>One way to solve for the <span class="math inline">\(\beta\)</span> vector is to transpose the <span class="math inline">\(X\)</span> matrix and multiply it by the <span class="math inline">\(X|y\)</span> augmented matrix.</p>
<p><span class="math display">\[X^TX|y = \begin{pmatrix} 1&amp;1&amp;1&amp;1 \\ 0&amp;0&amp;1&amp;1 \\ 0&amp;1&amp;0&amp;1 \end{pmatrix} \begin{pmatrix} 1&amp;0&amp;0&amp;|&amp;10 \\ 1&amp;0&amp;1&amp;|&amp;17 \\ 1&amp;1&amp;0&amp;|&amp;15 \\ 1&amp;1&amp;1&amp;|&amp;22 \end{pmatrix} = \begin{pmatrix} 4&amp;2&amp;2&amp;|&amp;64 \\ 2&amp;2&amp;1&amp;|&amp;37 \\ 2&amp;1&amp;2&amp;|&amp;39 \end{pmatrix}\]</span></p>
<p>Use Gaussian elimination to reduce the resulting matrix by first multiplying the top row by <span class="math inline">\(-\frac{1}{2}\)</span> and adding those values to the second row.</p>
<p><span class="math display">\[\begin{pmatrix} 4&amp;2&amp;2&amp;|&amp;64 \\ 0&amp;1&amp;0&amp;|&amp;5 \\ 2&amp;1&amp;2&amp;|&amp;39 \end{pmatrix}\]</span></p>
<p>Reduce further using the same process on the third row.</p>
<p><span class="math display">\[\begin{pmatrix} 4&amp;2&amp;2&amp;|&amp;64 \\ 0&amp;1&amp;0&amp;|&amp;5 \\ 0&amp;0&amp;1&amp;|&amp;7 \end{pmatrix}\]</span></p>
<p>We find that</p>
<p><span class="math display">\[\beta_2 = 7\]</span>
and</p>
<p><span class="math display">\[\beta_1 = 5\]</span></p>
<p>and using back substitution we get</p>
<p><span class="math display">\[4\beta_0 + 2(5) + 2(7) = 64, \enspace so \enspace \beta_0 = 10\]</span></p>
<p>The resulting equation:</p>
<p><span class="math display">\[y=10+5x_1+7x_2\]</span></p>
<p>defines the best fit plane for this data, which is visualized below.</p>
<div id="htmlwidget-5bc403e1f7b162d7d6b0" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-5bc403e1f7b162d7d6b0">{"x":{"visdat":{"e9c57c9ebd0":["function () ","plotlyVisDat"],"e9c248ee3ad":["function () ","data"],"e9c7df6bdda":["function () ","data"]},"cur_data":"e9c7df6bdda","attrs":{"e9c248ee3ad":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"color":"black","size":7},"showlegend":false,"inherit":true},"e9c7df6bdda":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"mesh3d","facecolor":["blue","blue"],"showlegend":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"Best Fit Plane","showlegend":false,"scene":{"camera":{"eye":{"x":-1.5,"y":-1.5,"z":0.3}},"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest"},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0,0,1,1],"y":[0,1,0,1],"z":[10,17,15,22],"type":"scatter3d","mode":"markers","marker":{"color":"black","size":7,"line":{"color":"rgba(31,119,180,1)"}},"showlegend":false,"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"y<br />y","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[0,0,1,1],"y":[0,1,0,1],"z":[10,17,15,22],"type":"mesh3d","facecolor":["blue","blue"],"showlegend":false,"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Of course, <em>R</em> has linear algebra functions, so we dont have to do all of that by hand. For example, we can solve for the <span class="math inline">\(\beta\)</span> vector by multiplying both sides of the equation <span class="math inline">\(X\beta \equiv \hat{y}\)</span> by <span class="math inline">\(X^T\)</span>.</p>
<p><span class="math display">\[X^TX\beta = X^Ty\]</span></p>
<p>Solving for <span class="math inline">\(\beta\)</span>, we get:</p>
<p><span class="math display">\[\beta=(X^TX)^{-1}X^Ty\]</span></p>
<p>Now use <code>solve()</code> function to calculate the <span class="math inline">\(\beta\)</span> vector (note that <code>solve()</code> inverts <span class="math inline">\(X^TX\)</span> automatically).</p>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb591-1" data-line-number="1">X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,</a>
<a class="sourceLine" id="cb591-2" data-line-number="2">             <span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,</a>
<a class="sourceLine" id="cb591-3" data-line-number="3">             <span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,</a>
<a class="sourceLine" id="cb591-4" data-line-number="4">             <span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">byrow =</span> <span class="ot">TRUE</span>, <span class="dt">ncol=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb591-5" data-line-number="5"></a>
<a class="sourceLine" id="cb591-6" data-line-number="6">y =<span class="st"> </span><span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span><span class="op">*</span>X[, <span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">7</span><span class="op">*</span>X[, <span class="dv">3</span>]</a>
<a class="sourceLine" id="cb591-7" data-line-number="7"></a>
<a class="sourceLine" id="cb591-8" data-line-number="8"><span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y)</a></code></pre></div>
<pre><code>##      [,1]
## [1,]   10
## [2,]    5
## [3,]    7</code></pre>
<p>Fitting a linear model in <em>R</em> using the <code>lm()</code> function produces coefficients identical to the above results.</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb593-1" data-line-number="1"><span class="kw">coef</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>X[, <span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>X[, <span class="dv">3</span>]))</a></code></pre></div>
<pre><code>## (Intercept)      X[, 2]      X[, 3] 
##          10           5           7</code></pre>
<p>Technically, neither the <code>solve()</code> nor the <code>lm()</code> functions use Gaussian elimination when solving the system of equations. According to <a href="https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-c/top/lapack-routines/lapack-least-squares-and-eigenvalue-problem-routines.html">this site</a>, for overdetermined systems (where there are more equations than unknowns) like the example were working with, they use QR factorization instead. The details of QR factorization are beyond the scope of this course, but are explained well on <a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/ls.pdf">these slides</a> for a course at UCLAs School of Engineering and Applied Sciences. In essence, the <span class="math inline">\(X\)</span> matrix is decomposed into <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> matrices that are substituted for <span class="math inline">\(X\)</span> in the equation.</p>
<p><span class="math display">\[X^TX\beta = X^Ty\]</span></p>
<p><span class="math display">\[(QR)^T(QR)\beta = (QR)^Ty\]</span>
Skipping a lot of math, we end up with:</p>
<p><span class="math display">\[R\beta=Q^Ty\]</span></p>
<p>In <em>R</em>, use <code>qr(X)</code> to decompose <span class="math inline">\(X\)</span>, and then use <code>solve.qr()</code> to calculate the <span class="math inline">\(\beta\)</span> vector.</p>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb595-1" data-line-number="1">QR =<span class="st"> </span><span class="kw">qr</span>(X)</a>
<a class="sourceLine" id="cb595-2" data-line-number="2"><span class="kw">solve.qr</span>(QR, y)</a></code></pre></div>
<pre><code>## [1] 10  5  7</code></pre>
<p>Now well make it a little more complicated by returning to the data set plotted at the beginning of this section. It consists of <span class="math inline">\(n=10\)</span> observations with random error.</p>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb597-1" data-line-number="1">mlr <span class="co"># multiple linear regression data set</span></a></code></pre></div>
<pre><code>## # A tibble: 10 x 3
##       x1    x2     y
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  9.15  4.58  5.99
##  2  9.37  7.19  7.11
##  3  2.86  9.35  3.22
##  4  8.30  2.55  4.26
##  5  6.42  4.62  4.42
##  6  5.19  9.40  5.37
##  7  7.37  9.78  4.57
##  8  1.35  1.17  1.47
##  9  6.57  4.75  2.13
## 10  7.05  5.60  5.95</code></pre>
<p>Using QR decomposition, we get the following coefficients:</p>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb599-1" data-line-number="1"><span class="co"># we need to add a column of 1&#39;s to get beta_0 for the intercept</span></a>
<a class="sourceLine" id="cb599-2" data-line-number="2">intercept =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb599-3" data-line-number="3">QR =<span class="st"> </span><span class="kw">qr</span>(<span class="kw">cbind</span>(intercept, mlr[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>])) </a>
<a class="sourceLine" id="cb599-4" data-line-number="4">betas =<span class="st"> </span><span class="kw">solve.qr</span>(QR, mlr<span class="op">$</span>y)</a>
<a class="sourceLine" id="cb599-5" data-line-number="5">betas</a></code></pre></div>
<pre><code>## intercept        x1        x2 
## 0.1672438 0.4908812 0.1964567</code></pre>
<p>And we get the following coefficients in the linear model:</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb601-1" data-line-number="1"><span class="kw">coef</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mlr))</a></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   0.1672438   0.4908812   0.1964567</code></pre>
<p>The following code chunk shows how the earlier interactive plot was generated. Note the following:</p>
<ul>
<li><p>The value of <code>y</code> defined by the plane at (<code>x1</code>, <code>x2</code>) = (0,0) is <span class="math inline">\(\beta_0\)</span> (shown by the red dot).</p></li>
<li><p>The slope of the line at the intersection of the plane with the <code>x1</code> axis is <span class="math inline">\(\beta_1\)</span>.</p></li>
<li><p>The slope of the line at the intersection of the plane with the <code>x2</code> axis is <span class="math inline">\(\beta_2\)</span>,</p></li>
</ul>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb603-1" data-line-number="1"><span class="co"># define the bast fit plane using the betas from QR decomposition</span></a>
<a class="sourceLine" id="cb603-2" data-line-number="2">plane =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x1=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb603-3" data-line-number="3">               <span class="dt">x2=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb603-4" data-line-number="4">               <span class="dt">y=</span><span class="kw">c</span>(betas[<span class="dv">1</span>], betas[<span class="dv">1</span>]<span class="op">+</span><span class="dv">10</span><span class="op">*</span>betas[<span class="dv">2</span>], betas[<span class="dv">1</span>]<span class="op">+</span><span class="dv">10</span><span class="op">*</span>betas[<span class="dv">3</span>], betas[<span class="dv">1</span>]<span class="op">+</span><span class="kw">sum</span>(<span class="dv">10</span><span class="op">*</span>betas[<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>])))</a>
<a class="sourceLine" id="cb603-5" data-line-number="5"></a>
<a class="sourceLine" id="cb603-6" data-line-number="6"><span class="co"># use plotly for interactive 3D graphs</span></a>
<a class="sourceLine" id="cb603-7" data-line-number="7"><span class="kw">plot_ly</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb603-8" data-line-number="8"><span class="st">  </span><span class="co"># add the points to the graph</span></a>
<a class="sourceLine" id="cb603-9" data-line-number="9"><span class="st">  </span><span class="kw">add_trace</span>(<span class="dt">data =</span> mlr, <span class="dt">x=</span><span class="op">~</span>x1, <span class="dt">y =</span> <span class="op">~</span>x2, <span class="dt">z =</span> <span class="op">~</span>y, <span class="dt">type=</span><span class="st">&#39;scatter3d&#39;</span>, <span class="dt">mode=</span><span class="st">&#39;markers&#39;</span>, </a>
<a class="sourceLine" id="cb603-10" data-line-number="10">            <span class="dt">marker=</span><span class="kw">list</span>(<span class="dt">color=</span><span class="st">&#39;black&#39;</span>, <span class="dt">size=</span><span class="dv">7</span>), <span class="dt">showlegend=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb603-11" data-line-number="11"><span class="st">  </span><span class="co"># add the plane</span></a>
<a class="sourceLine" id="cb603-12" data-line-number="12"><span class="st">  </span><span class="kw">add_trace</span>(<span class="dt">data =</span> plane, <span class="dt">x=</span><span class="op">~</span>x1, <span class="dt">y =</span> <span class="op">~</span>x2, <span class="dt">z =</span> <span class="op">~</span>y, <span class="dt">type=</span><span class="st">&#39;mesh3d&#39;</span>, </a>
<a class="sourceLine" id="cb603-13" data-line-number="13">            <span class="dt">facecolor=</span><span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;blue&#39;</span>), <span class="dt">opacity =</span> <span class="fl">0.75</span>, <span class="dt">showlegend=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb603-14" data-line-number="14"><span class="st">  </span><span class="co"># add the red dot</span></a>
<a class="sourceLine" id="cb603-15" data-line-number="15"><span class="st">  </span><span class="kw">add_trace</span>(<span class="dt">x=</span><span class="dv">0</span>, <span class="dt">y=</span><span class="dv">0</span>, <span class="dt">z=</span>betas[<span class="dv">1</span>], <span class="dt">type=</span><span class="st">&#39;scatter3d&#39;</span>, <span class="dt">mode=</span><span class="st">&#39;markers&#39;</span>,</a>
<a class="sourceLine" id="cb603-16" data-line-number="16">            <span class="dt">marker=</span><span class="kw">list</span>(<span class="dt">color=</span><span class="st">&#39;red&#39;</span>, <span class="dt">size=</span><span class="dv">7</span>), <span class="dt">showlegend=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb603-17" data-line-number="17"><span class="st">  </span><span class="co"># adjust the layout</span></a>
<a class="sourceLine" id="cb603-18" data-line-number="18"><span class="st">  </span><span class="kw">layout</span>(<span class="dt">title =</span> <span class="st">&#39;Best Fit Plane&#39;</span>, <span class="dt">showlegend =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb603-19" data-line-number="19">         <span class="dt">scene =</span> <span class="kw">list</span>(<span class="dt">xaxis =</span> <span class="kw">list</span>(<span class="dt">range=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)),</a>
<a class="sourceLine" id="cb603-20" data-line-number="20">                      <span class="dt">yaxis =</span> <span class="kw">list</span>(<span class="dt">range=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)),</a>
<a class="sourceLine" id="cb603-21" data-line-number="21">                      <span class="dt">camera =</span> <span class="kw">list</span>(<span class="dt">eye =</span> <span class="kw">list</span>(<span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">y =</span> <span class="dv">-2</span>, <span class="dt">z =</span> <span class="fl">0.3</span>))))</a></code></pre></div>
<div id="htmlwidget-f4997105858173027594" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-f4997105858173027594">{"x":{"visdat":{"e9c52ff8cd":["function () ","plotlyVisDat"],"e9c5617dad0":["function () ","data"],"e9c6be9b850":["function () ","data"]},"cur_data":"e9c6be9b850","attrs":{"e9c5617dad0":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"color":"black","size":7},"showlegend":false,"inherit":true},"e9c6be9b850":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"mesh3d","facecolor":["blue","blue"],"opacity":0.75,"showlegend":false,"inherit":true},"e9c6be9b850.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":0,"y":0,"z":0.167243763471062,"type":"scatter3d","mode":"markers","marker":{"color":"red","size":7},"showlegend":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"Best Fit Plane","showlegend":false,"scene":{"xaxis":{"range":[0,10],"title":"x1"},"yaxis":{"range":[0,10],"title":"x2"},"camera":{"eye":{"x":0,"y":-2,"z":0.3}},"zaxis":{"title":"y"}},"hovermode":"closest"},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[9.14806043496355,9.37075413297862,2.86139534786344,8.30447626067325,6.41745518893003,5.19095949130133,7.36588314641267,1.3466659723781,6.56992290401831,7.05064784036949],"y":[4.5774177624844,7.19112251652405,9.34672247152776,2.55428824340925,4.62292822543532,9.40014522755519,9.78226428385824,1.17487361654639,4.74997081561014,5.60332746244967],"z":[5.99114356409588,7.11473922517624,3.22154518985722,4.26414945838675,4.41869783432461,5.3655056340129,4.57315445009747,1.46962155854145,2.1255277574059,5.95281211087115],"type":"scatter3d","mode":"markers","marker":{"color":"black","size":7,"line":{"color":"rgba(31,119,180,1)"}},"showlegend":false,"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"y<br />y","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[0,10,0,10],"y":[0,0,10,10],"z":[0.167243763471062,5.07605548829849,2.13181054993435,7.04062227476178],"type":"mesh3d","facecolor":["blue","blue"],"opacity":0.75,"showlegend":false,"frame":null},{"x":[0],"y":[0],"z":[0.167243763471062],"type":"scatter3d","mode":"markers","marker":{"color":"red","size":7,"line":{"color":"rgba(44,160,44,1)"}},"showlegend":false,"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div id="r-example" class="section level3">
<h3><span class="header-section-number">7.4.1</span> <em>R</em> Example</h3>
<p>Below is a short example on doing multiple linear regression in <em>R</em>. This example uses a <a href = '/_Chapter6_ProblemSets/PatientSatData.csv'> data set </a> on patient satisfaction as a function of their age, illness severity, anxiety level, and a surgery variable (this is a binary variable, we will ignore for this exercise). We will attempt to model patient satisfaction as a function of age, illness severity, and anxiety level.</p>
<p>First, read the data and build the linear model.</p>
<div class="sourceCode" id="cb604"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb604-1" data-line-number="1"><span class="co"># Read the data</span></a>
<a class="sourceLine" id="cb604-2" data-line-number="2">pt &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;../html/_Chapter6_ProblemSets/PatientSatData.csv&#39;</span>, <span class="dt">sep =</span> <span class="st">&#39;,&#39;</span>, <span class="dt">header =</span> T)</a>
<a class="sourceLine" id="cb604-3" data-line-number="3"><span class="co"># let&#39;s drop SurgMed as we&#39;re not going to use it</span></a>
<a class="sourceLine" id="cb604-4" data-line-number="4">pt &lt;-<span class="st"> </span>pt <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>SurgMed)</a>
<a class="sourceLine" id="cb604-5" data-line-number="5"><span class="co"># View the data</span></a>
<a class="sourceLine" id="cb604-6" data-line-number="6">pt</a></code></pre></div>
<pre><code>##    Age illSeverity Anxiety Satisfaction
## 1   55          50     2.1           68
## 2   46          24     2.8           77
## 3   30          46     3.3           96
## 4   35          48     4.5           80
## 5   59          58     2.0           43
## 6   61          60     5.1           44
## 7   74          65     5.5           26
## 8   38          42     3.2           88
## 9   27          42     3.1           75
## 10  51          50     2.4           57
## 11  53          38     2.2           56
## 12  41          30     2.1           88
## 13  37          31     1.9           88
## 14  24          34     3.1          102
## 15  42          30     3.0           88
## 16  50          48     4.2           70
## 17  58          61     4.6           52
## 18  60          71     5.3           43
## 19  62          62     7.2           46
## 20  68          38     7.8           56
## 21  70          41     7.0           59
## 22  79          66     6.2           26
## 23  63          31     4.1           52
## 24  39          42     3.5           83
## 25  49          40     2.1           75</code></pre>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb606-1" data-line-number="1"><span class="co"># Note that our data is formatted in numeric format, which is what we need for this sort of modeling.</span></a>
<a class="sourceLine" id="cb606-2" data-line-number="2"></a>
<a class="sourceLine" id="cb606-3" data-line-number="3"><span class="co"># we can look at our data.  In multiple regression, `pairs` is useful:</span></a>
<a class="sourceLine" id="cb606-4" data-line-number="4"><span class="kw">pairs</span>(pt)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb607-1" data-line-number="1"><span class="co"># We can see some useful things:</span></a>
<a class="sourceLine" id="cb607-2" data-line-number="2"><span class="co"># 1) Age and satisfaction appear to have a linear relationship (the bottom left corner)</span></a>
<a class="sourceLine" id="cb607-3" data-line-number="3"><span class="co"># 2) illness severity and satisfaction appear to have a linear relationship, thoguh not as strongly</span></a>
<a class="sourceLine" id="cb607-4" data-line-number="4"><span class="co"># 3) it&#39;s less clear for anxiety and satisfaction</span></a>
<a class="sourceLine" id="cb607-5" data-line-number="5"><span class="co"># 4) Age and illness severity do not appear to have a relationship</span></a>
<a class="sourceLine" id="cb607-6" data-line-number="6"><span class="co"># 5) Age and anxiety might have a relationship, but its not fully clear</span></a>
<a class="sourceLine" id="cb607-7" data-line-number="7"><span class="co"># 6) illness severity and anxiety do not appear to have a relationship</span></a>
<a class="sourceLine" id="cb607-8" data-line-number="8"></a>
<a class="sourceLine" id="cb607-9" data-line-number="9"><span class="co"># Model the data</span></a>
<a class="sourceLine" id="cb607-10" data-line-number="10"><span class="co"># Note how this format is analogous to ANOVA with multiple factors</span></a>
<a class="sourceLine" id="cb607-11" data-line-number="11"><span class="co"># and simple linear regression</span></a>
<a class="sourceLine" id="cb607-12" data-line-number="12">ptLM &lt;-<span class="st"> </span><span class="kw">lm</span>(Satisfaction <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>illSeverity <span class="op">+</span><span class="st"> </span>Anxiety, <span class="dt">data =</span> pt)</a></code></pre></div>
<p>We can now view our model results. We will use <span class="math inline">\(\alpha = .05\)</span> as our appropriate significance level.</p>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb608-1" data-line-number="1"><span class="co"># we view the summary results</span></a>
<a class="sourceLine" id="cb608-2" data-line-number="2"><span class="kw">summary</span>(ptLM)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Satisfaction ~ Age + illSeverity + Anxiety, data = pt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -18.2812  -3.8635   0.6427   4.5324  11.8734 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 143.8952     5.8975  24.399  &lt; 2e-16 ***
## Age          -1.1135     0.1326  -8.398 3.75e-08 ***
## illSeverity  -0.5849     0.1320  -4.430 0.000232 ***
## Anxiety       1.2962     1.0560   1.227 0.233231    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.037 on 21 degrees of freedom
## Multiple R-squared:  0.9035, Adjusted R-squared:  0.8897 
## F-statistic: 65.55 on 3 and 21 DF,  p-value: 7.85e-11</code></pre>
<p>We see several things. First, we can say that the intercept, age, and illness severity are all statistically significant. Anxiety does not appear to be significant. As expected given our individual results, our F-statistic (sometimes called a model utility test) shows us that there is at least one predictor that is significant. Further we can see that our <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R_{adj}^2\)</span> are both relatively high, which shows that these predictors explain much of the variability in the data. We can see our RSE is about 7, which is not too extreme given our the range on our outputs.</p>
<p>As we do not find anxiety significant, we can drop it as an independent variable (we discuss model selection in the next chapter). Our new model is then:</p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb610-1" data-line-number="1">ptLM2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Satisfaction <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>illSeverity, <span class="dt">data =</span> pt)</a>
<a class="sourceLine" id="cb610-2" data-line-number="2"><span class="kw">summary</span>(ptLM2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Satisfaction ~ Age + illSeverity, data = pt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.2800  -5.0316   0.9276   4.2911  10.4993 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 143.4720     5.9548  24.093  &lt; 2e-16 ***
## Age          -1.0311     0.1156  -8.918 9.28e-09 ***
## illSeverity  -0.5560     0.1314  -4.231 0.000343 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.118 on 22 degrees of freedom
## Multiple R-squared:  0.8966, Adjusted R-squared:  0.8872 
## F-statistic: 95.38 on 2 and 22 DF,  p-value: 1.446e-11</code></pre>
<p>We get similar results. We can then build our linear model:</p>
<p><span class="math display">\[[Patient Satisfaction] = 143 + -1.03[Age] + -0.556[Illness Severity] + \epsilon\]</span></p>
<p>We can interpret this as saying that for every additional year of age, a patients satisfaction drops about a point and for every additional point of illness severity, a patient loses about half a point of satisfaction. That is, the older and sicker you are, the less likely you are to be satisfied. This generally seems to make sense.</p>
<p>Moreover, we can use the model to show our confidence intervals on our coefficients using <code>confint</code>.</p>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb612-1" data-line-number="1"><span class="co"># Note that confint requires:</span></a>
<a class="sourceLine" id="cb612-2" data-line-number="2"><span class="co"># A model, called object in this case</span></a>
<a class="sourceLine" id="cb612-3" data-line-number="3"><span class="co"># You can also pass it your 1-alpha level (the default is alpha = .05, or .95 confidence)</span></a>
<a class="sourceLine" id="cb612-4" data-line-number="4"><span class="co"># You can also pass it specific parameters to check (useful if working with amodel with many parameters)</span></a>
<a class="sourceLine" id="cb612-5" data-line-number="5"><span class="kw">confint</span>(ptLM2, <span class="dt">level =</span> <span class="fl">.95</span>)</a></code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 131.122434 155.8215898
## Age          -1.270816  -0.7912905
## illSeverity  -0.828566  -0.2835096</code></pre>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb614-1" data-line-number="1"><span class="co"># We can then say, with 95% confidence that our intercept is in the interval ~ (131, 156)</span></a></code></pre></div>
<p>We can use our model to predict a patients satisfaction given their age and illness severity using <code>predict</code> in the same manner as simple linear regression.</p>
<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb615-1" data-line-number="1"><span class="co"># Point estimate</span></a>
<a class="sourceLine" id="cb615-2" data-line-number="2"><span class="kw">predict</span>(ptLM2, <span class="kw">list</span>(<span class="dt">Age =</span> <span class="dv">35</span>, <span class="dt">illSeverity=</span><span class="dv">50</span>))</a></code></pre></div>
<pre><code>##        1 
## 79.58325</code></pre>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb617-1" data-line-number="1"><span class="co"># An individual response will be in this interval</span></a>
<a class="sourceLine" id="cb617-2" data-line-number="2"><span class="kw">predict</span>(ptLM2,<span class="kw">list</span>(<span class="dt">Age=</span><span class="dv">35</span>, <span class="dt">illSeverity=</span><span class="dv">50</span>),<span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</a></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 79.58325 63.87546 95.29105</code></pre>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb619-1" data-line-number="1"><span class="co"># The mean response for someone with these inputs will be</span></a>
<a class="sourceLine" id="cb619-2" data-line-number="2"><span class="kw">predict</span>(ptLM2,<span class="kw">list</span>(<span class="dt">Age=</span><span class="dv">35</span>, <span class="dt">illSeverity=</span><span class="dv">50</span>),<span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>)</a></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 79.58325 74.21262 84.95388</code></pre>
<p>We can also plot these values. Note that our result with two predictors is a plane in this case. With 3+ predictors, it is a hyperplane. Often, we will plot either a contour map where each line corresponds to a fixed level of a predictor or just choose a single predictor.</p>
<p>We can produce a contour plot using <code>geom_contour</code> (one can also use <code>stat_contour</code>). Of course, this works for two predictors. As the number of independent variables increases, visualizing the data becomes somewhat more challenging and requires visualizing the solution only a few dimensions at a time.</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb621-1" data-line-number="1"><span class="co"># produce a contour plot</span></a>
<a class="sourceLine" id="cb621-2" data-line-number="2"></a>
<a class="sourceLine" id="cb621-3" data-line-number="3"><span class="co"># Requires a set of points with predictions:</span></a>
<a class="sourceLine" id="cb621-4" data-line-number="4">mySurface &lt;-<span class="st"> </span><span class="kw">expand_grid</span>( <span class="co"># produce a data frame that is every combination of the following vectors</span></a>
<a class="sourceLine" id="cb621-5" data-line-number="5">  <span class="dt">Age =</span> <span class="kw">seq</span>(<span class="kw">min</span>(pt<span class="op">$</span>Age), <span class="kw">max</span>(pt<span class="op">$</span>Age), <span class="dt">by =</span> <span class="dv">1</span>), <span class="co"># a sequence of ages from the min observation to the max by 1s</span></a>
<a class="sourceLine" id="cb621-6" data-line-number="6">  <span class="dt">illSeverity =</span> <span class="kw">seq</span>(<span class="kw">min</span>(pt<span class="op">$</span>illSeverity), <span class="kw">max</span>(pt<span class="op">$</span>illSeverity), <span class="dt">by =</span> <span class="dv">1</span>)) <span class="co"># a sequence of illness severity min to max observations</span></a>
<a class="sourceLine" id="cb621-7" data-line-number="7"></a>
<a class="sourceLine" id="cb621-8" data-line-number="8"><span class="co"># look at our data</span></a>
<a class="sourceLine" id="cb621-9" data-line-number="9"><span class="kw">head</span>(mySurface)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 2
##     Age illSeverity
##   &lt;dbl&gt;       &lt;dbl&gt;
## 1    24          24
## 2    24          25
## 3    24          26
## 4    24          27
## 5    24          28
## 6    24          29</code></pre>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb623-1" data-line-number="1"><span class="co"># add in our predictions. Recall predict can take a data frame with </span></a>
<a class="sourceLine" id="cb623-2" data-line-number="2"><span class="co"># columns that have the same name as the variables in the model</span></a>
<a class="sourceLine" id="cb623-3" data-line-number="3">mySurface<span class="op">$</span>Satisfaction &lt;-<span class="st"> </span><span class="kw">predict</span>(ptLM2, mySurface) </a>
<a class="sourceLine" id="cb623-4" data-line-number="4"><span class="kw">head</span>(mySurface)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##     Age illSeverity Satisfaction
##   &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;
## 1    24          24         105.
## 2    24          25         105.
## 3    24          26         104.
## 4    24          27         104.
## 5    24          28         103.
## 6    24          29         103.</code></pre>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb625-1" data-line-number="1"><span class="co"># Plot the contours for our response</span></a>
<a class="sourceLine" id="cb625-2" data-line-number="2"><span class="kw">ggplot</span>(<span class="dt">data =</span> mySurface, <span class="co"># requires a data frame with an x and y (your predictors), and z (your response)</span></a>
<a class="sourceLine" id="cb625-3" data-line-number="3">               <span class="kw">aes</span>(<span class="dt">x =</span> Age, <span class="dt">y =</span> illSeverity, <span class="dt">z =</span> Satisfaction)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb625-4" data-line-number="4"><span class="st">  </span><span class="co"># you can use a number of ways to do this.  geom_contour works</span></a>
<a class="sourceLine" id="cb625-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_contour</span>(<span class="kw">aes</span>(<span class="dt">color =</span> <span class="kw">after_stat</span>(level))) <span class="op">+</span><span class="st"> </span><span class="co"># This color argument varies the color of your contours by their level</span></a>
<a class="sourceLine" id="cb625-6" data-line-number="6"><span class="st">  </span><span class="kw">scale_color_distiller</span>(<span class="dt">palette =</span> <span class="st">&#39;Spectral&#39;</span>, <span class="dt">direction =</span> <span class="dv">-1</span>) <span class="op">+</span><span class="st"> </span><span class="co"># change the color from indecipherable blue</span></a>
<a class="sourceLine" id="cb625-7" data-line-number="7"><span class="st">  </span><span class="co"># clean up the plot</span></a>
<a class="sourceLine" id="cb625-8" data-line-number="8"><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&#39;Age&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&#39;Illness Severity&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&#39;Patient Satisfaction Response Surface&#39;</span>) </a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>With this plot, we can clearly see at least two things:</p>
<ul>
<li>Our mathematical interpretation holds true. The younger and less severely ill the patient, the more satisfied they are (in general, as based on our model).</li>
<li>Our model is a plane. We see this with the evenly spaced, linear contour lines.</li>
</ul>
<p>It is also useful to overlay the actual observations on the plot. We can do this as follows:</p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb626-1" data-line-number="1"><span class="co"># This is our original contour plot as produced above, with one exception.  </span></a>
<a class="sourceLine" id="cb626-2" data-line-number="2"><span class="co"># We move the data for the contour to the geom_contour so we can also plot the observations</span></a>
<a class="sourceLine" id="cb626-3" data-line-number="3"><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb626-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data =</span> mySurface, <span class="kw">aes</span>(<span class="dt">x =</span> Age, <span class="dt">y =</span> illSeverity, <span class="dt">z =</span> Satisfaction, <span class="dt">color =</span> <span class="kw">after_stat</span>(level))) <span class="op">+</span><span class="st"> </span><span class="co"># This color argument varies the color of your contours by their level</span></a>
<a class="sourceLine" id="cb626-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_color_distiller</span>(<span class="dt">palette =</span> <span class="st">&#39;Spectral&#39;</span>, <span class="dt">direction =</span> <span class="dv">-1</span>) <span class="op">+</span><span class="st"> </span><span class="co"># change the color from indecipherable blue</span></a>
<a class="sourceLine" id="cb626-6" data-line-number="6"><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&#39;Age&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&#39;Illness Severity&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&#39;Patient Satisfaction Response Surface&#39;</span>)  <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb626-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> pt, <span class="kw">aes</span>(<span class="dt">x =</span> Age, <span class="dt">y =</span> illSeverity, <span class="dt">color =</span> Satisfaction))</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p>By plotting these points, we can compare our results (the contour lines) to the observations. The first thing this allows us to do is look for outliers. For example, there are two around the age 30 and a severity of illness; note how their colors are disjoint from what the contour colors predict. This, of course, is harder to interpret than a simple linear regression as it involves comparing colors. In general, it is easier to use the numbers for higher dimensional models. Second, we can get an idea of leverage or areas of our model that are not informed by data. For example, there are no observations in this region:</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>That means that any predictions in this region are ill-informed and extrapolations beyond the data. In an advanced design section, we will discuss how ensuring we get coverage or space-filling is an important property for good experimental designs so we can avoid this problem.</p>
<p>Finally, we can check our model to ensure that it is legitimate.</p>
<p>Check assumptions:</p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb627-1" data-line-number="1"><span class="co"># Plot our standard diagnostic plots</span></a>
<a class="sourceLine" id="cb627-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb627-3" data-line-number="3"><span class="kw">plot</span>(ptLM2)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb628-1" data-line-number="1"><span class="co"># It appears that we meet our linearity, independence, normality and homoscedasticity assumptions.</span></a>
<a class="sourceLine" id="cb628-2" data-line-number="2"><span class="co"># There are no significant patterns, though we may have a few unusual observations</span></a>
<a class="sourceLine" id="cb628-3" data-line-number="3"></a>
<a class="sourceLine" id="cb628-4" data-line-number="4"><span class="co"># Check normality</span></a>
<a class="sourceLine" id="cb628-5" data-line-number="5"><span class="kw">shapiro.test</span>(ptLM2<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  ptLM2$residuals
## W = 0.95367, p-value = 0.3028</code></pre>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb630-1" data-line-number="1"><span class="co"># Check homosceasticity</span></a>
<a class="sourceLine" id="cb630-2" data-line-number="2">car<span class="op">::</span><span class="kw">ncvTest</span>(ptLM2)</a></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 1.380242, Df = 1, p = 0.24006</code></pre>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb632-1" data-line-number="1"><span class="co"># We meet our assumptions</span></a></code></pre></div>
<p>Unusual Observations</p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb633-1" data-line-number="1"><span class="co"># Outliers</span></a>
<a class="sourceLine" id="cb633-2" data-line-number="2"><span class="co"># We can identify points that have residuals greater than 2 standard deviations away from our model&#39;s prediction</span></a>
<a class="sourceLine" id="cb633-3" data-line-number="3">ptLM2<span class="op">$</span>residuals[<span class="kw">abs</span>(ptLM2<span class="op">$</span>residuals) <span class="op">&gt;=</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">sd</span>(ptLM2<span class="op">$</span>residuals)]</a></code></pre></div>
<pre><code>##         9 
## -17.27998</code></pre>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb635-1" data-line-number="1"><span class="co"># Point 9 is an outlier</span></a>
<a class="sourceLine" id="cb635-2" data-line-number="2"></a>
<a class="sourceLine" id="cb635-3" data-line-number="3"><span class="co"># We can check for leverage points with a number of ways.  We&#39;ll check using Cook&#39;s distance</span></a>
<a class="sourceLine" id="cb635-4" data-line-number="4"><span class="kw">plot</span>(ptLM2, <span class="dt">which =</span> <span class="dv">4</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb636-1" data-line-number="1"><span class="co"># Again point 9 is a point of significant leverage.  </span></a></code></pre></div>
<p>Based on these results, we may consider dropping point nine. Before doing so, we should check for data entry errors or anything unusual about that data point. If we do drop it, we should note that we did so in our analysis.</p>
<p>If we do conclude that point nine should be dropped, we can build a new linear model:</p>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb637-1" data-line-number="1"><span class="co"># Just check the summary of the model with Point 9 dropped</span></a>
<a class="sourceLine" id="cb637-2" data-line-number="2"><span class="kw">summary</span>(<span class="kw">lm</span>(Satisfaction <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>illSeverity, <span class="dt">data =</span> pt[<span class="op">-</span><span class="dv">9</span>,]))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Satisfaction ~ Age + illSeverity, data = pt[-9, 
##     ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.870  -3.700   0.834   3.595  12.169 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 147.9415     5.2186  28.349  &lt; 2e-16 ***
## Age          -1.1484     0.1044 -11.003 3.54e-10 ***
## illSeverity  -0.5054     0.1120  -4.513 0.000191 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.003 on 21 degrees of freedom
## Multiple R-squared:  0.9292, Adjusted R-squared:  0.9224 
## F-statistic: 137.8 on 2 and 21 DF,  p-value: 8.449e-13</code></pre>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb639-1" data-line-number="1"><span class="co"># Note that our standard errors decrease somewhat and our R^2 increases, indicating this is a better model</span></a>
<a class="sourceLine" id="cb639-2" data-line-number="2"><span class="co"># (though on a smaller subset of the data)</span></a></code></pre></div>
</div>
<div id="multiple-linear-regression-problem-set" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Multiple Linear Regression Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter6_ProblemSets/Multi_Linear_Regression_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter6_ProblemSets/Multi_Linear_Regression_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter6_ProblemSets/Multi_Linear_Regression_PS_Answers.html'>here</a>.</p>
<!-------------------------------------------------------------------------------------------------------------------------->
</div>
</div>
<div id="categorical-variables" class="section level2">
<h2><span class="header-section-number">7.5</span> Categorical Variables</h2>
<p>In the ANOVA chapter, you were introduced to a method for evaluating differences in a response for three or more sample populations. In the dataset, each sample population was associated with a different level of a categorical variable. In linear regression, the categorical variable itself can be included as one of the predictors as long as we ensure that the categorical variable is a <code>factor</code> variable type in the formula we supply to the <code>lm()</code> function.</p>
<p>Well revisit the <code>ames</code> dataset from the ANOVA chapter. Well just consider one predictor, <code>Mo.Sold</code>, with <code>SalePrice</code> as the response. Recall that <code>Mo.Sold</code> is a discrete number between 1 and 12 that represents the month a house was sold. When we read the data into a tibble, <code>Mo.Sold</code> is a numeric data type since it consists of discrete numbers.</p>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb640-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb640-2" data-line-number="2"></a>
<a class="sourceLine" id="cb640-3" data-line-number="3">ames =<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_csv</span>(<span class="st">&#39;../html/_Chapter3_ProblemSets/ames.csv&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(Mo.Sold, SalePrice)</a>
<a class="sourceLine" id="cb640-4" data-line-number="4"></a>
<a class="sourceLine" id="cb640-5" data-line-number="5"><span class="kw">class</span>(ames<span class="op">$</span>Mo.Sold)</a></code></pre></div>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
<p>We need to convert <code>Mo.Sold</code> to a factor to correctly perform ANOVA.</p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb642-1" data-line-number="1">ames<span class="op">$</span>Mo.Sold =<span class="st"> </span><span class="kw">as.factor</span>(ames<span class="op">$</span>Mo.Sold)</a>
<a class="sourceLine" id="cb642-2" data-line-number="2"></a>
<a class="sourceLine" id="cb642-3" data-line-number="3">ames.aov =<span class="st"> </span><span class="kw">aov</span>(SalePrice <span class="op">~</span><span class="st"> </span>Mo.Sold, <span class="dt">data=</span>ames)</a>
<a class="sourceLine" id="cb642-4" data-line-number="4"><span class="kw">summary</span>(ames.aov)</a></code></pre></div>
<pre><code>##               Df    Sum Sq  Mean Sq F value Pr(&gt;F)  
## Mo.Sold       11 1.353e+11 1.23e+10   1.934  0.031 *
## Residuals   2918 1.856e+13 6.36e+09                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>And now we can also include <code>Mo.Sold</code> as a predictor in a linear model.</p>
<div class="sourceCode" id="cb644"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb644-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">lm</span>(SalePrice <span class="op">~</span><span class="st"> </span>Mo.Sold, <span class="dt">data=</span>ames))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = SalePrice ~ Mo.Sold, data = ames)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -168754  -51556  -19087   33297  560790 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   194210       7190  27.009  &lt; 2e-16 ***
## Mo.Sold2      -15846       9976  -1.588  0.11231    
## Mo.Sold3      -18080       8895  -2.033  0.04218 *  
## Mo.Sold4      -26498       8631  -3.070  0.00216 ** 
## Mo.Sold5      -20510       8234  -2.491  0.01280 *  
## Mo.Sold6      -12668       8018  -1.580  0.11427    
## Mo.Sold7       -9843       8116  -1.213  0.22529    
## Mo.Sold8       -7988       8888  -0.899  0.36890    
## Mo.Sold9       -2658       9550  -0.278  0.78080    
## Mo.Sold10     -14153       9406  -1.505  0.13250    
## Mo.Sold11      -6559       9807  -0.669  0.50368    
## Mo.Sold12      -9756      10623  -0.918  0.35851    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 79750 on 2918 degrees of freedom
## Multiple R-squared:  0.007239,   Adjusted R-squared:  0.003497 
## F-statistic: 1.934 on 11 and 2918 DF,  p-value: 0.03103</code></pre>
<p>Notice that the model F-statistic, degrees of freedom, and p-value of the linear model are identical to those of ANOVA. If we extract the coefficients from the ANOVA model, we see that they, too, are identical to the linear model coefficients.</p>
<div class="sourceCode" id="cb646"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb646-1" data-line-number="1">ames.aov<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>## (Intercept)    Mo.Sold2    Mo.Sold3    Mo.Sold4    Mo.Sold5    Mo.Sold6 
##  194210.016  -15845.670  -18079.555  -26498.027  -20509.801  -12667.454 
##    Mo.Sold7    Mo.Sold8    Mo.Sold9   Mo.Sold10   Mo.Sold11   Mo.Sold12 
##   -9843.152   -7987.553   -2657.873  -14152.953   -6558.751   -9755.968</code></pre>
<p>The difference between ANOVA and linear regression is that in ANOVA, the categorical variable is effect coded, and in linear regression, the categorical variable is dummy coded. Effect coding means that each factor level is coded with 1s and -1s so that each categorys mean is compared to the overall mean. For example, the mean sale price for the month of January is:</p>
<div class="sourceCode" id="cb648"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb648-1" data-line-number="1">ames <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Mo.Sold<span class="op">==</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">meanPrice =</span> <span class="kw">mean</span>(SalePrice))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   meanPrice
##       &lt;dbl&gt;
## 1   194210.</code></pre>
<p>Dummy coding means that factor levels are coded with 0s for all levels except the one of interest, which is coded with a 1.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a> This creates dummy variables in the linear regression equation where each level is represented in the equation by its own variable. For the <code>ames</code> data, the regression equation becomes:</p>
<center>
<span class="math display">\[y=\beta_{0}+\beta_{1}x_{1}+...+\beta_{11}x_{11}+\varepsilon\]</span>
</center>
<p>Here, <span class="math inline">\(\beta_{0}\)</span> represents the coefficient for January, <span class="math inline">\(\beta_{1}\)</span> is the coefficient for February, and so on. If we want to find the mean sale price for January, we set <span class="math inline">\(x_{1}\)</span> through <span class="math inline">\(x_{11}\)</span> to 0, and were left with <span class="math inline">\(y=\beta_{0}\)</span>. In other words, the y-intercept is the mean sale price for January. We can do this explicitly by:</p>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb650-1" data-line-number="1">ames <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb650-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dummy =</span> <span class="kw">ifelse</span>(Mo.Sold<span class="op">==</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>),  <span class="co"># one-hot encoding</span></a>
<a class="sourceLine" id="cb650-3" data-line-number="3">         <span class="dt">jan =</span> dummy <span class="op">*</span><span class="st"> </span>SalePrice) <span class="op">%&gt;%</span><span class="st">       </span><span class="co"># multiply dummy variable and sale price</span></a>
<a class="sourceLine" id="cb650-4" data-line-number="4"><span class="st">  </span><span class="kw">filter</span>(jan<span class="op">!=</span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st">                        </span><span class="co"># don&#39;t include the zeros in the mean</span></a>
<a class="sourceLine" id="cb650-5" data-line-number="5"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">meanPrice =</span> <span class="kw">mean</span>(jan))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   meanPrice
##       &lt;dbl&gt;
## 1   194210.</code></pre>
<p>You can see that this is the same value as the intercept coefficient in the linear model summary. Fundamentally, these two coding techniques are just different ways of doing the same thing, and so in <em>R</em>, the <code>aov()</code> function is just a wrapper for the <code>lm()</code> function. For a more detailed discussion on effect versus dummy coding, please refer to <a href="https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis/">this site</a> at the UCLA Institute for Digital Research and Education.</p>
<p>A nice feature of the <code>lm()</code> function is that we dont have to manually create dummy variables for factor levels - it does it for us. The following code demonstrates. First the manual example with just the first three months.</p>
<div class="sourceCode" id="cb652"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb652-1" data-line-number="1">ames =<span class="st"> </span>ames <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb652-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">feb=</span><span class="kw">ifelse</span>(Mo.Sold<span class="op">==</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb652-3" data-line-number="3">         <span class="dt">mar=</span><span class="kw">ifelse</span>(Mo.Sold<span class="op">==</span><span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb652-4" data-line-number="4"></a>
<a class="sourceLine" id="cb652-5" data-line-number="5"><span class="kw">summary</span>(<span class="kw">lm</span>(SalePrice <span class="op">~</span><span class="st"> </span>feb <span class="op">+</span><span class="st"> </span>mar, <span class="dt">data=</span>ames <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Mo.Sold <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>))))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = SalePrice ~ feb + mar, data = ames %&gt;% filter(Mo.Sold %in% 
##     c(1, 2, 3)))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -165264  -53701  -17997   30319  560790 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   194210       7960  24.398   &lt;2e-16 ***
## feb           -15846      11044  -1.435    0.152    
## mar           -18080       9847  -1.836    0.067 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 88280 on 485 degrees of freedom
## Multiple R-squared:  0.007313,   Adjusted R-squared:  0.003219 
## F-statistic: 1.786 on 2 and 485 DF,  p-value: 0.1687</code></pre>
<p>Now let <code>lm()</code> do all the work.</p>
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb654-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">lm</span>(SalePrice <span class="op">~</span><span class="st"> </span>Mo.Sold, <span class="dt">data=</span>ames <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Mo.Sold <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>))))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = SalePrice ~ Mo.Sold, data = ames %&gt;% filter(Mo.Sold %in% 
##     c(1, 2, 3)))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -165264  -53701  -17997   30319  560790 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   194210       7960  24.398   &lt;2e-16 ***
## Mo.Sold2      -15846      11044  -1.435    0.152    
## Mo.Sold3      -18080       9847  -1.836    0.067 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 88280 on 485 degrees of freedom
## Multiple R-squared:  0.007313,   Adjusted R-squared:  0.003219 
## F-statistic: 1.786 on 2 and 485 DF,  p-value: 0.1687</code></pre>
<div id="categorical-linear-regression-problem-set" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Categorical Linear Regression Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter6_ProblemSets/Categorical_Linear_Regression_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter6_ProblemSets/Categorical_Linear_Regression_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter6_ProblemSets/Categorical_Linear_Regression_PS_Answers.html'>here</a>.</p>
</div>
</div>
<div id="transformation" class="section level2">
<h2><span class="header-section-number">7.6</span> Transformation</h2>
<p>Now that we have designs that include factors with more than two levels, we have the ability to evaluate non-linear relationships between predictor and response variables. Incorporating non-linear terms in a linear model is accomplished by transforming either the response or predictors. For an overview of transformation, read <a href = "http://fmwww.bc.edu/RePEc/bocode/t/transint.html">Transformations: an introduction</a> by Nicholas J. Cox at Durham University. Skip the section titled How to do transformations in Stata; well replace that with How to do transformations in <em>R</em> below.</p>
<div id="identifying-non-linear-relationships" class="section level3">
<h3><span class="header-section-number">7.6.1</span> Identifying Non-Linear Relationships</h3>
<p>I think the simplest way to screen your data for potential non-linear relationships is with a pairs plot that includes a smoother. To demonstrate, Ill take the CCD we created earlier and add a response, y, that has a non-linear relationship with the speed and stealth factors. I also substracted 2 from the stealth factor to center it at 0.</p>
<div class="sourceCode" id="cb656"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb656-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb656-2" data-line-number="2"></a>
<a class="sourceLine" id="cb656-3" data-line-number="3">ccdGrid =<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb656-4" data-line-number="4">  <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="dv">4</span>)),</a>
<a class="sourceLine" id="cb656-5" data-line-number="5">  <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">each =</span> <span class="dv">2</span>)),</a>
<a class="sourceLine" id="cb656-6" data-line-number="6">  <span class="dt">x3 =</span> <span class="kw">c</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>), <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">4</span>), <span class="kw">rep</span>(<span class="dv">3</span>,<span class="dv">4</span>)),</a>
<a class="sourceLine" id="cb656-7" data-line-number="7">  <span class="dt">star =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&#39;y&#39;</span>, <span class="dv">7</span>), <span class="kw">rep</span>(<span class="st">&#39;n&#39;</span>,<span class="dv">8</span>)),</a>
<a class="sourceLine" id="cb656-8" data-line-number="8">  <span class="dt">line =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&#39;line1&#39;</span>,<span class="dv">2</span>), <span class="kw">rep</span>(<span class="st">&#39;line2&#39;</span>,<span class="dv">2</span>), <span class="kw">rep</span>(<span class="st">&#39;line3&#39;</span>,<span class="dv">2</span>), <span class="kw">rep</span>(<span class="st">&#39;none&#39;</span>,<span class="dv">9</span>))</a>
<a class="sourceLine" id="cb656-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb656-10" data-line-number="10"></a>
<a class="sourceLine" id="cb656-11" data-line-number="11"></a>
<a class="sourceLine" id="cb656-12" data-line-number="12">ccd =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">speed =</span> ccdGrid<span class="op">$</span>x1, </a>
<a class="sourceLine" id="cb656-13" data-line-number="13">             <span class="dt">stealth =</span> ccdGrid<span class="op">$</span>x2 <span class="op">-</span><span class="st"> </span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb656-14" data-line-number="14">             <span class="dt">surv =</span> ccdGrid<span class="op">$</span>x3, </a>
<a class="sourceLine" id="cb656-15" data-line-number="15">             <span class="dt">y =</span> <span class="kw">log</span>(speed) <span class="op">-</span><span class="st"> </span>stealth<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>surv)</a>
<a class="sourceLine" id="cb656-16" data-line-number="16"></a>
<a class="sourceLine" id="cb656-17" data-line-number="17">smooth_fn &lt;-<span class="st"> </span><span class="cf">function</span>(data, mapping, ...){</a>
<a class="sourceLine" id="cb656-18" data-line-number="18">  <span class="kw">ggplot</span>(<span class="dt">data =</span> data, <span class="dt">mapping =</span> mapping) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb656-19" data-line-number="19"><span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb656-20" data-line-number="20"><span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">formula =</span> y<span class="op">~</span>x, <span class="dt">method=</span>loess, <span class="dt">fill=</span><span class="st">&quot;red&quot;</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, ...)</a>
<a class="sourceLine" id="cb656-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb656-22" data-line-number="22"></a>
<a class="sourceLine" id="cb656-23" data-line-number="23"><span class="kw">ggpairs</span>(ccd, <span class="dt">lower=</span><span class="kw">list</span>(<span class="dt">continuous=</span>smooth_fn), <span class="dt">progress=</span><span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<p>A visual inspection of the last row of plots is enough to identify the non-linear relationships that speed and stealth have with the response. We can also look at the density plot for the response (the lower right curve) and see some skewness away from a normal distribution.</p>
</div>
<div id="checking-model-structure" class="section level3">
<h3><span class="header-section-number">7.6.2</span> Checking Model Structure</h3>
<p>Generating a pairs plot is a screening process only. For a more complete analysis, we need to check the model structure. Recall that one of the key assumptions of the linear regression model is that the regression errors are independent and identically distributed. If that assumption is not true, then the non-linear portion of the relationship between predictor and response will be contained in the (estimated) residuals, <span class="math inline">\(\hat{\varepsilon}\)</span>. Plotting the residuals versus the individual predictors is one method of checking model structure. In <em>R</em>, we can do this with <code>termplot</code>.</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb657-1" data-line-number="1"><span class="co"># first, we need a linear model</span></a>
<a class="sourceLine" id="cb657-2" data-line-number="2">ccd.lm =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>ccd)</a>
<a class="sourceLine" id="cb657-3" data-line-number="3"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb657-4" data-line-number="4"><span class="kw">termplot</span>(ccd.lm, <span class="dt">partial.resid =</span> <span class="ot">TRUE</span>, <span class="dt">col.res=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Residuals vs. Predictor&quot;</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>In these plots, were checking for whether there is a non-linear shape to the data by looking for trends in the blue circles. The red lines are the coefficients from the linear model for reference. The non-linear shape to stealth is clearly visible, but were missing the non-linearity in speed. Unfortunately, partial residual plots only <em>suggest</em> transformations for the predictors because they are influenced by other predictor variables, and (if present) influential observations and multicollinearity. The process is done manually in <em>R</em>. First, since stealth appears to be an inverse square, well transform that variable, then re-fit the model, and check the partial residuals again. Before we do that, we need to know how to transform variables.</p>
</div>
<div id="how-to-transform-variables-in-r" class="section level3">
<h3><span class="header-section-number">7.6.3</span> How To Transform Variables In <em>R</em></h3>
<p>To account for non-linear relationships in a linear model, we need to transform the variables in the <code>lm</code> function. From the partial residuals plot, we know we should try a quadratic term for stealth. Summaries of linear models without and then with the transformation are shown below for the <code>ccd</code> data.</p>
<div class="sourceCode" id="cb658"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb658-1" data-line-number="1"><span class="co"># without transformation</span></a>
<a class="sourceLine" id="cb658-2" data-line-number="2">ccd.lm =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>speed <span class="op">+</span><span class="st"> </span>stealth <span class="op">+</span><span class="st"> </span>surv, <span class="dt">data =</span> ccd)</a>
<a class="sourceLine" id="cb658-3" data-line-number="3"><span class="kw">summary</span>(ccd.lm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ speed + stealth + surv, data = ccd)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.3813 -0.3813 -0.3813  0.6187  0.7626 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.168e+00  5.462e-01  -2.139  0.05574 .  
## speed        5.493e-01  1.855e-01   2.961  0.01295 *  
## stealth     -2.708e-18  1.855e-01   0.000  1.00000    
## surv         1.000e+00  1.855e-01   5.390  0.00022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5867 on 11 degrees of freedom
## Multiple R-squared:  0.7747, Adjusted R-squared:  0.7132 
## F-statistic: 12.61 on 3 and 11 DF,  p-value: 0.0006996</code></pre>
<div class="sourceCode" id="cb660"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb660-1" data-line-number="1"><span class="co"># with transformation</span></a>
<a class="sourceLine" id="cb660-2" data-line-number="2">ccd_t.lm =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>speed <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(stealth<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>surv, <span class="dt">data =</span> ccd)</a>
<a class="sourceLine" id="cb660-3" data-line-number="3"><span class="kw">summary</span>(ccd_t.lm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ speed + I(stealth^2) + surv, data = ccd)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.08631 -0.02877 -0.02877  0.05754  0.11507 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.46300    0.07257   -6.38 5.22e-05 ***
## speed         0.54931    0.02295   23.94 7.72e-11 ***
## I(stealth^2) -1.05754    0.03975  -26.61 2.46e-11 ***
## surv          1.00000    0.02295   43.58 1.13e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.07257 on 11 degrees of freedom
## Multiple R-squared:  0.9966, Adjusted R-squared:  0.9956 
## F-statistic:  1060 on 3 and 11 DF,  p-value: 8.06e-14</code></pre>
<p>Notice in the call to <code>lm</code> with the transformed variables, the polynomial term is surrounded by <code>I()</code>. This is to avoid confusion between arithmetic and symbolic uses of <code>+</code> in the <code>formula</code> function (see <code>?formula</code> for more details). Lets take another look at the partial residual plots with stealth transformed.</p>
<div class="sourceCode" id="cb662"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb662-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb662-2" data-line-number="2"><span class="kw">termplot</span>(ccd_t.lm, <span class="dt">partial.resid =</span> <span class="ot">TRUE</span>, <span class="dt">col.res=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Residuals vs. Predictor&quot;</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>Stealth looks much better, and now were able to see the non-linear relationship with speed. Re-fit and plot again with a log transformation on speed.</p>
<div class="sourceCode" id="cb663"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb663-1" data-line-number="1">ccd_t2.lm =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(speed) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(stealth<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>surv, <span class="dt">data =</span> ccd)</a>
<a class="sourceLine" id="cb663-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb663-3" data-line-number="3"><span class="kw">termplot</span>(ccd_t2.lm, <span class="dt">partial.resid =</span> <span class="ot">TRUE</span>, <span class="dt">col.res=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Residuals vs. Predictor&quot;</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>I didnt add any error to the data, so we now have a perfect fit. With real-world data, there will be noise, and the best transformation isnt known in advance. The following chart from <a href="https://statswithcats.wordpress.com/2010/11/21/fifty-ways-to-fix-your-data/">Stats With Cats</a> is a useful guide when determining what transformations to try.</p>
<center>
<img src="https://statswithcats.files.wordpress.com/2010/11/independent-variable-transformations.jpg">
</center>
<p>Now consider results from the 17-point NOLH we created earlier. I added a response, y, with a non-linear relationship to one of the factors, and I added some noise to make this example more realistic. Plotting just these two variables with a linear regression line reveals a little curvature to the trend, but its not extreme.</p>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb664-1" data-line-number="1">nolh.lm1 =<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x, <span class="dt">data=</span>nolh)</a>
<a class="sourceLine" id="cb664-2" data-line-number="2"><span class="kw">termplot</span>(nolh.lm1, <span class="dt">partial.resid =</span> <span class="ot">TRUE</span>, <span class="dt">col.res=</span><span class="st">&#39;blue&#39;</span>)</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<p>Weve picked up the non-linear shape, and it looks like we need some degree of polynomial as a transformation. For reference, lets look at the linear model summary.</p>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb665-1" data-line-number="1"><span class="kw">summary</span>(nolh.lm1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = nolh)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -665.4 -497.3 -108.8  364.6 1245.6 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -1193.3      314.6  -3.793  0.00177 ** 
## x              286.8       30.7   9.343 1.21e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 620.1 on 15 degrees of freedom
## Multiple R-squared:  0.8534, Adjusted R-squared:  0.8436 
## F-statistic: 87.29 on 1 and 15 DF,  p-value: 1.212e-07</code></pre>
<p>Thats not a bad fit, but we can probably improve it by trying a transformation. The curvature suggests a polynomial might be better, so lets try a second degree polynomial fit. First the plot, then the linear model summary.</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = y ~ I(x^2), data = nolh)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -304.4 -243.1  -25.1  228.5  495.9 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -349.0775    97.0449  -3.597  0.00264 ** 
## I(x^2)        16.5459     0.6993  23.660 2.73e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 261.6 on 15 degrees of freedom
## Multiple R-squared:  0.9739, Adjusted R-squared:  0.9722 
## F-statistic: 559.8 on 1 and 15 DF,  p-value: 2.73e-13</code></pre>
<p>The plot looks better than the first one. This model also has a higher <span class="math inline">\(R^{2}\)</span> than the first one, so perhaps it is a better fit. What happens if we continue to add higher order polynomials? The <code>poly()</code> function is useful for this purpose.</p>
<div class="sourceCode" id="cb668"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb668-1" data-line-number="1">nolh.poly =<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span><span class="kw">poly</span>(x, <span class="dv">15</span>), <span class="dt">data =</span> nolh)</a>
<a class="sourceLine" id="cb668-2" data-line-number="2"><span class="kw">summary</span>(nolh.poly)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ poly(x, 15), data = nolh)
## 
## Residuals:
##          1          2          3          4          5          6          7 
##  0.0003231 -0.0051699  0.0387741 -0.1809457  0.5880735 -1.4113764  2.5875234 
##          8          9         10         11         12         13         14 
## -3.6964620  4.1585197 -3.6964620  2.5875234 -1.4113764  0.5880735 -0.1809457 
##         15         16         17 
##  0.0387741 -0.0051699  0.0003231 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1388.2398     1.9213 722.541 0.000881 ***
## poly(x, 15)1  5793.9598     7.9218 731.390 0.000870 ***
## poly(x, 15)2  2372.8190     7.9218 299.528 0.002125 ** 
## poly(x, 15)3   371.3985     7.9218  46.883 0.013577 *  
## poly(x, 15)4     6.3875     7.9218   0.806 0.568003    
## poly(x, 15)5    -1.2064     7.9218  -0.152 0.903787    
## poly(x, 15)6    -0.4781     7.9218  -0.060 0.961626    
## poly(x, 15)7    -8.2612     7.9218  -1.043 0.486653    
## poly(x, 15)8    -0.9076     7.9218  -0.115 0.927379    
## poly(x, 15)9    -2.6003     7.9218  -0.328 0.798087    
## poly(x, 15)10    4.4736     7.9218   0.565 0.672730    
## poly(x, 15)11    3.9691     7.9218   0.501 0.704304    
## poly(x, 15)12   -1.8700     7.9218  -0.236 0.852423    
## poly(x, 15)13   -5.3439     7.9218  -0.675 0.622195    
## poly(x, 15)14   -7.5554     7.9218  -0.954 0.515068    
## poly(x, 15)15   -6.5343     7.9218  -0.825 0.560919    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.922 on 1 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:      1 
## F-statistic: 4.179e+04 on 15 and 1 DF,  p-value: 0.003839</code></pre>
<p>With a 15th order polynomial we get an <span class="math inline">\(R^{2}\)</span> of 0.9999745, which is a nearly perfect fit. Notice the p-values, though. They indicate that the best model is actually the one with the second order term. Why is the best model not the one with the highest <span class="math inline">\(R^{2}\)</span>? What weve done is over-fit the model to the data. We can see this by plotting the two polynomial fits.</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-75-1.png" width="672" /></p>
<p>While the model with a 15th order polynomial perfectly fits these data, the model with the second order polynomial will generalize much better.</p>
</div>
<div id="transformed-regression-problem-set" class="section level3">
<h3><span class="header-section-number">7.6.4</span> Transformed Regression Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter6_ProblemSets/Transformed_Regression_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter6_ProblemSets/Transformed_Regression_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter6_ProblemSets/Transformed_Regression_PS_Answers.html'>here</a>.</p>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">7.7</span> Logistic Regression</h2>
<p>So far, weve seen regression techniques for continuous and categorical response variables. There is a different form of regression called logistic regression for the case when the response is a binary variable. We could encounter this situation when analyzing AWARS results, if we were interested in something like whether a unit defeated a counterattack, reached an objective, or ended the simulation at some strength above a fixed threshold.</p>
<div id="motivating-example" class="section level3">
<h3><span class="header-section-number">7.7.1</span> Motivating Example</h3>
<p>The need for a different type of regression can be seen using an example. Well look at just one predictor (<code>age</code>) and the response (<code>chd</code>) in the <code>SAheart</code> dataset from the <code>bestglm</code> package. Here the response, <code>chd</code> is a binary variable indicating whether someone did (1) or did not (0) develop coronoary heart disease. For now, well just look at the first 20 observations with a scatter plot.</p>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb670-1" data-line-number="1">SAheart =<span class="st"> </span>bestglm<span class="op">::</span>SAheart</a>
<a class="sourceLine" id="cb670-2" data-line-number="2"></a>
<a class="sourceLine" id="cb670-3" data-line-number="3"><span class="co"># look at the first 20 observations only and only age vs. chd</span></a>
<a class="sourceLine" id="cb670-4" data-line-number="4">saheart =<span class="st"> </span>SAheart[<span class="dv">1</span><span class="op">:</span><span class="dv">21</span>, <span class="kw">c</span>(<span class="st">&#39;age&#39;</span>, <span class="st">&#39;chd&#39;</span>)]</a>
<a class="sourceLine" id="cb670-5" data-line-number="5"></a>
<a class="sourceLine" id="cb670-6" data-line-number="6"><span class="kw">ggplot</span>(saheart[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>,], <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>chd)) <span class="op">+</span></a>
<a class="sourceLine" id="cb670-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb670-8" data-line-number="8"><span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb670-9" data-line-number="9"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<p>Theres a clear trend here - younger typically didnt have heart disease while older people did - but what exactly is the nature of the relationship? We can also think about this relationship in terms of probability. People under 20 have a virtually 0 probability of heart disease, and people over 60 have a near 1.0 probability of heart disease. But how do we connect those two extremes? If we assume there is a linear relationship, wed get the following plot.</p>
<div class="sourceCode" id="cb671"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb671-1" data-line-number="1"><span class="kw">ggplot</span>(saheart[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>,], <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>chd)) <span class="op">+</span></a>
<a class="sourceLine" id="cb671-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb671-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x, <span class="dt">method=</span>lm, <span class="dt">se=</span><span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb671-4" data-line-number="4"><span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb671-5" data-line-number="5"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<p>The 21st observation happens to be associated with a 20-year old who happened to have heart disease. If we include this new observation and re-fit the linear regression line, we get the following.</p>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb672-1" data-line-number="1"><span class="kw">ggplot</span>(saheart[<span class="dv">1</span><span class="op">:</span><span class="dv">21</span>,], <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>chd)) <span class="op">+</span></a>
<a class="sourceLine" id="cb672-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb672-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x, <span class="dt">method=</span>lm, <span class="dt">se=</span><span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb672-4" data-line-number="4"><span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb672-5" data-line-number="5"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<p>Adding the single observation didnt give us any new information about the probability of heart disease for people in their 40s, 50, and 60s, but considerably changed the fit. Additionally, if we were to extend the regression line to the right to predict the probability of heart disease of an 80-year old, wed get a probability &gt; 1. For these reasons, linear regression doesnt model the relationship well, so we need to find something better.</p>
</div>
<div id="logit-function" class="section level3">
<h3><span class="header-section-number">7.7.2</span> Logit Function</h3>
<p>An alternative to linear regression is to use a <em>logit function</em>, <span class="math inline">\(\eta\)</span> to replace <span class="math inline">\(y\)</span> in the linear regression equation.</p>
<p><span class="math display">\[\eta = \beta_{0}+\beta_{1}x_{1}+...+\beta_{i}x_{i}+\varepsilon\]</span></p>
<p>where,</p>
<p><span class="math display">\[\eta = log\left\lgroup{\frac{p}{1-p}}\right\rgroup\]</span></p>
<p>and where <span class="math inline">\(p\)</span> is the probability of heart disease. In this form <span class="math inline">\(\eta\)</span> can also be thought of in terms of <span class="math inline">\(log(odds)\)</span>. To enforce <span class="math inline">\(0\le p \le 1\)</span>, we further define <span class="math inline">\(p\)</span> as:</p>
<p><span class="math display">\[p=\frac{e^{\eta}}{1+e^{\eta}}\]</span></p>
<p>With one predictor, as in our case, we can rewrite this to become:</p>
<p><span class="math display">\[p=\frac{e^{\beta_{0}+\beta_{1}x_{1}}}{1+e^{\beta_{0}+\beta_{1}x_{1}}}\]</span></p>
<p>If we now set <span class="math inline">\(\beta_{0}=0\)</span> and allow <span class="math inline">\(\beta_{1}\)</span> to vary, we can see the shape of the response for different coefficient values.</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<p>Note that if <span class="math inline">\(\beta_{1}=0\)</span>, that is the equivalent of saying that <span class="math inline">\(p\)</span> is not a function of <span class="math inline">\(x\)</span>. The reverse (allowing <span class="math inline">\(\beta_{0}\)</span> to vary while holding <span class="math inline">\(\beta_{1}=1\)</span>), shifts the curve horizontally.</p>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-80-1.png" width="672" /></p>
</div>
<div id="logistic-regression-in-r" class="section level3">
<h3><span class="header-section-number">7.7.3</span> Logistic Regression in <em>R</em></h3>
<p>To fit a logistic regression model in <em>R</em>, use <code>glm()</code> instead of <code>lm()</code> and specify <code>family=binomial</code>.</p>
<div class="sourceCode" id="cb673"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb673-1" data-line-number="1">sa.glm =<span class="st"> </span><span class="kw">glm</span>(chd<span class="op">~</span>age, <span class="dt">family=</span>binomial, <span class="dt">data=</span>SAheart)</a>
<a class="sourceLine" id="cb673-2" data-line-number="2"><span class="kw">summary</span>(sa.glm)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = chd ~ age, family = binomial, data = SAheart)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4321  -0.9215  -0.5392   1.0952   2.2433  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.521710   0.416031  -8.465  &lt; 2e-16 ***
## age          0.064108   0.008532   7.513 5.76e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 596.11  on 461  degrees of freedom
## Residual deviance: 525.56  on 460  degrees of freedom
## AIC: 529.56
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>From the summary, we see that <span class="math inline">\(\beta_{0} = -3.522\)</span> and <span class="math inline">\(\beta_{1} = 0.064\)</span>, which gives us the equation for the estimated linear predictor:</p>
<p><span class="math display">\[\hat{\eta} = -3.522 + 0.064x\]</span></p>
<p>and the equation for the fitted probabilities.</p>
<p><span class="math display">\[\hat{p}=\frac{e^{-3.522 + 0.064x}}{1+e^{-3.522 + 0.064x}}\]</span></p>
<p>Given a 40-year old, we find <span class="math inline">\(\hat{\eta}=\)</span> -0.962 and <span class="math inline">\(\hat{p}=\)</span> 0.2764779. This highlights an important distinction when using <code>predict()</code> with a binomial response. To calculate <span class="math inline">\(\hat{\eta}\)</span>:</p>
<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb675-1" data-line-number="1"><span class="kw">predict</span>(sa.glm, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">age=</span><span class="dv">40</span>))</a></code></pre></div>
<pre><code>##         1 
## -0.957389</code></pre>
<p>but to calculate <span class="math inline">\(\hat{p}\)</span>, we need to specify <code>type = &quot;response&quot;</code>.</p>
<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb677-1" data-line-number="1">p.hat =<span class="st"> </span><span class="kw">predict</span>(sa.glm, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">age=</span><span class="dv">40</span>), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</a>
<a class="sourceLine" id="cb677-2" data-line-number="2">p.hat</a></code></pre></div>
<pre><code>##         1 
## 0.2774013</code></pre>
<p>We can see that this is a much lower estimate of the probability of heart disease than was estimated by the linear model produced by <code>lm()</code>. Since <span class="math inline">\(\beta_{0}\)</span> is negative, the regression curve will be shifted to the right of the mean age, and a low value for <span class="math inline">\(\beta_{1}\)</span> will stretch out the s curve. A plot of <span class="math inline">\(\hat{p}\)</span> versus age with the binomial regression curve and our estimated probability for a 40-year old is shown below.</p>
<div class="sourceCode" id="cb679"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb679-1" data-line-number="1">ages =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">10</span>, <span class="dv">80</span>, <span class="dt">length.out =</span> <span class="kw">nrow</span>(SAheart))</a>
<a class="sourceLine" id="cb679-2" data-line-number="2">pred =<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb679-3" data-line-number="3">  <span class="dt">p =</span> <span class="kw">predict</span>(sa.glm, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">age=</span>ages), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>),</a>
<a class="sourceLine" id="cb679-4" data-line-number="4">  <span class="dt">se =</span> <span class="kw">predict</span>(sa.glm, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">age=</span>ages), <span class="dt">type=</span><span class="st">&quot;response&quot;</span>, <span class="dt">se=</span><span class="ot">TRUE</span>)<span class="op">$</span>se, <span class="co"># standard error</span></a>
<a class="sourceLine" id="cb679-5" data-line-number="5">  <span class="dt">age =</span> ages</a>
<a class="sourceLine" id="cb679-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb679-7" data-line-number="7"></a>
<a class="sourceLine" id="cb679-8" data-line-number="8"><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb679-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> pred, <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>p), <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-10" data-line-number="10"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> pred, <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>p<span class="op">+</span>se), <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">3</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-11" data-line-number="11"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> pred, <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>p<span class="op">-</span>se), <span class="dt">color=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">linetype=</span><span class="dv">3</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-12" data-line-number="12"><span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">data =</span> SAheart, <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>chd), <span class="dt">shape=</span><span class="dv">124</span>, <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">width=</span><span class="fl">0.2</span>, <span class="dt">height=</span><span class="dv">0</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-13" data-line-number="13"><span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">x=</span><span class="dv">40</span>, <span class="dt">xend=</span><span class="dv">40</span>, <span class="dt">y=</span><span class="dv">0</span>, <span class="dt">yend=</span>p.hat), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-14" data-line-number="14"><span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">x=</span><span class="dv">10</span>, <span class="dt">xend=</span><span class="dv">40</span>, <span class="dt">y=</span>p.hat, <span class="dt">yend=</span>p.hat), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>, <span class="dt">linetype=</span><span class="dv">2</span>, <span class="dt">size=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-15" data-line-number="15"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Age (years)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-16" data-line-number="16"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Heart Disease (1=yes, 0=no)</span><span class="ch">\n</span><span class="st"> p.hat&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb679-17" data-line-number="17"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-84-1.png" width="672" /></p>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb680-1" data-line-number="1"><span class="kw">rm</span>(ages, p.hat)</a></code></pre></div>
</div>
<div id="logistic-regression-diagnostics" class="section level3">
<h3><span class="header-section-number">7.7.4</span> Logistic Regression Diagnostics</h3>
<p>Diagnostics for logistic regression follows the same philosophy as linear regression: we will check the model assumptions and look for outliers and high leverage observations. First, well look for violations of the equal variance assumption, but instead of using the raw residuals as we did in linear regression, we need to look at the <em>deviance residuals</em>. For logistic regression, we have the following definitions:</p>
<ul>
<li>Fitted values are <span class="math inline">\(\hat{\eta} = \hat{\beta_0} + \sum_{i=1}^n \hat{\beta_i}x_i\)</span></li>
<li>Raw residuals are <span class="math inline">\(e_{i} = y_{i} - \hat{p_{i}}\)</span></li>
<li>Deviance residuals are <span class="math inline">\(r_{i} = sign(y_{i}-\hat{p_{i}}) \sqrt{-2 \left\{y_{i} ln(\hat{p_{i}}) + (1-y_{i}) ln(1-\hat{p_{i}})\right\}}\)</span>
<ul>
<li>where <span class="math inline">\(y_{i}\)</span> is either 0 or 1, so if <span class="math inline">\(y_{i}=0\)</span>, then <span class="math inline">\(sign() = +\)</span></li>
</ul></li>
</ul>
<p>As with linear model diagnostics, we can plot fitted values and deviance residuals; however, the plot is not particularly useful. Note that the upper row of points correspond to <span class="math inline">\(y_{i}=1\)</span>, and the lower row to <span class="math inline">\(y_{i}=0\)</span>. With a sufficiently large dataset, we can generate a more useful diagnostic plot by binning the observations based on their predicted value, and calculating the mean deviance residual for each bin.</p>
<div class="sourceCode" id="cb681"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb681-1" data-line-number="1">df =<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb681-2" data-line-number="2">  <span class="dt">resid =</span> <span class="kw">residuals</span>(sa.glm), <span class="co"># for raw residuals, specify residuals(sa.glm, type = &quot;response&quot;)</span></a>
<a class="sourceLine" id="cb681-3" data-line-number="3">  <span class="dt">preds =</span> <span class="kw">predict</span>(sa.glm))</a>
<a class="sourceLine" id="cb681-4" data-line-number="4"></a>
<a class="sourceLine" id="cb681-5" data-line-number="5"><span class="kw">ggplot</span>(df) <span class="op">+</span></a>
<a class="sourceLine" id="cb681-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_rect</span>(<span class="kw">aes</span>(<span class="dt">xmin=</span><span class="op">-</span><span class="fl">2.2</span>, <span class="dt">xmax=</span><span class="op">-</span><span class="fl">1.8</span>, <span class="dt">ymin=</span><span class="op">-</span><span class="fl">0.8</span>, <span class="dt">ymax=</span><span class="fl">2.2</span>), <span class="dt">fill=</span><span class="st">&#39;lightgray&#39;</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb681-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>preds, <span class="dt">y=</span>resid)) <span class="op">+</span></a>
<a class="sourceLine" id="cb681-8" data-line-number="8"><span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="dt">x=</span><span class="op">-</span><span class="dv">2</span>, <span class="dt">y=</span><span class="dv">1</span>, <span class="dt">label=</span><span class="st">&quot;Example</span><span class="ch">\n</span><span class="st">Bin&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb681-9" data-line-number="9"><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted Linear Predictor&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb681-10" data-line-number="10"><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Deviance Residuals&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb681-11" data-line-number="11"><span class="st">    </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb682-1" data-line-number="1"><span class="co"># alternatively, plot(sa.glm, which=1)</span></a></code></pre></div>
<p>A general guideline is to create bins with at least 30 observations each, which for the <code>SAheart</code> dataset, means <code>462 %/% 30 = 15</code> bins. Now we have a much more useful diagnostic plot.</p>
<div class="sourceCode" id="cb683"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb683-1" data-line-number="1">df =<span class="st"> </span>df <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb683-2" data-line-number="2"><span class="st">  </span><span class="kw">arrange</span>(preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb683-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">bin =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dt">each=</span><span class="dv">30</span>), <span class="kw">rep</span>(<span class="dv">15</span>, <span class="kw">nrow</span>(SAheart)<span class="op">-</span><span class="dv">15</span><span class="op">*</span><span class="dv">30</span>)))</a>
<a class="sourceLine" id="cb683-4" data-line-number="4"></a>
<a class="sourceLine" id="cb683-5" data-line-number="5">df <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb683-6" data-line-number="6"><span class="st">  </span><span class="kw">group_by</span>(bin) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb683-7" data-line-number="7"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb683-8" data-line-number="8">    <span class="dt">meanResid =</span> <span class="kw">mean</span>(resid),</a>
<a class="sourceLine" id="cb683-9" data-line-number="9">    <span class="dt">meanPred =</span> <span class="kw">mean</span>(preds), <span class="dt">.groups =</span> <span class="st">&#39;drop&#39;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb683-10" data-line-number="10"><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb683-11" data-line-number="11"><span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>meanPred, <span class="dt">y=</span>meanResid)) <span class="op">+</span></a>
<a class="sourceLine" id="cb683-12" data-line-number="12"><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted Linear Predictor&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb683-13" data-line-number="13"><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Deviance Residuals&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb683-14" data-line-number="14"><span class="st">    </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="05-Regression_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<p>We identify unusual observations in logistic regression the same way as we did with linear regression but with slightly different definitions for residuals. We already covered raw residuals and deviance residuals. If we now represent the deviance residuals as <span class="math inline">\(r_{D}\)</span>, then we have the following additional definitions:</p>
<table>
<colgroup>
<col width="19%" />
<col width="38%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th align="center">Definition</th>
<th align="center"><em>R</em> Command</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Standardized deviance residuals</td>
<td align="center"><span class="math inline">\(r_{SD}=\frac{r_{D}}{\sqrt{1-h}}\)</span></td>
<td align="center"><code>rstandard(sa.glm)</code></td>
</tr>
<tr class="even">
<td>Pearson residuals</td>
<td align="center"><span class="math inline">\(r_{P}=\frac{y-\hat{p}}{\sqrt{\hat{p}(1-\hat{p})}}\)</span></td>
<td align="center"><code>residuals(sa.glm, type=&quot;pearson&quot;)</code></td>
</tr>
<tr class="odd">
<td>Pearson standardized residuals</td>
<td align="center"><span class="math inline">\(r_{SP}=\frac{r_{P}}{\sqrt{1-h}}\)</span></td>
<td align="center">none</td>
</tr>
<tr class="even">
<td>Cooks Distance</td>
<td align="center"><span class="math inline">\(D=\frac{(r_{SP})^{2}}{q+1} \left({\frac{h}{1-h}} \right)\)</span></td>
<td align="center"><code>cooks.distance(sa.glm)</code></td>
</tr>
</tbody>
</table>
<p>Apply the same rules of thumb when identifying unusual observations as with linear regression.</p>
<p>Lastly, we can assess the goodness of fit for a model using several methods. A simple approximation akin to measuring <span class="math inline">\(R^2\)</span> is:</p>
<p><span class="math display">\[R^{2}=\frac{D_{NULL}-D}{D_{NULL}}\]</span></p>
<p>where <span class="math inline">\(D_{NULL}\)</span> is the null model devience (i.e., the total sum of squares) and <span class="math inline">\(D\)</span> is the logistic regression model deviance. From the calculation below, we find that approximately 12% of the variance in <code>chd</code> is explained by <code>age</code>.</p>
<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb684-1" data-line-number="1">(sa.glm<span class="op">$</span>null <span class="op">-</span><span class="st"> </span>sa.glm<span class="op">$</span>dev) <span class="op">/</span><span class="st"> </span>sa.glm<span class="op">$</span>null</a></code></pre></div>
<pre><code>## [1] 0.1183444</code></pre>
<p><span class="citation">Faraway (<a href="#ref-faraway2006">2006</a>)</span> proposes a more sophisticated measure:</p>
<p><span class="math display">\[R^{2}=\frac{1 - exp\left\{ (D-D_{NULL})/N \right\}} {1 - exp\left\{-D_{NULL}/N \right\}}\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of binary trials.</p>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb686-1" data-line-number="1">(<span class="dv">1</span><span class="op">-</span><span class="kw">exp</span>( (sa.glm<span class="op">$</span>dev <span class="op">-</span><span class="st"> </span>sa.glm<span class="op">$</span>null)<span class="op">/</span><span class="kw">nrow</span>(SAheart))) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span><span class="kw">exp</span>( (<span class="op">-</span><span class="st"> </span>sa.glm<span class="op">$</span>null)<span class="op">/</span><span class="kw">nrow</span>(SAheart)))</a></code></pre></div>
<pre><code>## [1] 0.195377</code></pre>
<p>Lastly, theres the Hosmer-Lemeshow goodness of fit test where the null hypothesis is the the model fit is good, and the alternative hypothesis is the the model is saturated (i.e, not a good fit). For our example, we fail to reject the null hypothesis at the 95% confidence level. For a detailed treatment of the test, read <a href="https://en.wikipedia.org/wiki/Hosmer%E2%80%93Lemeshow_test">this article</a>.</p>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb688-1" data-line-number="1"><span class="kw">library</span>(ResourceSelection)</a>
<a class="sourceLine" id="cb688-2" data-line-number="2"></a>
<a class="sourceLine" id="cb688-3" data-line-number="3">p.hat =<span class="st"> </span><span class="kw">predict</span>(sa.glm, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</a>
<a class="sourceLine" id="cb688-4" data-line-number="4"><span class="kw">hoslem.test</span>(SAheart<span class="op">$</span>chd, p.hat)</a></code></pre></div>
<pre><code>## 
##  Hosmer and Lemeshow goodness of fit (GOF) test
## 
## data:  SAheart$chd, p.hat
## X-squared = 9.6408, df = 8, p-value = 0.2911</code></pre>
</div>
<div id="logistic-regression-problem-set" class="section level3">
<h3><span class="header-section-number">7.7.5</span> Logistic Regression Problem Set</h3>
<p>The problem set for this section is located <a href = '/_Chapter6_ProblemSets/Logistic_Regression_PS_Questions.html'>here</a>.</p>
<p>For your convenience, the R markdown version is <a href = '/_Chapter6_ProblemSets/Logistic_Regression_PS_Questions.Rmd'>here</a>.</p>
<p>The solutions are located <a href = '/_Chapter6_ProblemSets/Logistic_Regression_PS_Answers.html'>here</a>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-faraway2006">
<p>Faraway, Julian. 2006. <em>Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models</em>. CRC Press.</p>
</div>
<div id="ref-faraway2014">
<p>Faraway, Julian. 2014. <em>Linear Models with R</em>. Chapman &amp; Hall.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="32">
<li id="fn32"><p>This model is linear in <span class="math inline">\(\beta\)</span> <strong>and</strong> <span class="math inline">\(x\)</span><a href="fundamentals-of-regression.html#fnref32" class="footnote-back"></a></p></li>
<li id="fn33"><p>This is actually logistic regression, which is covered later.<a href="fundamentals-of-regression.html#fnref33" class="footnote-back"></a></p></li>
<li id="fn34"><p>The variable was excluded for simplicity at this point, not because we cant include binary predictors in a linear model. Well cover this in a later section.<a href="fundamentals-of-regression.html#fnref34" class="footnote-back"></a></p></li>
<li id="fn35"><p>This is also called one-hot encoding<a href="fundamentals-of-regression.html#fnref35" class="footnote-back"></a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fractional-factorial-designs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-selection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "chapter"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
